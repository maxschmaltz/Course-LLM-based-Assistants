<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../../../genindex.html" /><link rel="search" title="Search" href="../../../../search.html" /><link rel="next" title="27.05. Multi-agent Environment 👾🤖👾" href="../2705/2705.html" /><link rel="prev" title="15.05. RAG Chatbot Pt. 1 📚" href="../1505/1505.html" />

    <!-- Generated with Sphinx 7.4.7 and Furo 2024.08.06 -->
        <title>20.05. RAG Chatbot Pt. 2 📚 - LLM-based Assistants</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../../index.html"><div class="brand">LLM-based Assistants</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../../index.html">
  
  
  <span class="sidebar-brand-text">LLM-based Assistants</span>
  
</a><form class="sidebar-search-container" method="get" action="../../../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Infos and Stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/topic_overview.html">Topics Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/formats/debates.html">Debates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/formats/pitches.html">Pitches</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/llm_inference_guide/README.html">LLM Inference Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 1: Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../block1_intro/2204.html">22.04. LLMs as a Form of Intelligence vs LLMs as Statistical Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block1_intro/2404.html">24.04. LLM &amp; Agent Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block1_intro/2904/2904.html">29.04. Intro to LangChain 🦜🔗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 2: Core Topics | Part 1: Business Applications</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0605.html">06.05. Virtual Assistants Pt. 1: Chatbots</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0805/0805.html">08.05. Basic LLM-based Chatbot 🤖</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1305.html">13.05. Virtual Assistants Pt. 2: RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1505/1505.html">15.05. RAG Chatbot Pt. 1 📚</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">20.05. RAG Chatbot Pt. 2 📚</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2705/2705.html">27.05. Multi-agent Environment 👾🤖👾</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2205.html">22.05. Virtual Assistants Pt. 3: Multi-agent Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0306.html">03.06. LLMs in Software Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0506/0506.html">05.06. LLM-powered Website 💻</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1706.html">17.06. Other Business Applications: Game Design, Financial Analysis etc.</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 2: Core Topics | Part 2: Applications in Science</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pt2_science/0107.html">01.07. LLMs for Research</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pt2_science/0307/0307.html">03.07. LLM-Based Hypothesis Generation💡</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 3: Wrap-up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/1007/README.html">10.07. <em>Pitch</em>: RAG Chatbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/1507/README.html">15.07. <em>Pitch</em>: Handling Customer Requests in a Multi-agent Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/1707/README.html">17.07. <em>Pitch</em>: Agent for Web Resumes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/2207/README.html">22.07. <em>Pitch</em>: LLM-based Research Assistant</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="rag-chatbot-pt-2">
<h1>20.05. RAG Chatbot Pt. 2 📚<a class="headerlink" href="#rag-chatbot-pt-2" title="Link to this heading">¶</a></h1>
<p>📍 <a class="reference external" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants/tree/main/llm-based-assistants/sessions/block2_core_topics/pt1_business/2005">Download notebook and session files</a></p>
<p>In today’l lab, we will complete our RAG chatbot and use the data we have preprocessed <a class="reference internal" href="../1505/1505.html"><span class="std std-doc">the last time</span></a> to inject our custom knowledge to the LLM.</p>
<p>Our plan for today:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#data"><span class="xref myst">Recap: Data Preprocessing</span></a></p></li>
<li><p><a class="reference internal" href="#rag"><span class="xref myst">Simple RAG</span></a></p></li>
<li><p><a class="reference internal" href="#adv_rag"><span class="xref myst">Advanced RAG</span></a></p></li>
</ul>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h2>
<p>To start with the tutorial, complete the steps <a class="reference internal" href="../../../../infos/llm_inference_guide/README.html#prerequisites"><span class="std std-ref">Prerequisites</span></a>, <a class="reference internal" href="../../../../infos/llm_inference_guide/README.html#environment-setup"><span class="std std-ref">Environment Setup</span></a>, and <a class="reference internal" href="../../../../infos/llm_inference_guide/README.html#getting-api-key"><span class="std std-ref">Getting API Key</span></a> from the <a class="reference internal" href="../../../../infos/llm_inference_guide/README.html"><span class="std std-doc">LLM Inference Guide</span></a>.</p>
<p>Today, we have more packages so we’ll use the requirements file to install the dependencies:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>We will also reproduce the basic chatbot we implemented <a class="reference internal" href="../0805/0805.html"><span class="std std-doc">earlier</span></a> as the base for the future RAG Chatbot. The only difference will be that we now need a simpler state that only keeps track of the message history (the other fields were demonstrational).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_nvidia_ai_endpoints</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatNVIDIA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.rate_limiters</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryRateLimiter</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># read system variables</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>

<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>    <span class="c1"># that loads the .env file variables into os.environ</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># choose any model, catalogue is available under https://build.nvidia.com/models</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;meta/llama-3.3-70b-instruct&quot;</span>

<span class="c1"># this rate limiter will ensure we do not exceed the rate limit</span>
<span class="c1"># of 40 RPM given by NVIDIA</span>
<span class="n">rate_limiter</span> <span class="o">=</span> <span class="n">InMemoryRateLimiter</span><span class="p">(</span>
    <span class="n">requests_per_second</span><span class="o">=</span><span class="mi">30</span> <span class="o">/</span> <span class="mi">60</span><span class="p">,</span>  <span class="c1"># 30 requests per minute to be sure</span>
    <span class="n">check_every_n_seconds</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># wake up every 100 ms to check whether allowed to make a request,</span>
    <span class="n">max_bucket_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>  <span class="c1"># controls the maximum burst size</span>
<span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatNVIDIA</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;NVIDIA_API_KEY&quot;</span><span class="p">),</span> 
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>   <span class="c1"># ensure reproducibility,</span>
    <span class="n">rate_limiter</span><span class="o">=</span><span class="n">rate_limiter</span>  <span class="c1"># bind the rate limiter</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">MermaidDrawMethod</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nest_asyncio</span>
<span class="n">nest_asyncio</span><span class="o">.</span><span class="n">apply</span><span class="p">()</span>  <span class="c1"># this is needed to draw the PNG in Jupyter</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="c1"># `messages` is a list of messages of any kind. The `add_messages` function</span>
    <span class="c1"># in the annotation defines how this state key should be updated</span>
    <span class="c1"># (in this case, it appends messages to the list, rather than overwriting them)</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="n">add_messages</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Chatbot</span><span class="p">:</span>

    <span class="n">_graph_path</span> <span class="o">=</span> <span class="s2">&quot;./graph.png&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_display_graph</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># graph builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">SimpleState</span><span class="p">)</span>
        <span class="c1"># add the nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond_node</span><span class="p">)</span>
        <span class="c1"># define edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># compile the graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compile</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_input_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">SimpleState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Your message: &quot;</span><span class="p">)</span>
        <span class="n">human_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_query</span><span class="p">)</span>
        <span class="c1"># add the input to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">human_message</span>   <span class="c1"># this will append the input to the messages</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_respond_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">SimpleState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>    <span class="c1"># will already contain the user query</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="c1"># add the response to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span>   <span class="c1"># this will append the response to the messages</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_is_quitting_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">SimpleState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># check if the user wants to quit</span>
        <span class="n">user_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="k">return</span> <span class="n">user_message</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;quit&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_display_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># unstable</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">(</span>
                <span class="n">draw_method</span><span class="o">=</span><span class="n">MermaidDrawMethod</span><span class="o">.</span><span class="n">PYPPETEER</span><span class="p">,</span>
                <span class="n">output_file_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph_path</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="c1"># add the run method</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">SystemMessage</span><span class="p">(</span>
                    <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful and honest assistant.&quot;</span> <span class="c1"># role</span>
                <span class="p">)</span>
            <span class="p">]</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>   <span class="c1">#stream_mode=&quot;updates&quot;):</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">:</span><span class="se">\t</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<h2 id="data">1. Recap: Data Preprocessing 📕</h2>
<p>We will now go over data preprocessing to rehearse the workflow and also to recreate the data collection (we used an in-memory index, so the index we created the last time was deleted after we interrupted the notebook kernel).</p>
<p>Data preprocessing includes:</p>
<ol class="arabic simple">
<li><p>Loading: load the source (document, website etc.) as a text.</p></li>
<li><p>Chunking: chunk the loaded text onto smaller pieces.</p></li>
<li><p>Converting to embeddings: embed the chunks into dense vector for further similarity search.</p></li>
<li><p>Indexing: put the embeddings into a so-called index – a special database for efficient storage and search of vectors.</p></li>
</ol>
<section id="loading">
<h3>Loading<a class="headerlink" href="#loading" title="Link to this heading">¶</a></h3>
<p>We will take a PDF version of the Topic Overview for this course. No LLM can know the contents of it, especially some highly specific facts such as dates or key points.</p>
<p>One of ways to load a PDF is to use <a class="reference external" href="https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/"><code class="docutils literal notranslate"><span class="pre">PyPDFLoader</span></code></a> that load simple textual PDFs and their metadata. In this tutorial, we focus on a simpler variant when there are no multimodal data in the PDF. You can find out more about advanced loading in tutorial <a class="reference external" href="https://python.langchain.com/docs/how_to/document_loader_pdf/">How to load PDFs</a> from LangChain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_community.document_loaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">PyPDFLoader</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">file_path</span> <span class="o">=</span> <span class="s2">&quot;./topic_overview.pdf&quot;</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">PyPDFLoader</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
<span class="n">pages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">async</span> <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">loader</span><span class="o">.</span><span class="n">alazy_load</span><span class="p">():</span>
    <span class="n">pages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">page</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ignoring wrong pointing object 10 0 (offset 0)
Ignoring wrong pointing object 31 0 (offset 0)
</pre></div>
</div>
</div>
</div>
<p>This function returns a list of <code class="docutils literal notranslate"><span class="pre">Document</span></code> objects, each containing the text of the PDF and its metadata such as title, page, creation date etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pages</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 0, &#39;page_label&#39;: &#39;1&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nTo p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!\nThe reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis \n: those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji \n: that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboardsetc. (also a couple of LLM-based games). For each of the sources, I also leave my subjectiveestimation of how important this work is for this specific topic: from yellow \n ‘partially useful’ thoughorange \n ‘useful’ to red \n ‘crucial findings / thoughts’.  T h e s e  e s t i m a t i o n s  w i l l  b e  c o n t i n u o u s l yupdated as I revise the materials.\nFor the labs, you are provided with practical tutorials that respective lab tasks will mostly derive from.The core tutorials are marked with a writing emoji \n; you are asked to inspect them in advance(better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to preparein advance to keep up with the lab.\nDisclaimer: the reading entries are no proper citations; the bibtex references as well as detailed infosabout the authors, publish date etc. can be found under the entry links.\nBlock 1: IntroWeek 122.04. Lecture: LLMs as a Form of Intelligence vs LLMs as Statistical MachinesThat is an introductory lecture, in which I will briefly introduce the course and we’ll have a warming updiscussion about different perspectives on LLMs’ nature. We will focus on two prominent outlooks: LLMis a form of intelligence and LLM is a complex statistical machine. We’ll discuss differences of LLMswith human intelligence and the degree to which LLMs exhibit (self-)awareness.\nKey points:\nCourse introduction\nDifferent perspectives on the nature of LLMs\nSimilarities and differences between human and artificial intelligence\nLLMs’ (self-)awareness\nCore Reading:\n The Debate Over Understanding in AI’s Large Language Models (pages 1-7), Santa Fe\nInstitute \nMeaning without reference in large language models, UC Berkeley &amp; DeepMind \nDissociating language and thought in large language models (intro [right after the abstract, seemore on the sectioning in this paper at the bottom of page 2], sections 1, 2.3 [LLMs are predictive…], 3-5), The University of Texas at Austin et al. \nAdditional Reading:\nLLM-basedAssistants\nINFOS AND STUFF\nBLOCK 1: INTRO\nBLOCK 2: CORE TOPICS | PART 1:BUSINESS APPLICATIONS\nBLOCK 2: CORE TOPICS | PART 2:APPLICATIONS IN SCIENCE\nBLOCK 3: WRAP-UP\nTopics Overview\nDebates\nPitches\nLLM Inference Guide\n22.04. LLMs as a Form ofIntelligence vs LLMs asStatistical Machines\n24.04. LLM &amp; Agent Basics\n29.04. Intro to LangChain \n!\n&quot;\n06.05. Virtual Assistants Pt. 1:Chatbots\n08.05. Basic LLM-basedChatbot \n#\nUnder development\nUnder development\nSearch&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 1, &#39;page_label&#39;: &#39;2&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 2 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nDo Large Language Models Understand Us?, Google Research \nSparks of Artificial General Intelligence: Early experiments with GPT-4 (chapters 1-8 &amp; 10),Microsoft Research \nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \n  (paragraphs 1, 5, 6.1),University of Washington et al. \nLarge Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective onUnderstanding, Leiden Institute of Advanced Computer Science &amp; Leiden University Medical\nCentre \n24.04. Lecture: LLM &amp; Agent BasicsIn this lecture, we’ll recap some basics about LLMs and LLM-based agents to make sure we’re on thesame page.\nKey points:\nLLM recap\nPrompting\nStructured output\nTool calling\nPiping &amp; Planning\nCore Reading:\nA Survey of Large Language Models, (sections 1, 2.1, 4.1, 4.2.1, 4.2.3-4.2.4, 4.3, 5.1.1-5.1.3, 5.2.1-5.2.4, 5.3.1, 6) Renmin University of China et al. \nEmergent Abilities of Large Language Models, Google Research, Stanford, UNC Chapel Hill,\nDeepMind\n“We Need Structured Output”: Towards User-centered Constraints on Large Language ModelOutput, Google Research &amp; Google\n Agent Instructs Large Language Models to be General Zero-Shot Reasoners (pages 1-9),Washington University &amp; UC Berkeley\nAdditional Reading:\nLanguage Models are Few-Shot Learners, OpenAI\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models, Google Research\nThe Llama 3 Herd of Models, Meta AI\nIntroducing Structured Outputs in the API, OpenAI\nTool Learning with Large Language Models: A Survey, Renmin University of China et al.\nToolACE: Winning the Points of LLM Function Calling, Huawei Noah’s Ark Lab et al.\nToolformer: Language Models Can Teach Themselves to Use Tools, Meta AI\nGranite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning ofGranular Tasks, IBM Research\n Berkeley Function-Calling Leaderboard, UC Berkeley (leaderboard)\nA Survey on Multimodal Large Language Models, University of Science and Technology of China\n&amp; Tencent YouTu Lab\nWeek 229.04. Lab: Intro to LangChainThe final introductory session will guide you through the most basic concepts of LangChain for thefurther practical sessions.&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 2, &#39;page_label&#39;: &#39;3&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 3 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nReading:\nRunnable interface, LangChain\nLangChain Expression Language (LCEL), LangChain\nMessages, LangChain\nChat models, LangChain\nStructured outputs, LangChain\nTools, LangChain\nTool calling, LangChain\n01.05.Ausfalltermin\nBlock 2: Core T opics\nPart 1: Business ApplicationsWeek 306.05. Lecture: Virtual Assistants Pt. 1: ChatbotsThe first core topic concerns chatbots. We’ll discuss how chatbots are built, how they (should) handleharmful requests and you can tune it for your use case.\nKey points:\nLLMs alignment\nMemory\nPrompting &amp; automated prompt generation\nEvaluation\nCore Reading:\n Aligning Large Language Models with Human: A Survey (pages 1-14), Huawei Noah’s Ark Lab\nSelf-Instruct: Aligning Language Models with Self-Generated Instructions, University of\nWashington et al.\nA Systematic Survey of Prompt Engineering in Large Language Models: Techniques andApplications, Indian Institute of Technology Patna, Stanford &amp; Amazon AI\nAdditional Reading:\nTraining language models to follow instructions with human feedback, OpenAI\nTraining a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,Anthropic\nA Survey on the Memory Mechanism of Large Language Model based Agents, Renmin University\nof China &amp; Huawei Noah’s Ark Lab\nAugmenting Language Models with Long-Term Memory, UC Santa Barbara &amp; Microsoft Research\nFrom LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of LargeLanguage Models, Beike Inc.\nAutomatic Prompt Selection for Large Language Models, Cinnamon AI, Hung Yen University of\nTechnology and Education &amp; Deakin University\nPromptGen: Automatically Generate Prompts using Generative Models, Baidu Research\nEvaluating Large Language Models. A Comprehensive Survey, Tianjin University&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 3, &#39;page_label&#39;: &#39;4&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 4 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\n08.05. Lab: Basic LLM-based Chatbot\nOn material of session 06.05\nIn this lab, we’ll build a chatbot and try different prompts and settings to see how it affects the output.\nReading:\n Build a Chatbot, LangChain\n LangGraph Quickstart: Build a Basic Chatbot (parts 1, 3), LangGraph\n How to add summary of the conversation history, LangGraph\nPrompt Templates, LangChain\nFew-shot prompting, LangChain\nWeek 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.\nKey points:\nGeneral knowledge vs context\nKnowledge indexing, retrieval &amp; ranking\nRetrieval tools\nAgentic RAG\nCore Reading:\n Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and HybridApproach (pages 1-7), Google DeepMind &amp; University of Michigan \nA Survey on Retrieval-Augmented Text Generation for Large Language Models (sections 1-7), York\nUniversity \nAdditional Reading:\nDon’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National\nChengchi University &amp; Academia Sinica \nSelf-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, University of\nWashington, Allen Institute for AI &amp; IBM Research AI\nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through QuestionComplexity, Korea Advanced Institute of Science and Technology\nAuto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese\nAcademy of Sciences\nQuerying Databases with Function Calling, Weaviate, Contextual AI &amp; Morningstar\n15.05. Lab: RAG Chatbot\nOn material of session 13.05\nIn this lab, we’ll expand the functionality of the chatbot built at the last lab to connect it to user-specificinformation.&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 4, &#39;page_label&#39;: &#39;5&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 5 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nReading:\nHow to load PDFs, LangChain\nText splitters, LangChain\nEmbedding models, LangChain\nVector stores, LangChain\nRetrievers, LangChain\n Retrieval augmented generation (RAG), LangChain\n LangGraph Quickstart: Build a Basic Chatbot (part 2), LangGraph\n Agentic RAG, LangGraph\nAdaptive RAG, LangGraph\nMultimodality, LangChain\nWeek 520.05. Lecture: Virtual Assistants Pt. 3: Multi-agent EnvironmentThis lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday /business operations in a multi-agent environment. We’ll look at how agents communicate with eachother, how their communication can be guided (both with and without involvement of a human), andthis all is used in real applications.\nKey points:\nMulti-agent environment\nHuman in the loop\nLLMs as evaluators\nExamples of pipelines for business operations\nCore Reading:\n LLM-based Multi-Agent Systems: Techniques and Business Perspectives (pages 1-8), Shanghai\nJiao Tong University &amp; OPPO Research Institute\nGenerative Agents: Interactive Simulacra of Human Behavior, Stanford, Google Research &amp;\nDeepMind\nAdditional Reading:\nImproving Factuality and Reasoning in Language Models through Multiagent Debate, MIT &amp; Google\nBrain\nExploring Collaboration Mechanisms for LLM Agents: A Social Psychology View, Zhejiang\nUniversity, National University of Singapore &amp; DeepMind\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, Microsoft Research\net al.\n How real-world businesses are transforming with AI — with more than 140 new stories,Microsoft (blog post)\n Built with LangGraph, LangGraph (website page)\nPlan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLMAgents As A Daily Assistant, Delft University of Technology &amp; The University of Queensland\n22.05. Lab: Multi-agent Environment\nOn material of session 20.05&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 5, &#39;page_label&#39;: &#39;6&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 6 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nThis lab will introduce a short walkthrough to creation of a multi-agent environment for automatedmeeting scheduling and preparation. We will see how the coordinator agent will communicate with twoauxiliary agents to check time availability and prepare an agenda for the meeting.\nReading:\n Multi-agent network, LangGraph\n Human-in-the-loop, LangGraph\nPlan-and-Execute, LangGraph\nReflection, LangGraph\n Multi-agent supervisor, LangGraph\nQuick Start, AutoGen\nWeek 627.05. Lecture: Software Development Pt. 1: Code Generation, Evaluation &amp;TestingThis lectures opens a new lecture mini-cycle dedicated to software development. The first lectureoverviews how LLMs are used to generate reliable code and how generated code is tested andimproved to deal with the errors.\nKey points:\nCode generation &amp; refining\nAutomated testing\nGenerated code evaluation\nCore Reading:\nLarge Language Model-Based Agents for Software Engineering: A Survey, Fudan University,\nNanyang Technological University &amp; University of Illinois at Urbana-Champaign\n CodeRL: Mastering Code Generation through Pretrained Models and Deep ReinforcementLearning (pages 1-20), Salesforce Research\nThe ART of LLM Refinement: Ask, Refine, and Trust, ETH Zurich &amp; Meta AI\nAdditional Reading:\nPlanning with Large Language Models for Code Generation, MIT-IBM Watson AI Lab et al.\nCode Repair with LLMs gives an Exploration-Exploitation Tradeoff, Cornell, Shanghai Jiao Tong\nUniversity &amp; University of Toronto\nChatUniTest: A Framework for LLM-Based Test Generation, Zhejiang University &amp; Hangzhou City\nUniversity\nTestART: Improving LLM-based Unit Testing via Co-evolution of Automated Generation and RepairIteration, Nanjing University &amp; Huawei Cloud Computing Technologies\nEvaluating Large Language Models Trained on Code, `OpenAI\n Code Generation on HumanEval, OpenAI (leaderboard)\nCodeJudge: Evaluating Code Generation with Large Language Models, Huazhong University of\nScience and Technology &amp; Purdue University\n29.05.Ausfalltermin&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 6, &#39;page_label&#39;: &#39;7&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 7 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nWeek 703.06. Lecture: Software Development Pt. 2: Copilots, LLM-powered WebsitesThe second and the last lecture of the software development cycle focuses on practical application ofLLM code generation, in particular, on widely-used copilots (real-time code generation assistants) andLLM-supported web development.\nKey points:\nCopilots &amp; real-time hints\nLLM-powered websites\nLLM-supported deployment\nFurther considerations: reliability, sustainability etc.\nCore Reading:\n LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities andLimitations (pages 1-11), University of Oslo\nA Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis,Google DeepMind &amp; The University of Tokyo\nCan ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large LanguageModel Code Generation, UC San Diego\nAdditional Reading:\nDesign and evaluation of AI copilots – case studies of retail copilot templates, Microsoft\n Your AI Companion, Microsoft (blog post)\nGitHub Copilot, GitHub (product page)\n Research: quantifying GitHub Copilot’s impact on developer productivity and happiness, GitHub\n(blog post)\n Cursor: The AI Code Editor, Cursor (product page)\nAutomated Unit Test Improvement using Large Language Models at Meta, Meta\nHuman-In-the-Loop Software Development Agents, Monash University, The University of\nMelbourne &amp; Atlassian\nAn LLM-based Agent for Reliable Docker Environment Configuration, Harbin Institute of\nTechnology &amp; ByteDance\nLearn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation, TWT GmbH\nScience &amp; Innovation et al.\nEnhancing Large Language Models for Secure Code Generation: A Dataset-driven Study onVulnerability Mitigation, South China University of Technology &amp; University of Innsbruck\n05.06 Lab: LLM-powered Website\nOn material of session 03.06\nIn this lab, we’ll have the LLM make a website for us: it will both generate the contents of the websiteand generate all the code required for rendering, styling and navigation.\nReading:\nsee session 22.05\n HTML: Creating the content, MDN\n Getting started with CSS, MDN&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 7, &#39;page_label&#39;: &#39;8&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nWeek 8: Having Some Rest10.06.Ausfalltermin\n12.06.Ausfalltermin\nWeek 917.06. Pitch: RAG Chatbot\nOn material of session 06.05 and session 13.05\nThe first pitch will be dedicated to a custom RAG chatbot that the contractors (the presentingstudents, see the infos about Pitches) will have prepared to present. The RAG chatbot will have to beable to retrieve specific information from the given documents (not from the general knowledge!) anduse it in its responses. Specific requirements will be released on 22.05.\nReading: see session 06.05, session 08.05, session 13.05, and session 15.05\n19.06.Ausfalltermin\nWeek 1024.06. Pitch: Handling Customer Requests in a Multi-agent Environment\nOn material of session 20.05\nIn the second pitch, the contractors will present their solution to automated handling of customerrequests. The solution will have to introduce a multi-agent environment to take off working load froman imagined support team. The solution will have to read and categorize tickets, generate replies and(in case of need) notify the human that their interference is required. Specific requirements will bereleased on 27.05.\nReading: see session 20.05 and session 22.05\n26.06. Lecture: Other Business Applications: Game Design, Financial Analysisetc.This lecture will serve a small break and will briefly go over other business scenarios that the LLMs areused in.\nKey points:\nGame design &amp; narrative games\nFinancial applications\nContent creation\nAdditional Reading:&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 8, &#39;page_label&#39;: &#39;9&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 9 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nPlayer-Driven Emergence in LLM-Driven Game Narrative, Microsoft Research\nGenerating Converging Narratives for Games with Large Language Models, U.S. Army Research\nLaboratory\nGame Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and BehaviorBranch, University of Tokyo\n AI Dungeon Games, AI Dungeon (game catalogue)\n AI Town, Andreessen Horowitz &amp; Convex (game)\nIntroducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs, HuggingFace\n(blog post)\nBlip, bliporg (GitHub repo)\ngigax, GigaxGames (GitHub repo)\nLarge Language Models in Finance: A Survey, Columbia &amp; New York University\nFinLlama: Financial Sentiment Classification for Algorithmic Trading Applications, Imperial College\nLondon &amp; MIT\nEquipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance, Monash\nUniversity\nLLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation,Shanghai Jiao Tong University et al.\nAssisting in Writing Wikipedia-like Articles From Scratch with Large Language Models, Stanford\nLarge Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools,MIT, Harvard University &amp; MIT-IBM Watson AI Lab\nPart 2: Applications in ScienceWeek 1101.07. Lecture: LLMs in Research: Experiment Planning &amp; HypothesisGenerationThe first lecture dedicated to scientific applications shows how LLMs are used to plan experiments andgenerate hypothesis to accelerate research.\nKey points:\nExperiment planning\nHypothesis generation\nPredicting possible results\nCore Reading:\n Hypothesis Generation with Large Language Models (pages 1-9), University of Chicago &amp;\nToyota Technological Institute at Chicago\n LLMs for Science: Usage for Code Generation and Data Analysis (pages 1-6), TUM\nEmergent autonomous scientific research capabilities of large language models, Carnegie Mellon\nUniversity\nAdditional Reading:&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 9, &#39;page_label&#39;: &#39;10&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 10 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nImproving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,University of Virginia\nPaper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance,University of Illinois at Urbana-Champaign, Carnegie Mellon University &amp; Carleton College\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding, University of Science and\nTechnology of China &amp; DP Technology\nMapping the Increasing Use of LLMs in Scientific Papers, Stanford\n03.07: Lab: Experiment Planning &amp; Hypothesis Generation\nOn material of session 01.07\nIn this lab, we’ll practice in facilitating researcher’s work with LLMs on the example of a toy scientificresearch.\nReading: see session 22.05\nWeek 1208.07: Pitch: Agent for Code Generation\nOn material of session 27.05\nThis pitch will revolve around the contractors’ implementation of a self-improving code generator. Thecode generator will have to generate both scripts and test cases for a problem given in the inputprompt, run the tests and refine the code if needed. Specific requirements will be released on 17.06.\nReading: see session 27.05 and session 05.06\n10.07. Lecture: Other Applications in Science: Drug Discovery, Math etc. &amp;Scientific ReliabilityThe final core topic will mention other scientific applications of LLMs that were not covered in theprevious lectures and address the question of reliability of the results obtained with LLMs.\nKey points:\nDrug discovery, math &amp; other applications\nScientific confidence &amp; reliability\nCore Reading:\n Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as ScienceCommunicators (pages 1-9), Indian Institute of Technology\nAdditional Reading:&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 10, &#39;page_label&#39;: &#39;11&#39;}, page_content=&#39;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 11 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nA Comprehensive Survey of Scientific Large Language Models and Their Applications in ScientificDiscovery, University of Illinois at Urbana-Champaign et al.\nLarge Language Models in Drug Discovery and Development: From Disease Mechanisms to ClinicalTrials, Department of Data Science and AI, Monash University et al.\nLLM-SR: Scientific Equation Discovery via Programming with Large Language Models, Virginia\nTech et al.\n Awesome Scientific Language Models, yuzhimanhua (GitHub repo)\nCURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning, Google\net al.\nMultiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-ConfidentEven When They Are Wrong, Nanjing University of Aeronautics and Astronautics et al.\nBlock 3: Wrap-upWeek 1315.07. Pitch: Agent for Web Development\nOn material of session 03.06\nThe contractors will present their agent that will have to generate full (minimalistic) websites by aprompt. For each website, the agent will have to generate its own style and a simple menu with workingnavigation as well as the contents. Specific requirements will be released on 24.06.\nReading: see session 03.06 and session 05.06\n17.07. Lecture: Role of AI in Recent YearsThe last lecture of the course will turn to societal considerations regarding LLMs and AI in general andwill investigate its role and influence on the humanity nowadays.\nKey points:\nStudies on influence of AI in the recent years\nStudies on AI integration rate\nEthical, legal &amp; environmental aspects\nCore Reading:\n Protecting Human Cognition in the Age of AI (pages 1-5), The University of Texas at Austin et al.\n Artificial intelligence governance: Ethical considerations and implications for social responsibility(pages 1-12), University of Malta\nAdditional Reading:&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 11, &#39;page_label&#39;: &#39;12&#39;}, page_content=&quot;12.05.25, 17:28Topics Overview - LLM-based Assistants\nPage 12 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\nAugmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI’sImpact on Creative Tasks, Tsinghua University &amp; Wuhan University of Technology\nHuman Creativity in the Age of LLMs: Randomized Experiments on Divergent and ConvergentThinking, University of Toronto\nEmpirical evidence of Large Language Model’s influence on human spoken communication, Max-\nPlanck Institute for Human Development\n The 2025 AI Index Report: Top Takeaways, Stanford\nGrowing Up: Navigating Generative AI’s Early Years – AI Adoption Report: Executive Summary, AI at\nWharton\nEthical Implications of AI in Data Collection: Balancing Innovation with Privacy, AI Data Chronicles\nLegal and ethical implications of AI-based crowd analysis: the AI Act and beyond, Vrije\nUniversiteit\nA Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges,Cleveland State University et al.\nWeek 1422.07. Pitch: LLM-based Research Assistant\nOn material of session 01.07\nThe last pitch will introduce an agent that will have to plan the research, generate hypotheses, find theliterature etc. for a given scientific problem. It will then have to introduce its results in form of a TODOor a guide for the researcher to start off of. Specific requirements will be released on 01.07.\nReading: see session 01.07 and session 03.07\n24.07. Debate: Role of AI in Recent Years + Wrap-up\nOn material of session 17.07\nThe course will be concluded by the final debates, after which a short Q&amp;A session will be held.\nDebate topics:\nLLM Behavior: Evidence of Awareness or Illusion of Understanding?\nShould We Limit the Usage of AI?\nReading: see session 17.07\nCopyright © 2025, Maksim ShmaltsMade with Sphinx and @pradyunsg&#39;s Furo&quot;)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!
The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis 
: those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji 
: that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboardsetc. (also a couple of LLM-based games). For each of the sources, I also leave my subjectiveestimation of how important this work is for this specific topic: from yellow 
 ‘partially useful’ thoughorange 
 ‘useful’ to red 
 ‘crucial findings / thoughts’.  T h e s e  e s t i m a t i o n s  w i l l  b e  c o n t i n u o u s l yupdated as I revise the materials.
For the labs, you are provided with practical tutorials that respective lab tasks will mostly derive from.The core tutorials are marked with a writing emoji 
; you are asked to inspect them in advance(better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to preparein advance to keep up with the lab.
Disclaimer: the reading entries are no proper citations; the bibtex references as well as detailed infosabout the authors, publish date etc. can be found under the entry links.
Block 1: IntroWeek 122.04. Lecture: LLMs as a Form of Intelligence vs LLMs as Statistical MachinesThat is an introductory lecture, in which I will briefly introduce the course and we’ll have a warming updiscussion about different perspectives on LLMs’ nature. We will focus on two prominent outlooks: LLMis a form of intelligence and LLM is a complex statistical machine. We’ll discuss differences of LLMswith human intelligence and the degree to which LLMs exhibit (self-)awareness.
Key points:
Course introduction
Different perspectives on the nature of LLMs
Similarities and differences between human and artificial intelligence
LLMs’ (self-)awareness
Core Reading:
 The Debate Over Understanding in AI’s Large Language Models (pages 1-7), Santa Fe
Institute 
Meaning without reference in large language models, UC Berkeley &amp; DeepMind 
Dissociating language and thought in large language models (intro [right after the abstract, seemore on the sectioning in this paper at the bottom of page 2], sections 1, 2.3 [LLMs are predictive…], 3-5), The University of Texas at Austin et al. 
Additional Reading:
LLM-basedAssistants
INFOS AND STUFF
BLOCK 1: INTRO
BLOCK 2: CORE TOPICS | PART 1:BUSINESS APPLICATIONS
BLOCK 2: CORE TOPICS | PART 2:APPLICATIONS IN SCIENCE
BLOCK 3: WRAP-UP
Topics Overview
Debates
Pitches
LLM Inference Guide
22.04. LLMs as a Form ofIntelligence vs LLMs asStatistical Machines
24.04. LLM &amp; Agent Basics
29.04. Intro to LangChain 
!
&quot;
06.05. Virtual Assistants Pt. 1:Chatbots
08.05. Basic LLM-basedChatbot 
#
Under development
Under development
Search
</pre></div>
</div>
</div>
</div>
<p>As you can see, the result is not satisfying because the PDF has a more complex structure than just one-paragraph text. To handle it’s layout, we could use <code class="docutils literal notranslate"><span class="pre">UnstructuredLoader</span></code> that will return a <code class="docutils literal notranslate"><span class="pre">Document</span></code> not for the whole page but for a single structure; for simplicity, let’s now go with <code class="docutils literal notranslate"><span class="pre">PyPDF</span></code>.</p>
</section>
<section id="chunking">
<h3>Chunking<a class="headerlink" href="#chunking" title="Link to this heading">¶</a></h3>
<p>During RAG, relevant documents are usually retrieved by semantic similarity that is calculated between the search query and each document in the index. However, if we calculate vectors for the entire PDF pages, we risk not to capture any meaning in the embedding because the context is just too long. That is why usually, loaded text is <em>chunked</em> in a RAG application; embeddings for smaller pieces of text are more discriminative, and thus the relevant context may be retrieved better. Furthermore, it ensure process consistency when working documents of varying sizes, and is just more computationally efficient.</p>
<p>Different approaches to chunking are described in tutorial <a class="reference external" href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a> from LangChain. We’ll use <code class="docutils literal notranslate"><span class="pre">RecursiveCharacterTextSplitter</span></code> – a good option in terms of simplicity-quality ratio for simple cases. This splitter tries to keep text structures (paragraphs, sentences) together and thus maintain text coherence in chunks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_text_splitters</span><span class="w"> </span><span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="c1"># maximum number of characters in a chunk</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span> <span class="c1"># number of characters to overlap between chunks</span>
<span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">split_page</span><span class="p">(</span><span class="n">page</span><span class="p">:</span> <span class="n">Document</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]:</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">page</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">Document</span><span class="p">(</span>
            <span class="n">page_content</span><span class="o">=</span><span class="n">chunk</span><span class="p">,</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">page</span><span class="o">.</span><span class="n">metadata</span><span class="p">,</span>
        <span class="p">)</span> 
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span>
    <span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">docs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">pages</span><span class="p">:</span>
    <span class="n">docs</span> <span class="o">+=</span> <span class="n">split_page</span><span class="p">(</span><span class="n">page</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converted </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pages</span><span class="p">)</span><span class="si">}</span><span class="s2"> pages into </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="si">}</span><span class="s2"> chunks.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Converted 12 pages into 66 chunks.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">docs</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>For the labs, you are provided with practical tutorials that respective lab tasks will mostly derive from.The core tutorials are marked with a writing emoji 
; you are asked to inspect them in advance(better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to preparein advance to keep up with the lab.
</pre></div>
</div>
</div>
</div>
</section>
<section id="convert-to-embeddings">
<h3>Convert to Embeddings<a class="headerlink" href="#convert-to-embeddings" title="Link to this heading">¶</a></h3>
<p>As discussed, the retrieval usually succeeds by vector similarity and the index contains not the actual texts but their vector representations. Vector representations are created by <em>embedding models</em> – models usually made specifically for this objective by being trained to create more similar vectors for more similar sentences and to push apart dissimilar sentences in the vector space.</p>
<p>In the last session, we used the <a class="reference external" href="https://build.nvidia.com/nvidia/nv-embedqa-e5-v5?snippet_tab=LangChain"><code class="docutils literal notranslate"><span class="pre">nv-embedqa-e5-v5</span></code></a> model – a model from NVIDIA pretrained for English QA. However, their didn’t work very stable, so in this session, we’ll substitute them with <a class="reference external" href="https://huggingface.co/sentence-transformers">HF Sentence Transformers Embeddings</a>: an open-source lightweight alternative that runs <strong>locally</strong>. However, the choice of the model here also heavily depends on the use case; for example, the model we will be using – <a class="reference external" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><code class="docutils literal notranslate"><span class="pre">all-MiniLM-L6-v2</span></code></a> – truncates input text longer than 256 word pieces by default, which is fine for our short passages but may be critical in other applications.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">EMBEDDING_NAME</span> <span class="o">=</span> <span class="s2">&quot;all-MiniLM-L6-v2&quot;</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_NAME</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/maxschmaltz/Documents/Course-LLM-based-Assistants/llm-based-assistants/sessions/block2_core_topics/pt1_business/2005/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
Xet Storage is enabled for this repo, but the &#39;hf_xet&#39; package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
</pre></div>
</div>
</div>
</div>
<p>An embedding model receives an input text and returns a dense vector that is believed to capture its semantic properties.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="s2">&quot;retrieval augmented generation&quot;</span><span class="p">)</span>
<span class="n">test_embedding</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.08718939125537872,
 -0.017552487552165985,
 -0.02729196660220623,
 0.05240696668624878,
 -0.021445486694574356,
 0.07236161828041077,
 0.06576938927173615,
 -0.05251311510801315,
 -0.010767153464257717,
 -0.03447244316339493,
 0.06100146472454071,
 -0.011422315612435341,
 0.07560884952545166,
 -0.03255048021674156,
 -0.021511150524020195,
 0.008553139865398407,
 0.034805942326784134,
 0.03402310237288475,
 -0.03876182436943054,
 -0.06949298828840256,
 -0.015863697975873947,
 0.040087372064590454,
 0.019959909841418266,
 -0.03369267284870148,
 0.003124572103843093,
 -0.01715978793799877,
 -0.006432410795241594,
 -0.03536583110690117,
 0.141240656375885,
 -0.02228039875626564,
 0.04633574187755585,
 0.10714968293905258,
 -0.07780978083610535,
 0.04519134759902954,
 -0.03231765329837799,
 0.05535123497247696,
 -0.10309430956840515,
 0.10207292437553406,
 0.009038606658577919,
 -0.03525232896208763,
 -0.049930084496736526,
 0.031047439202666283,
 0.03019150160253048,
 -0.02269360050559044,
 0.06476514786481857,
 -0.06673639267683029,
 -0.09085537493228912,
 -0.0030079304706305265,
 0.0018228460103273392,
 0.011870376765727997,
 -0.04707968235015869,
 -0.004527844022959471,
 -0.025533059611916542,
 -0.016370372846722603,
 0.011221242137253284,
 0.06357301026582718,
 0.0033866423182189465,
 -0.0373876690864563,
 -0.08252184092998505,
 -0.06394127756357193,
 -0.014947792515158653,
 -0.1283581256866455,
 -0.07762394100427628,
 -0.0534483939409256,
 0.0475839339196682,
 -0.02412143163383007,
 0.06741201132535934,
 -0.0056900521740317345,
 0.011296355165541172,
 0.019414132460951805,
 -0.06329578906297684,
 -0.010496418923139572,
 -0.025765469297766685,
 0.08242304623126984,
 0.04078071564435959,
 0.1073637381196022,
 0.012940718792378902,
 -0.0813097432255745,
 0.03622240945696831,
 0.004342259839177132,
 0.006024372298270464,
 -0.03921079635620117,
 0.06007775664329529,
 0.00166932528372854,
 0.034063201397657394,
 0.0020538237877190113,
 0.03759211301803589,
 -0.04690760374069214,
 0.016682617366313934,
 0.027662847191095352,
 -0.02605142630636692,
 -0.03171469271183014,
 0.03400687873363495,
 -0.013206538744270802,
 -0.026792503893375397,
 -0.0156557559967041,
 0.09320804476737976,
 -0.06534723192453384,
 0.016737347468733788,
 0.09889230132102966,
 0.07293327897787094,
 0.0716785416007042,
 0.02019762620329857,
 -0.014685732312500477,
 -0.044228456914424896,
 -0.027015535160899162,
 0.044873982667922974,
 0.02416594699025154,
 -0.01541962567716837,
 -0.07742788642644882,
 -0.014204404316842556,
 -0.012951254844665527,
 0.02990981936454773,
 -0.0670965239405632,
 -0.023469531908631325,
 0.019714754074811935,
 -0.006615940947085619,
 -0.02472003735601902,
 0.07622068375349045,
 -0.10430707782506943,
 0.012092015706002712,
 0.0103624127805233,
 0.0072660925798118114,
 0.03822939842939377,
 -0.05302713066339493,
 -0.05786251276731491,
 0.02362321875989437,
 -3.1146500760242126e-33,
 0.02739580161869526,
 0.007799218408763409,
 0.030314011499285698,
 0.09257230907678604,
 0.009595884941518307,
 0.0165739506483078,
 0.0775405615568161,
 0.04281589388847351,
 -0.04223857447504997,
 -0.107220359146595,
 -0.04938485473394394,
 0.08949682861566544,
 -0.018288100138306618,
 0.08887151628732681,
 0.06218535080552101,
 -0.021059619262814522,
 -0.022731175646185875,
 0.09228453040122986,
 0.03685983642935753,
 0.016744032502174377,
 0.0068116458132863045,
 0.014910407364368439,
 0.021146360784769058,
 -0.06683419644832611,
 0.014837664552032948,
 0.00986749492585659,
 -0.013698508962988853,
 -0.04000495746731758,
 -0.06458384543657303,
 0.000607188674621284,
 -0.05840948224067688,
 0.042031846940517426,
 -0.01182975247502327,
 0.024098893627524376,
 -0.02406744472682476,
 0.04856685549020767,
 0.048356227576732635,
 -0.00706762308254838,
 0.0214801374822855,
 -0.007435947190970182,
 0.04717991128563881,
 0.03317352756857872,
 0.06760194897651672,
 -0.0725097581744194,
 -0.08834613114595413,
 0.004588903859257698,
 0.0333319753408432,
 0.04611990600824356,
 -0.06713993847370148,
 0.0068238903768360615,
 0.034443460404872894,
 0.07798873633146286,
 -0.12097905576229095,
 -0.0878448337316513,
 0.0739184096455574,
 -0.004626526031643152,
 -0.049848511815071106,
 0.0845608115196228,
 0.056117650121450424,
 0.05702828988432884,
 0.04476482793688774,
 0.053383275866508484,
 0.09401398152112961,
 0.056368619203567505,
 -0.064379021525383,
 0.04319624975323677,
 0.045496802777051926,
 -0.04076174646615982,
 0.17445245385169983,
 0.016478080302476883,
 -0.03775310888886452,
 -0.036436427384614944,
 0.013020608574151993,
 -0.1050802543759346,
 0.08973690122365952,
 -0.05857545882463455,
 -0.027984371408820152,
 -0.038374338299036026,
 0.010340191423892975,
 -0.04257754981517792,
 -0.06980421394109726,
 -0.022713419049978256,
 -0.034653082489967346,
 0.002139865653589368,
 0.019379043951630592,
 -0.07190203666687012,
 0.04081670939922333,
 -0.09128168970346451,
 -0.007772673387080431,
 -0.069792740046978,
 0.011165888048708439,
 0.04168428108096123,
 -0.02773478999733925,
 0.027545265853405,
 0.043000251054763794,
 1.9090292743582884e-33,
 0.023852724581956863,
 -0.05112120881676674,
 -0.006110853049904108,
 -0.018936337903141975,
 0.012887736782431602,
 -0.040601737797260284,
 -0.007608255371451378,
 -0.064246766269207,
 -0.05957970395684242,
 -0.028532736003398895,
 -0.049880608916282654,
 -0.0249363761395216,
 0.014040950685739517,
 -0.02930120751261711,
 -0.02681984193623066,
 -0.001526402309536934,
 -0.03989081457257271,
 -0.06108027324080467,
 -0.025966564193367958,
 0.07759884744882584,
 0.016837269067764282,
 0.024788782000541687,
 -0.04913561791181564,
 -0.011247153393924236,
 0.031221885234117508,
 0.02929314598441124,
 0.01990882307291031,
 -0.046744197607040405,
 -0.015650060027837753,
 -0.0033993797842413187,
 0.01988779380917549,
 -0.05016341432929039,
 0.03616257384419441,
 -0.009688613004982471,
 -0.053137846291065216,
 0.015548999421298504,
 0.08536969870328903,
 0.00046374136582016945,
 -0.10475442558526993,
 0.0959901511669159,
 -0.010590538382530212,
 0.0005332615692168474,
 -0.0315580889582634,
 0.06814399361610413,
 0.024714365601539612,
 -0.01642696000635624,
 -0.044301100075244904,
 0.05846698209643364,
 0.0305433738976717,
 0.0351850762963295,
 0.03364138677716255,
 0.03364896401762962,
 -0.05115457624197006,
 -0.07231186330318451,
 -0.04751227796077728,
 -0.070430226624012,
 -0.03169975429773331,
 -0.056794293224811554,
 0.069991834461689,
 0.023399094119668007,
 -0.057767603546381,
 -0.058912817388772964,
 -0.02279546484351158,
 0.0005012541078031063,
 0.05812565237283707,
 -0.0464903861284256,
 -0.017210209742188454,
 0.005402414593845606,
 -0.08622312545776367,
 -0.000911232375074178,
 0.021159576252102852,
 -0.002533447928726673,
 0.0549214668571949,
 -0.04015940800309181,
 0.11574247479438782,
 0.031063484027981758,
 0.04054126515984535,
 0.05624070018529892,
 0.05388813093304634,
 -0.07476453483104706,
 -0.12748748064041138,
 0.07535384595394135,
 0.04001101851463318,
 0.1253139078617096,
 -0.08527559787034988,
 -0.036197226494550705,
 0.039345644414424896,
 0.05518045648932457,
 -0.021991057321429253,
 -0.01585155911743641,
 -0.017070360481739044,
 -0.007107829209417105,
 -0.016361547634005547,
 0.016446873545646667,
 0.04554374888539314,
 -1.115378278626622e-08,
 -0.08995316177606583,
 0.0313422828912735,
 -0.03209814056754112,
 0.03675302863121033,
 0.054376643151044846,
 0.03777965530753136,
 -0.05917801335453987,
 0.06558234244585037,
 -0.04349064826965332,
 -0.069773368537426,
 -0.010083134286105633,
 -0.005738626234233379,
 -0.01205505058169365,
 0.015104155987501144,
 0.040337566286325455,
 -0.008147886022925377,
 0.014851447194814682,
 0.03131204470992088,
 -0.032955992966890335,
 0.00409349799156189,
 0.03588413819670677,
 0.07718852907419205,
 0.04611629992723465,
 0.01334471721202135,
 0.023300277069211006,
 0.02821965515613556,
 -0.0066739823669195175,
 -0.0029166792519390583,
 0.1073087826371193,
 0.0037622086238116026,
 0.049567416310310364,
 -0.009718682616949081,
 0.04146944731473923,
 0.01478524599224329,
 0.06772028654813766,
 0.007248429581522942,
 -0.04934094473719597,
 -0.0584770031273365,
 -0.04953351989388466,
 -0.06965198367834091,
 0.03667891025543213,
 0.02164360135793686,
 -0.04110253229737282,
 -0.024316079914569855,
 0.038025762885808945,
 -0.02508619800209999,
 0.03187211602926254,
 -0.0794006884098053,
 -0.013028672896325588,
 -0.03650461882352829,
 -0.11604329198598862,
 -0.13706286251544952,
 0.04139665886759758,
 0.011682813055813313,
 0.1062411218881607,
 0.013992193154990673,
 0.029270553961396217,
 -0.015916042029857635,
 0.06916730105876923,
 0.0022920502815395594,
 0.1077147051692009,
 0.055848877876996994,
 -0.06970211863517761,
 0.0435895211994648]
</pre></div>
</div>
</div>
</div>
</section>
<section id="indexing">
<h3>Indexing<a class="headerlink" href="#indexing" title="Link to this heading">¶</a></h3>
<p>Now that we have split our data and initialized the embeddings, we can start indexing it. There are a lot of different implementations of indexes, you can take a lot at available options in <a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/">Vector stores</a>. One of the popular choices is <a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/qdrant/">Qdrant</a> that provides a simple data management and can be deployed both locally, on a remote machine, and on the cloud.</p>
<p>Qdrant support persisting your vector storage, i.e. storing it on the working machine, but for simplicity, we will use it in the in-memory mode, so that the storage exists only as long as the notebook does.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_qdrant</span><span class="w"> </span><span class="kn">import</span> <span class="n">QdrantVectorStore</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">qdrant_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">QdrantClient</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">qdrant_client.http.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Distance</span><span class="p">,</span> <span class="n">VectorParams</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">uuid</span><span class="w"> </span><span class="kn">import</span> <span class="n">uuid4</span>
</pre></div>
</div>
</div>
</div>
<p>First things first, we need to create a <em>client</em> – a Qdrant instance that will be the entrypoint for all the actions we do with the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qd_client</span> <span class="o">=</span> <span class="n">QdrantClient</span><span class="p">(</span><span class="s2">&quot;:memory:&quot;</span><span class="p">)</span>    <span class="c1"># in-memory Qdrant client</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">test_embedding</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>384
</pre></div>
</div>
</div>
</div>
<p>Then, as we use an in-memory client that does not store the index between the notebook sessions, we need to initialize a <em>collection</em>. Alternatively, if we were persisting the data, we would perform a check if the collection exists and then either create or load it.</p>
<p>For Qdrant to initialize the structure of the index correctly, we need to provide the dimentionality of the embedding we will be using as well as teh distance metric.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;2005&quot;</span>

<span class="n">qd_client</span><span class="o">.</span><span class="n">create_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
    <span class="c1"># embedding params here</span>
    <span class="n">vectors_config</span><span class="o">=</span><span class="n">VectorParams</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">test_embedding</span><span class="p">),</span>   <span class="c1"># is there a better way?</span>
        <span class="n">distance</span><span class="o">=</span><span class="n">Distance</span><span class="o">.</span><span class="n">COSINE</span>    <span class="c1"># cosine distance</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Finally, we use a LangChain wrapper to connect to the index to unify the workflow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vector_store</span> <span class="o">=</span> <span class="n">QdrantVectorStore</span><span class="p">(</span>
    <span class="n">client</span><span class="o">=</span><span class="n">qd_client</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we are ready to add our chunks to the vector storage. As we will be adding the chunks, the index will take care about converting our passages into embeddings.</p>
<p>In order to be able to delete / modify the chunks afterwards, we assign them with unique ids that we generate dynamically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">uuid4</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">))]</span>
<span class="n">vector_store</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span>
    <span class="n">docs</span><span class="p">,</span>
    <span class="n">ids</span><span class="o">=</span><span class="n">ids</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;94ad0d3e-d41f-4fa6-833c-6a6c4afeaa45&#39;,
 &#39;cd8a6fd5-8024-46a2-9b74-e890379f40bb&#39;,
 &#39;1a8a5f6c-1672-4293-896d-86a5e6bc189c&#39;,
 &#39;f925722c-424c-4356-89ef-4d2ad767f4d5&#39;,
 &#39;6513f0d5-258f-42ef-8d22-0dd3ce0954a3&#39;,
 &#39;c2a4adac-17a7-4c34-836e-11096649aeb5&#39;,
 &#39;d0c1cd52-bbe1-4a3d-a125-e3aa8acdaeec&#39;,
 &#39;1c5e520d-12c3-4495-ad71-2641612906ad&#39;,
 &#39;0cc8904d-8ea9-4695-9bdf-3078177df420&#39;,
 &#39;beae1abc-e3f9-49b4-b86e-b1490eb4e330&#39;,
 &#39;a4753493-4ff4-4b85-ae34-f898076d0029&#39;,
 &#39;ed48ce57-885f-491c-9fc8-197e3a9481fc&#39;,
 &#39;b932bf6d-5f37-4841-a8d7-8680df916167&#39;,
 &#39;9b894a91-d821-4909-8828-53ba77d14c4a&#39;,
 &#39;fec59831-30d9-45b3-a062-64b0af932838&#39;,
 &#39;b4dd4e44-5083-439f-b4e8-44452c0705b9&#39;,
 &#39;e3620974-4fbc-4f2e-be60-bdbc002d7740&#39;,
 &#39;72b1b0ec-ade1-4c8f-9c43-016365e3b487&#39;,
 &#39;a3c866b0-b216-4906-8a5b-5c3c5bc11e28&#39;,
 &#39;6cd4afdc-8741-41bc-bd01-464274d4c4fd&#39;,
 &#39;cdbdbfff-b43c-4e11-b63b-a74a6147316b&#39;,
 &#39;a6660fde-2c01-46d6-9a95-056512134dae&#39;,
 &#39;656c7c62-a746-4ee3-80a9-b5ff36210ffe&#39;,
 &#39;b0070e4a-8bac-4d7e-a311-7daed4774bb1&#39;,
 &#39;f5aef846-c175-4281-9dfa-76487e125130&#39;,
 &#39;82b3f0d8-3ed9-46dc-aa2c-8e176e08ade3&#39;,
 &#39;e4c4ab49-5c0d-441b-88f3-94675e81a567&#39;,
 &#39;06ae5d03-3b6e-4e80-b86d-a7b3f67cbb8d&#39;,
 &#39;99d08640-94ff-453e-90c0-7e4941d00d56&#39;,
 &#39;f230f890-1c31-44d2-81aa-bd504fe7539b&#39;,
 &#39;17245b54-6142-4875-af33-2251e691c684&#39;,
 &#39;8f677e73-462e-4f50-9ef5-55d11d7b00e0&#39;,
 &#39;965381f6-922c-4a0c-87d7-1bb66790fd53&#39;,
 &#39;ff6ea092-d77d-4206-95ed-43f9c4346b03&#39;,
 &#39;9b0391ae-8fff-4544-a240-e82be44ccf24&#39;,
 &#39;98037bee-8d70-4e3b-bd38-58ea5453eeea&#39;,
 &#39;adf2d1b5-65d6-4504-9421-f3304133a4a8&#39;,
 &#39;79608c97-df83-44a1-93e2-447032afa715&#39;,
 &#39;b0614f9f-a8ca-447f-bc2b-e64209f84b09&#39;,
 &#39;804a1a29-94f2-4dea-b895-5ab54ec000be&#39;,
 &#39;d641eacd-7f61-49d5-afed-ad68c8f32d11&#39;,
 &#39;670a00f0-e061-4a8e-af11-ff02493ad162&#39;,
 &#39;edad71fe-b07a-4bd6-aa89-1229a949956c&#39;,
 &#39;0ef16281-6afb-48a5-8aab-b0ec0db79683&#39;,
 &#39;f211358d-5d5d-4217-94e1-fa5296bfebd0&#39;,
 &#39;d158c4d9-ce85-42b7-9ba5-87780f413d1f&#39;,
 &#39;2d7e4e85-c65a-496a-ae5b-9ead0484fb1a&#39;,
 &#39;e719c032-a1ee-4616-84ce-e0a382e5b85e&#39;,
 &#39;0f6f9a90-b0d2-4711-9292-6e7d5445e760&#39;,
 &#39;39e8dd3a-8a74-4f0a-97d3-365b271bc9d3&#39;,
 &#39;f11c78f5-f17f-4705-a557-3dd27ef8d92e&#39;,
 &#39;ffbe8c5a-395f-4fdb-a7d1-206b3b5eb59b&#39;,
 &#39;4e92b582-84af-40ac-a1cd-08ab66972325&#39;,
 &#39;17c583b3-e480-4367-9616-48f500dfc2a5&#39;,
 &#39;eddaa35d-d185-4ca2-b92c-7e769ee5e96e&#39;,
 &#39;6f86e05c-0a56-4188-985d-9391ae506624&#39;,
 &#39;317d9449-575d-4290-b1e0-6aafbd90df42&#39;,
 &#39;5d9754bc-dcf5-403b-8c44-1991bf22f22f&#39;,
 &#39;2dbd8454-4515-4918-b4cd-ac7627878c38&#39;,
 &#39;1e0b5a80-1746-4000-bb98-b38735f3cfc5&#39;,
 &#39;81aa5b84-9a40-4e57-8fc8-c6ec0b682be8&#39;,
 &#39;7fbaacdb-1ad9-4e26-9736-9cb79f2ab2cf&#39;,
 &#39;1d3dba43-222f-4f13-8b82-3b97c1f1e4b4&#39;,
 &#39;c1537d53-64c9-4477-913c-7878e0b2c448&#39;,
 &#39;6c3beac6-3274-4267-8312-8638c49015bd&#39;,
 &#39;36bf6a1a-c1ac-4cac-923f-d11049855c4e&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vector_store</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="s2">&quot;retrieval augmented generation&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 3, &#39;page_label&#39;: &#39;4&#39;, &#39;_id&#39;: &#39;656c7c62-a746-4ee3-80a9-b5ff36210ffe&#39;, &#39;_collection_name&#39;: &#39;2005&#39;}, page_content=&#39;Retrieval tools\nAgentic RAG\nCore Reading:\n Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and HybridApproach (pages 1-7), Google DeepMind &amp; University of Michigan \nA Survey on Retrieval-Augmented Text Generation for Large Language Models (sections 1-7), York\nUniversity \nAdditional Reading:\nDon’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National\nChengchi University &amp; Academia Sinica&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 3, &#39;page_label&#39;: &#39;4&#39;, &#39;_id&#39;: &#39;b0070e4a-8bac-4d7e-a311-7daed4774bb1&#39;, &#39;_collection_name&#39;: &#39;2005&#39;}, page_content=&#39;Chengchi University &amp; Academia Sinica \nSelf-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, University of\nWashington, Allen Institute for AI &amp; IBM Research AI\nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through QuestionComplexity, Korea Advanced Institute of Science and Technology\nAuto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese\nAcademy of Sciences&#39;),
 Document(metadata={&#39;producer&#39;: &#39;macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext&#39;, &#39;creator&#39;: &#39;Safari&#39;, &#39;creationdate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;title&#39;: &#39;Topics Overview - LLM-based Assistants&#39;, &#39;moddate&#39;: &quot;D:20250512152829Z00&#39;00&#39;&quot;, &#39;source&#39;: &#39;./topic_overview.pdf&#39;, &#39;total_pages&#39;: 12, &#39;page&#39;: 5, &#39;page_label&#39;: &#39;6&#39;, &#39;_id&#39;: &#39;965381f6-922c-4a0c-87d7-1bb66790fd53&#39;, &#39;_collection_name&#39;: &#39;2005&#39;}, page_content=&#39;Code generation &amp; refining\nAutomated testing\nGenerated code evaluation\nCore Reading:\nLarge Language Model-Based Agents for Software Engineering: A Survey, Fudan University,\nNanyang Technological University &amp; University of Illinois at Urbana-Champaign\n CodeRL: Mastering Code Generation through Pretrained Models and Deep ReinforcementLearning (pages 1-20), Salesforce Research\nThe ART of LLM Refinement: Ask, Refine, and Trust, ETH Zurich &amp; Meta AI\nAdditional Reading:&#39;)]
</pre></div>
</div>
</div>
</div>
<h2 id="rag">2. Simple RAG 💉</h2>
<p>The basic RAG workflow is pretty straightforward: we just retrieve <em>k</em> most relevant documents and them insert them into the prompt as a part of the context.</p>
<p>For that, we will combine the skills we have obtained so far to build a LangGraph agent that receives the input, checks if the user wants to quit, then do the retrieval and generate a context-aware response if not. We will build on the basic version of our first chatbot; to add the RAG functionality, we need to add a retrieval node and modify the generation prompt to inject the retrieved documents.</p>
<p>First, we need to initialize our LLM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">MessagesPlaceholder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.tools.retriever</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_retriever_tool</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># role: restrict it from the parametric knowledge</span>
<span class="n">basic_rag_system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">You are an assistant that has access to a knowledge base. </span><span class="se">\</span>
<span class="s2">You should use the knowledge base to answer the user&#39;s questions.</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="c1"># this will add the context to the input</span>
<span class="n">context_injection_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The user is asking a question. </span><span class="se">\</span>
<span class="s2">You should answer using the following context:</span>

<span class="s2">==========================</span>
<span class="si">{context}</span>
<span class="s2">==========================</span>


<span class="s2">The user question is:</span>
<span class="si">{input}</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="c1"># finally, gather the system message, the previous messages,</span>
<span class="c1"># and the input with the context</span>
<span class="n">basic_rag_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">basic_rag_system_prompt</span><span class="p">),</span>   <span class="c1"># system message</span>
        <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;messages&quot;</span><span class="p">),</span>  <span class="c1"># previous messages</span>
        <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">context_injection_prompt</span><span class="p">)</span>  <span class="c1"># user message</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>LangGraph provides a pre-built tool to conveniently create a retriever tool. As this is basic RAG, we don’t generate queries for the retriever for now and just use the user input as the query.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BasicRAGChatbot</span><span class="p">(</span><span class="n">Chatbot</span><span class="p">):</span>

    <span class="n">_graph_path</span> <span class="o">=</span> <span class="s2">&quot;./graph_basic_rag.png&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">basic_rag_prompt</span> <span class="o">=</span> <span class="n">basic_rag_prompt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="n">k</span><span class="p">})</span>    <span class="c1"># retrieve 5 documents</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">retriever_tool</span> <span class="o">=</span> <span class="n">create_retriever_tool</span><span class="p">(</span>    <span class="c1"># and this is the tool</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="p">,</span>
            <span class="s2">&quot;retrieve_internal_data&quot;</span><span class="p">,</span>  <span class="c1"># name</span>
            <span class="s2">&quot;Search relevant information in internal documents.&quot;</span><span class="p">,</span>   <span class="c1"># description</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># graph builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">SimpleState</span><span class="p">)</span>
        <span class="c1"># add the nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond_node</span><span class="p">)</span>
        <span class="c1"># define edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># basic rag: no planning, just always retrieve</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="s2">&quot;respond&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># compile the graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_retrieve_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">SimpleState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># retrieve the context</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>  <span class="c1"># use the last message as the query</span>
        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">retriever_tool</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">user_query</span><span class="p">})</span>
        <span class="c1"># add the context to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">context</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_respond_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">SimpleState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># the workflow is designed so that the context is always the last message</span>
        <span class="c1"># and the user query is the second to last message;</span>
        <span class="c1"># finally, we will be combining the context and the user query</span>
        <span class="c1"># into a single message so we remove those two from the messages</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_rag_prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">],</span>  <span class="c1"># this goes to the message placeholder</span>
                <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">context</span><span class="p">,</span>  <span class="c1"># this goes to the user message</span>
                <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">user_query</span>    <span class="c1"># this goes to the user message</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="c1"># add the response to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[]}</span>
        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
                <span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">basic_rag_chatbot</span> <span class="o">=</span> <span class="n">BasicRAGChatbot</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">basic_rag_chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

what sessions do I have about virtual assistants?


Adaptive RAG, LangGraph
Multimodality, LangChain
Week 520.05. Lecture: Virtual Assistants Pt. 3: Multi-agent EnvironmentThis lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday /business operations in a multi-agent environment. We’ll look at how agents communicate with eachother, how their communication can be guided (both with and without involvement of a human), andthis all is used in real applications.
Key points:
Multi-agent environment
Human in the loop

Prompt Templates, LangChain
Few-shot prompting, LangChain
Week 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.
Key points:
General knowledge vs context
Knowledge indexing, retrieval &amp; ranking
Retrieval tools
Agentic RAG
Core Reading:

01.05.Ausfalltermin
Block 2: Core T opics
Part 1: Business ApplicationsWeek 306.05. Lecture: Virtual Assistants Pt. 1: ChatbotsThe first core topic concerns chatbots. We’ll discuss how chatbots are built, how they (should) handleharmful requests and you can tune it for your use case.
Key points:
LLMs alignment
Memory
Prompting &amp; automated prompt generation
Evaluation
Core Reading:
 Aligning Large Language Models with Human: A Survey (pages 1-14), Huawei Noah’s Ark Lab

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05

Built with LangGraph, LangGraph (website page)
Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLMAgents As A Daily Assistant, Delft University of Technology &amp; The University of Queensland
22.05. Lab: Multi-agent Environment
On material of session 20.05
================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

Adaptive RAG, LangGraph
Multimodality, LangChain
Week 520.05. Lecture: Virtual Assistants Pt. 3: Multi-agent EnvironmentThis lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday /business operations in a multi-agent environment. We’ll look at how agents communicate with eachother, how their communication can be guided (both with and without involvement of a human), andthis all is used in real applications.
Key points:
Multi-agent environment
Human in the loop

Prompt Templates, LangChain
Few-shot prompting, LangChain
Week 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.
Key points:
General knowledge vs context
Knowledge indexing, retrieval &amp; ranking
Retrieval tools
Agentic RAG
Core Reading:

01.05.Ausfalltermin
Block 2: Core T opics
Part 1: Business ApplicationsWeek 306.05. Lecture: Virtual Assistants Pt. 1: ChatbotsThe first core topic concerns chatbots. We’ll discuss how chatbots are built, how they (should) handleharmful requests and you can tune it for your use case.
Key points:
LLMs alignment
Memory
Prompting &amp; automated prompt generation
Evaluation
Core Reading:
 Aligning Large Language Models with Human: A Survey (pages 1-14), Huawei Noah’s Ark Lab

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05

Built with LangGraph, LangGraph (website page)
Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLMAgents As A Daily Assistant, Delft University of Technology &amp; The University of Queensland
22.05. Lab: Multi-agent Environment
On material of session 20.05


The user is asking a question. You should answer using the following context:

==========================
Adaptive RAG, LangGraph
Multimodality, LangChain
Week 520.05. Lecture: Virtual Assistants Pt. 3: Multi-agent EnvironmentThis lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday /business operations in a multi-agent environment. We’ll look at how agents communicate with eachother, how their communication can be guided (both with and without involvement of a human), andthis all is used in real applications.
Key points:
Multi-agent environment
Human in the loop

Prompt Templates, LangChain
Few-shot prompting, LangChain
Week 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.
Key points:
General knowledge vs context
Knowledge indexing, retrieval &amp; ranking
Retrieval tools
Agentic RAG
Core Reading:

01.05.Ausfalltermin
Block 2: Core T opics
Part 1: Business ApplicationsWeek 306.05. Lecture: Virtual Assistants Pt. 1: ChatbotsThe first core topic concerns chatbots. We’ll discuss how chatbots are built, how they (should) handleharmful requests and you can tune it for your use case.
Key points:
LLMs alignment
Memory
Prompting &amp; automated prompt generation
Evaluation
Core Reading:
 Aligning Large Language Models with Human: A Survey (pages 1-14), Huawei Noah’s Ark Lab

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05

Built with LangGraph, LangGraph (website page)
Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLMAgents As A Daily Assistant, Delft University of Technology &amp; The University of Queensland
22.05. Lab: Multi-agent Environment
On material of session 20.05
==========================


The user question is:
what sessions do I have about virtual assistants?

==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

You have three sessions about Virtual Assistants:

1. Week 3: Lecture: Virtual Assistants Pt. 1: Chatbots (06.05)
2. Week 4: Lecture: Virtual Assistants Pt. 2: RAG (13.05)
3. Week 5: Lecture: Virtual Assistants Pt. 3: Multi-agent Environment (20.05)

Additionally, you have a pitch session on RAG Chatbot in Week 9 (17.06) and a lab session on Multi-agent Environment (22.05) which may also be related to Virtual Assistants.


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

what are their dates?


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

Disclaimer: the reading entries are no proper citations; the bibtex references as well as detailed infosabout the authors, publish date etc. can be found under the entry links.

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!
The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis

On material of session 01.07
The last pitch will introduce an agent that will have to plan the research, generate hypotheses, find theliterature etc. for a given scientific problem. It will then have to introduce its results in form of a TODOor a guide for the researcher to start off of. Specific requirements will be released on 01.07.
Reading: see session 01.07 and session 03.07
24.07. Debate: Role of AI in Recent Years + Wrap-up
On material of session 17.07

On material of session 06.05 and session 13.05
The first pitch will be dedicated to a custom RAG chatbot that the contractors (the presentingstudents, see the infos about Pitches) will have prepared to present. The RAG chatbot will have to beable to retrieve specific information from the given documents (not from the general knowledge!) anduse it in its responses. Specific requirements will be released on 22.05.
Reading: see session 06.05, session 08.05, session 13.05, and session 15.05
19.06.Ausfalltermin


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

Based on the provided context, the dates mentioned are:

1. 06.05 (session on Virtual Assistants Pt. 1: Chatbots)
2. 13.05 (session on Virtual Assistants Pt. 2: RAG)
3. 17.06 (Pitch: RAG Chatbot)
4. 10.06 (Ausfalltermin - Week 8)
5. 12.06 (Ausfalltermin - Week 8)
6. 19.06 (Ausfalltermin)
7. 22.05 (release of specific requirements for the RAG chatbot)
8. 01.07 (session with specific requirements for the last pitch)
9. 03.07 (session related to the last pitch)
10. 17.07 (session related to the debate)
11. 24.07 (Debate: Role of AI in Recent Years + Wrap-up)


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

quit
</pre></div>
</div>
</div>
</div>
<p>As you can see, it already works pretty well, but as the retrieval goes by the user query directly, all the previous context of the conversation is not considered. To handle that, let’s add a node that would reformulate the query taking in consideration the previous interaction.</p>
<p>For that, we need an additional prompt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># finally, gather the system message, the previous messages,</span>
<span class="c1"># and the input with the context</span>
<span class="n">reformulate_query_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span>
            <span class="s2">&quot;system&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Given the previous conversation, reformulate the user query in the last message to a full question. &quot;</span>
            <span class="s2">&quot;Return only the reformulated query, without any other text.&quot;</span>
        <span class="p">),</span>   <span class="c1"># system message</span>
        <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span>  <span class="c1"># previous messages</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">BasicPlusRAGChatbot</span><span class="p">(</span><span class="n">BasicRAGChatbot</span><span class="p">):</span>

    <span class="n">_graph_path</span> <span class="o">=</span> <span class="s2">&quot;./graph_basic_plus_rag.png&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reformulate_query_prompt</span> <span class="o">=</span> <span class="n">reformulate_query_prompt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># graph builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">SimpleState</span><span class="p">)</span>
        <span class="c1"># add the nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;reformulate_query&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reformulate_query_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond_node</span><span class="p">)</span>
        <span class="c1"># define edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># basic rag: no planning, just always retrieve</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;reformulate_query&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;reformulate_query&quot;</span><span class="p">,</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="s2">&quot;respond&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># compile the graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reformulate_query_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">SimpleState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reformulate_query_prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">generated_query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="c1"># since we use the generated query instead of the user query,</span>
        <span class="c1"># we need to remove the user query from the messages</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">generated_query</span> <span class="c1"># append the generated query to the messages</span>
        <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">basic_plus_rag_chatbot</span> <span class="o">=</span> <span class="n">BasicPlusRAGChatbot</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">basic_plus_rag_chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

what do I have about RAG?


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

What information do I have about RAG?


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

On material of session 06.05 and session 13.05
The first pitch will be dedicated to a custom RAG chatbot that the contractors (the presentingstudents, see the infos about Pitches) will have prepared to present. The RAG chatbot will have to beable to retrieve specific information from the given documents (not from the general knowledge!) anduse it in its responses. Specific requirements will be released on 22.05.
Reading: see session 06.05, session 08.05, session 13.05, and session 15.05
19.06.Ausfalltermin

Retrieval tools
Agentic RAG
Core Reading:
 Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and HybridApproach (pages 1-7), Google DeepMind &amp; University of Michigan 
A Survey on Retrieval-Augmented Text Generation for Large Language Models (sections 1-7), York
University 
Additional Reading:
Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National
Chengchi University &amp; Academia Sinica

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 5 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Reading:
How to load PDFs, LangChain
Text splitters, LangChain
Embedding models, LangChain
Vector stores, LangChain
Retrievers, LangChain
 Retrieval augmented generation (RAG), LangChain
 LangGraph Quickstart: Build a Basic Chatbot (part 2), LangGraph
 Agentic RAG, LangGraph
Adaptive RAG, LangGraph
Multimodality, LangChain

Prompt Templates, LangChain
Few-shot prompting, LangChain
Week 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.
Key points:
General knowledge vs context
Knowledge indexing, retrieval &amp; ranking
Retrieval tools
Agentic RAG
Core Reading:

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05

Chengchi University &amp; Academia Sinica 
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, University of
Washington, Allen Institute for AI &amp; IBM Research AI
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through QuestionComplexity, Korea Advanced Institute of Science and Technology
Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese
Academy of Sciences

Disclaimer: the reading entries are no proper citations; the bibtex references as well as detailed infosabout the authors, publish date etc. can be found under the entry links.


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

Based on the provided context, here&#39;s what I found about RAG (Retrieval Augmented Generation):

1. **Definition and Purpose**: RAG is a technique used in large language models to retrieve specific information from a given knowledge base and use it in its responses.
2. **Requirements**: For the pitch on June 17th, a custom RAG chatbot needs to be prepared, which can retrieve specific information from given documents (not from general knowledge) and use it in its responses. Specific requirements will be released on May 22nd.
3. **Reading Materials**: There are several reading materials available on RAG, including:
	* &quot;Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach&quot; by Google DeepMind &amp; University of Michigan (pages 1-7)
	* &quot;A Survey on Retrieval-Augmented Text Generation for Large Language Models&quot; by York University (sections 1-7)
	* &quot;Don&#39;t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks&quot; by National Chengchi University &amp; Academia Sinica
4. **Types of RAG**: There are different types of RAG, including:
	* Agentic RAG
	* Adaptive RAG
	* Auto-RAG
	* Self-RAG
5. **Tools and Resources**: LangChain provides various tools and resources for RAG, including text splitters, embedding models, vector stores, retrievers, and prompt templates.
6. **Lecture and Pitch**: There will be a lecture on May 13th about Virtual Assistants Pt. 2: RAG, which will expand the scope of chatbot functionality and teach it to refer to a custom knowledge base. The pitch on June 17th will be dedicated to a custom RAG chatbot.

I hope this information helps! Let me know if you have any further questions.


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

what is the name of the course?


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

What is the name of the course being referred to in the context of the RAG chatbot and the upcoming pitch on June 17th?


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

On material of session 06.05 and session 13.05
The first pitch will be dedicated to a custom RAG chatbot that the contractors (the presentingstudents, see the infos about Pitches) will have prepared to present. The RAG chatbot will have to beable to retrieve specific information from the given documents (not from the general knowledge!) anduse it in its responses. Specific requirements will be released on 22.05.
Reading: see session 06.05, session 08.05, session 13.05, and session 15.05
19.06.Ausfalltermin

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05

On material of session 17.07
The course will be concluded by the final debates, after which a short Q&amp;A session will be held.
Debate topics:
LLM Behavior: Evidence of Awareness or Illusion of Understanding?
Should We Limit the Usage of AI?
Reading: see session 17.07
Copyright © 2025, Maksim ShmaltsMade with Sphinx and @pradyunsg&#39;s Furo

Prompt Templates, LangChain
Few-shot prompting, LangChain
Week 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.
Key points:
General knowledge vs context
Knowledge indexing, retrieval &amp; ranking
Retrieval tools
Agentic RAG
Core Reading:

12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 4 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
08.05. Lab: Basic LLM-based Chatbot
On material of session 06.05
In this lab, we’ll build a chatbot and try different prompts and settings to see how it affects the output.
Reading:
 Build a Chatbot, LangChain
 LangGraph Quickstart: Build a Basic Chatbot (parts 1, 3), LangGraph
 How to add summary of the conversation history, LangGraph

19.06.Ausfalltermin
Week 1024.06. Pitch: Handling Customer Requests in a Multi-agent Environment
On material of session 20.05

Academy of Sciences
Querying Databases with Function Calling, Weaviate, Contextual AI &amp; Morningstar
15.05. Lab: RAG Chatbot
On material of session 13.05
In this lab, we’ll expand the functionality of the chatbot built at the last lab to connect it to user-specificinformation.


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

The name of the course being referred to in the context of the RAG chatbot and the upcoming pitch on June 17th is &quot;LLM-based Assistants&quot;.


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

quit
</pre></div>
</div>
</div>
</div>
<h2 id="adv_rag">3. Advanced RAG 😎</h2>
<p>Now we can move to a more complicated implementation. We will no make an iterative RAG chatbot: this chatbot will retrieve contexts iteratively and decide at each step whether the chunks retrieved so far are sufficient to answer the question; the answer is generated only when the retrieved contexts are enough.</p>
<p>Basically, we have almost everything we need to implement an iterative RAG pipeline. We only need to add three more nodes:</p>
<ol class="arabic simple">
<li><p>A node to generate search queries for the index: now we won’t use the user query but specifically generate queries for the index.</p></li>
<li><p>A decision node, in which the LLM will decide whether the context retrieved so far is enough to proceed to generation of the response.</p></li>
<li><p>Query transformer that will reformulate the query to retrieve further chunks when it’s needed.</p></li>
</ol>
<p>As a useful addition, we will also add LLM-based filtering of the retrieved documents to filter out the documents that are semantically similar to the query but are not really relevant for answering the question.</p>
<p>Thus, we need to add 4 nodes totally.</p>
<p>We will start with the grader that will output a binary score for the relevance: <code class="docutils literal notranslate"><span class="pre">True</span></code>(relevant) or <code class="docutils literal notranslate"><span class="pre">False</span></code> (irrelevant). To implement this functionality, we’ll bind the LLM to a true/false structured output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">YesNoVerdict</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">verdict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Boolean answer to the given binary question.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will also need to transform the state to accumulate the contexts gathered so far.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AdvancedRAGState</span><span class="p">(</span><span class="n">SimpleState</span><span class="p">):</span> <span class="c1"># &quot;messages&quot; is already defined in SimpleRAGState</span>
    <span class="n">contexts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Document</span><span class="p">]]</span>    <span class="c1"># this is the list of retrieved documents, one list per retrieval</span>
</pre></div>
</div>
</div>
</div>
<p>And we also need prompts to filter the documents, to decide whether the contexts are supportive enough, and to transform the query if not.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_query_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The user is asking a question. </span><span class="se">\</span>
<span class="s2">You have an access to a knowledge base. </span><span class="se">\</span>
<span class="s2">Your task is to generate a query that will retrieve the most relevant documents </span><span class="se">\</span>
<span class="s2">from the knowledge base to answer the user question. </span><span class="se">\</span>
<span class="s2">Return the query only, without any other text.</span>


<span class="s2">The user question is:</span>
<span class="si">{input}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">generate_query_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">generate_query_template</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">context_relevant_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The user is asking a question. </span><span class="se">\</span>
<span class="s2">For answering the question, you colleague have retrieved the following document:</span>


<span class="s2">===========================</span>
<span class="si">{context}</span>
<span class="s2">===========================</span>


<span class="s2">You task is to assess whether this document is relevant to answer the user question. </span><span class="se">\</span>
<span class="s2">Relevant means that the document contains specific information should be used </span><span class="se">\</span>
<span class="s2">directly to answer the user question. </span><span class="se">\</span>
<span class="s2">Return True if the document is relevant, and False otherwise.</span>


<span class="s2">The user question is:</span>
<span class="si">{input}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">context_relevant_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">context_relevant_template</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">contexts_sufficient_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The user is asking a question. </span><span class="se">\</span>

<span class="s2">For answering the question, you colleague have retrieved the following document:</span>


<span class="s2">===========================</span>
<span class="si">{contexts_str}</span>
<span class="s2">===========================</span>


<span class="s2">You task is to assess whether the retrieved documents contain an answer the user question. </span><span class="se">\</span>
<span class="s2">Return True if the documents are sufficient, and False otherwise.</span>


<span class="s2">The user question is:</span>
<span class="si">{input}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">contexts_sufficient_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">contexts_sufficient_template</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transform_query_template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">The user is asking a question. </span><span class="se">\</span>

<span class="s2">For answering the question, your colleague the following document has been retrieved:</span>


<span class="s2">===========================</span>
<span class="si">{contexts_str}</span>
<span class="s2">===========================</span>


<span class="s2">To retrieve these documents, the following query has been used:</span>
<span class="si">{query}</span>


<span class="s2">However, the query is not very good so the retrieved documents were not helpful. </span><span class="se">\</span>
<span class="s2">Your task is to transform the query into a better one, so that the retrieved documents are more relevant. </span><span class="se">\</span>
<span class="s2">Return the transformed query only, without any other text.</span>


<span class="s2">The user question is:</span>
<span class="si">{input}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">transform_query_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">transform_query_template</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">IterativeRAGChatbot</span><span class="p">(</span><span class="n">BasicPlusRAGChatbot</span><span class="p">):</span>

    <span class="n">_graph_path</span> <span class="o">=</span> <span class="s2">&quot;./graph_iterative_rag.png&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_generations</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_generations</span> <span class="o">=</span> <span class="n">max_generations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">boolean_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">YesNoVerdict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generate_query_prompt</span> <span class="o">=</span> <span class="n">generate_query_prompt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_relevant_grader</span> <span class="o">=</span> <span class="n">context_relevant_prompt</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">boolean_llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">contexts_sufficient_grader</span> <span class="o">=</span> <span class="n">contexts_sufficient_prompt</span> <span class="o">|</span> <span class="bp">self</span><span class="o">.</span><span class="n">boolean_llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transform_query_prompt</span> <span class="o">=</span> <span class="n">transform_query_prompt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># graph builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">AdvancedRAGState</span><span class="p">)</span>
        <span class="c1"># add the nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;reformulate_query&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reformulate_query_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;generate_query&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_query_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_retrieve_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;filter_documents&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filter_documents_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;transform_query&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform_query_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond_node</span><span class="p">)</span>
        <span class="c1"># define edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># basic rag: no planning, just always retrieve</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;reformulate_query&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;reformulate_query&quot;</span><span class="p">,</span> <span class="s2">&quot;generate_query&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;generate_query&quot;</span><span class="p">,</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;retrieve&quot;</span><span class="p">,</span> <span class="s2">&quot;filter_documents&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>
            <span class="s2">&quot;filter_documents&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_contexts_sufficient_node</span><span class="p">,</span>
            <span class="p">{</span>
                <span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;transform_query&quot;</span><span class="p">,</span>
                <span class="kc">True</span><span class="p">:</span> <span class="s2">&quot;respond&quot;</span><span class="p">,</span>
                <span class="kc">None</span><span class="p">:</span> <span class="n">END</span>   <span class="c1"># max generations reached</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;transform_query&quot;</span><span class="p">,</span> <span class="s2">&quot;retrieve&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># compile the graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_query_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">AdvancedRAGState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>    
        <span class="n">user_query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>  <span class="c1"># that will be the reformulated user query</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">generate_query_prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">user_query</span><span class="p">})</span>
        <span class="n">search_query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">search_query</span>
        <span class="p">}</span>

    <span class="c1"># now store the contexts in the separate field</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_retrieve_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">AdvancedRAGState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>    
        <span class="c1"># retrieve the context</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>  <span class="c1"># that will be the generated query</span>
        <span class="c1"># now use the retriever directly to get a list of documents and not a combined string</span>
        <span class="n">contexts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="c1"># add the context to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;contexts&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;contexts&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">contexts</span><span class="p">]</span>  <span class="c1"># could have also used `Annotated` here</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_filter_documents_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">AdvancedRAGState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>  <span class="c1"># that will be the generated query</span>
        <span class="c1"># since the retrieved documents are graded at the same step,</span>
        <span class="c1"># we only need to pass the last batch of documents</span>
        <span class="n">contexts</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;contexts&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># will be replaced with the filtered ones</span>
        <span class="c1"># grade each document separately and only keep the relevant ones</span>
        <span class="n">relevant_contexts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">contexts</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Grading document:</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
            <span class="n">verdict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_relevant_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="n">context</span><span class="o">.</span><span class="n">page_content</span><span class="p">,</span>    <span class="c1"># this is a Document object</span>
                    <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">query</span>
                <span class="p">}</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Verdict: </span><span class="si">{</span><span class="n">verdict</span><span class="o">.</span><span class="n">verdict</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">=====================</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">verdict</span><span class="o">.</span><span class="n">verdict</span><span class="p">:</span>    <span class="c1"># boolean value according to the Pydantic model</span>
                <span class="n">relevant_contexts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;contexts&quot;</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;contexts&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">relevant_contexts</span><span class="p">]</span>  <span class="c1"># could have also used `Annotated` here</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_contexts_sufficient_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">AdvancedRAGState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>   <span class="c1"># that will be the reformulated user query, -1 is the generated search query</span>
        <span class="n">all_contexts</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;contexts&quot;</span><span class="p">]</span>
        <span class="c1"># flatten and transform the list of lists into a single list</span>
        <span class="n">contexts</span> <span class="o">=</span> <span class="p">[</span><span class="n">context</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">all_contexts</span> <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
        <span class="n">contexts_str</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">context</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">contexts</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Deciding whether the documents are sufficient&quot;</span><span class="p">)</span>
        <span class="n">verdict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">contexts_sufficient_grader</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                <span class="p">{</span>
                <span class="s2">&quot;contexts_str&quot;</span><span class="p">:</span> <span class="n">contexts_str</span><span class="p">,</span>    <span class="c1"># this is a Document object</span>
                <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">query</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Verdict: </span><span class="si">{</span><span class="n">verdict</span><span class="o">.</span><span class="n">verdict</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">=====================</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">verdict</span><span class="o">.</span><span class="n">verdict</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_contexts</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_generations</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># will route to END</span>
        <span class="k">return</span> <span class="n">verdict</span><span class="o">.</span><span class="n">verdict</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_transform_query_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">AdvancedRAGState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># since we will be replacing the user query with the transformed one,</span>
        <span class="c1"># we need to remove the old query</span>
        <span class="n">search_query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>   <span class="c1"># this is the generated search query</span>
        <span class="c1"># the the reformulated user query is the last message</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="n">all_contexts</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;contexts&quot;</span><span class="p">]</span>
        <span class="c1"># flatten and transform the list of lists into a single list</span>
        <span class="n">contexts</span> <span class="o">=</span> <span class="p">[</span><span class="n">context</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">all_contexts</span> <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
        <span class="n">contexts_str</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">context</span><span class="o">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">contexts</span><span class="p">])</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_query_prompt</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;contexts_str&quot;</span><span class="p">:</span> <span class="n">contexts_str</span><span class="p">,</span>
                <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="n">search_query</span><span class="p">,</span>
                <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">user_query</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="n">transformed_search_query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">transformed_search_query</span>   <span class="c1"># this will append the transformed query to the messages</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;contexts&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;suka&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]:</span>
                <span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make small k to ensure one retrieval is not enough</span>
<span class="n">iterative_rag_chatbot</span> <span class="o">=</span> <span class="n">IterativeRAGChatbot</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iterative_rag_chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

what are the key point of the next lecture after the one on 13.05


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

What are the key points of the next lecture after the one scheduled for May 13th?


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

lecture schedule May 13th next lecture key points


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

lecture schedule May 13th next lecture key points


Grading document:

 12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!
The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis
Verdict: False


=====================


Grading document:

 12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
Week 8: Having Some Rest10.06.Ausfalltermin
12.06.Ausfalltermin
Week 917.06. Pitch: RAG Chatbot
On material of session 06.05 and session 13.05
Verdict: False


=====================


Deciding whether the documents are sufficient
Verdict: False


=====================


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

lecture schedule May 13th next lecture key points


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th key points summary


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th key points summary


Grading document:

 12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!
The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis
Verdict: False


=====================


Grading document:

 : those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji
Verdict: False


=====================


Deciding whether the documents are sufficient
Verdict: False


=====================


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th key points summary


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th summary of key points


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th summary of key points


Grading document:

 : those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji
Verdict: False


=====================


Grading document:

 12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!
The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis
Verdict: False


=====================


Deciding whether the documents are sufficient
Verdict: False


=====================


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th summary of key points


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th key points summary notes


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th key points summary notes


Grading document:

 12.05.25, 17:28Topics Overview - LLM-based Assistants
Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html
To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!
The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis
Verdict: False


=====================


Grading document:

 : those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji
Verdict: False


=====================


Deciding whether the documents are sufficient
Verdict: False


=====================


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

next lecture after May 13th key points summary notes
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./sessions/block2_core_topics/pt1_business/2005"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../2705/2705.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">27.05. Multi-agent Environment 👾🤖👾</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../1505/1505.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">15.05. RAG Chatbot Pt. 1 📚</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, Maksim Shmalts
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
          <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
</svg>
</a>
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants/issues/new" aria-label="Issues"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
  <path d="M8 15A7 7 0 1 0 8 1a7 7 0 0 0 0 14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"/>
  <path d="M7.002 11a1 1 0 1 1 2 0 1 1 0 0 1-2 0zm.1-6.995a.905.905 0 0 1 1.8 0l-.35 4.5a.55.55 0 0 1-1.1 0l-.35-4.5z"/>
</svg>
</a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">20.05. RAG Chatbot Pt. 2 📚</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a><ul>
<li><a class="reference internal" href="#loading">Loading</a></li>
<li><a class="reference internal" href="#chunking">Chunking</a></li>
<li><a class="reference internal" href="#convert-to-embeddings">Convert to Embeddings</a></li>
<li><a class="reference internal" href="#indexing">Indexing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    </body>
</html>