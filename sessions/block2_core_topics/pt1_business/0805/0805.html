<!doctype html>
<html class="no-js" lang="en" data-content_root="../../../../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../../../../genindex.html" /><link rel="search" title="Search" href="../../../../search.html" /><link rel="next" title="13.05. Virtual Assistants Pt. 2: RAG" href="../1305.html" /><link rel="prev" title="06.05. Virtual Assistants Pt. 1: Chatbots" href="../0605.html" />

    <!-- Generated with Sphinx 7.4.7 and Furo 2024.08.06 -->
        <title>08.05. Basic LLM-based Chatbot 🤖 - LLM-based Assistants</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../../index.html"><div class="brand">LLM-based Assistants</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../../index.html">
  
  
  <span class="sidebar-brand-text">LLM-based Assistants</span>
  
</a><form class="sidebar-search-container" method="get" action="../../../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Infos and Stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/topic_overview.html">Topics Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/formats/debates.html">Debates</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/formats/pitches.html">Pitches</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../infos/llm_inference_guide/README.html">LLM Inference Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 1: Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../block1_intro/2204.html">22.04. LLMs as a Form of Intelligence vs LLMs as Statistical Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block1_intro/2404.html">24.04. LLM &amp; Agent Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block1_intro/2904/2904.html">29.04. Intro to LangChain 🦜🔗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 2: Core Topics | Part 1: Business Applications</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0605.html">06.05. Virtual Assistants Pt. 1: Chatbots</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">08.05. Basic LLM-based Chatbot 🤖</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1305.html">13.05. Virtual Assistants Pt. 2: RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1505/1505.html">15.05. RAG Chatbot Pt. 1 📚</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2005/2005.html">20.05. RAG Chatbot Pt. 2 📚</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2705/2705.html">27.05. Multi-agent Environment 👾🤖👾</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2205.html">22.05. Virtual Assistants Pt. 3: Multi-agent Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0306.html">03.06. LLMs in Software Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0506/0506.html">05.06. LLM-powered Website 💻</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1706.html">17.06. Other Business Applications: Game Design, Financial Analysis etc.</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 2: Core Topics | Part 2: Applications in Science</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../pt2_science/under_development.html">Under development</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 3: Wrap-up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/1007/README.html">10.07. <em>Pitch</em>: RAG Chatbot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/1507/README.html">17.07. <em>Pitch</em>: Handling Customer Requests in a Multi-agent Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../block3_wrapup/1707/README.html">17.07. <em>Pitch</em>: Agent for Web Resumes</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="basic-llm-based-chatbot">
<h1>08.05. Basic LLM-based Chatbot 🤖<a class="headerlink" href="#basic-llm-based-chatbot" title="Link to this heading">¶</a></h1>
<p>📍 <a class="reference external" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants/tree/main/llm-based-assistants/sessions/block2_core_topics/pt1_business/0805">Download notebook and session files</a></p>
<p>In today’l lab, we will be making a basic LLM-based chatbot with LangChain and LangGraph. We will try a few different settings and see how they affect the behavior of the chatbot.</p>
<p>Our plan for today:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#recap"><span class="xref myst">Recap: Messages and Chat Models</span></a></p></li>
<li><p><a class="reference internal" href="#basic_chatbot"><span class="xref myst">Basic Chatbot</span></a></p></li>
<li><p><a class="reference internal" href="#langgraph"><span class="xref myst">Switching to LangGraph</span></a></p></li>
<li><p><a class="reference internal" href="#checkpointing"><span class="xref myst">Checkpointing</span></a></p></li>
<li><p><a class="reference internal" href="#memory"><span class="xref myst">Memory Enhancement</span></a></p></li>
</ul>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h2>
<p>To start with the tutorial, complete the steps <a class="reference internal" href="#../../../infos/llm_inference_guide/README.md#prerequisites"><span class="xref myst">Prerequisites</span></a>, <a class="reference internal" href="#../../../infos/llm_inference_guide/README.md#environment-setup"><span class="xref myst">Environment Setup</span></a>, and <a class="reference internal" href="#../../../infos/llm_inference_guide/README.md#getting-api-key"><span class="xref myst">Getting API Key</span></a> from the <a class="reference internal" href="#../../../infos/llm_inference_guide/README.md"><span class="xref myst">LLM Inference Guide</span></a>.</p>
<p>After that, you need to install a few more packages:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">langgraph</span> <span class="n">pyppeteer</span>
</pre></div>
</div>
<h2 id="recap">1. Recap: Messages and Chat Models 💬</h2>
<p><code class="docutils literal notranslate"><span class="pre">ChatModel</span></code>s provide a simple and intuitive interface for you to make inference to LLMs from different providers. It accepts a sequence of messages and returns you the generation from the LLM. Different types of messages help control the behavior of the model in multi-turn settings.</p>
<p>There are 3 basic message types:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SystemMessage</span></code>: sets LLM role and describes the desired behavior</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HumanMessage</span></code>: user input</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AIMessage</span></code>: model output</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_nvidia_ai_endpoints</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatNVIDIA</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.rate_limiters</span><span class="w"> </span><span class="kn">import</span> <span class="n">InMemoryRateLimiter</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># read system variables</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>

<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>    <span class="c1"># that loads the .env file variables into os.environ</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">SystemMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful and honest assistant.&quot;</span> <span class="c1"># role</span>
    <span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="s2">&quot;How big is the distance between the Earth and the Moon?&quot;</span> <span class="c1"># user request</span>
    <span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># choose any model, catalogue is available under https://build.nvidia.com/models</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s2">&quot;meta/llama-3.3-70b-instruct&quot;</span>

<span class="c1"># this rate limiter will ensure we do not exceed the rate limit</span>
<span class="c1"># of 40 RPM given by NVIDIA</span>
<span class="n">rate_limiter</span> <span class="o">=</span> <span class="n">InMemoryRateLimiter</span><span class="p">(</span>
    <span class="n">requests_per_second</span><span class="o">=</span><span class="mi">35</span> <span class="o">/</span> <span class="mi">60</span><span class="p">,</span>  <span class="c1"># 35 requests per minute to be sure</span>
    <span class="n">check_every_n_seconds</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># wake up every 100 ms to check whether allowed to make a request,</span>
    <span class="n">max_bucket_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>  <span class="c1"># controls the maximum burst size</span>
<span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatNVIDIA</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">MODEL_NAME</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;NVIDIA_API_KEY&quot;</span><span class="p">),</span> 
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>   <span class="c1"># ensure reproducibility,</span>
    <span class="n">rate_limiter</span><span class="o">=</span><span class="n">rate_limiter</span>  <span class="c1"># bind the rate limiter</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;The average distance between the Earth and the Moon is approximately 384,400 kilometers (238,900 miles). This distance is constantly changing due to the elliptical shape of the Moon&#39;s orbit around the Earth.\n\nAt its closest point, called perigee, the distance is about 356,400 kilometers (221,500 miles), and at its farthest point, called apogee, the distance is about 405,500 kilometers (252,000 miles).\n\nIt&#39;s worth noting that the Moon&#39;s orbit is not a perfect circle and its distance from Earth varies slightly over the course of a month. However, the average distance of 384,400 kilometers is a commonly cited and useful figure for understanding the scale of our celestial neighborhood.&quot;
</pre></div>
</div>
</div>
</div>
<h2 id="basic_chatbot">2. Basic Chatbot 🤖</h2>
<p>Almost there! We already have an LLM to interact with the user, now we should wrap it into some kind of interface.</p>
<p>For the sake of simplicity, we will now limit ourselves to the most basic <code class="docutils literal notranslate"><span class="pre">while</span></code> loop until the user says <code class="docutils literal notranslate"><span class="pre">&quot;quit&quot;</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">respond</span><span class="p">(</span><span class="n">user_query</span><span class="p">):</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">SystemMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful and honest assistant.&quot;</span> <span class="c1"># role</span>
        <span class="p">),</span>
        <span class="n">HumanMessage</span><span class="p">(</span>
            <span class="n">content</span><span class="o">=</span><span class="n">user_query</span> <span class="c1"># user request</span>
        <span class="p">)</span>
    <span class="p">]</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_chatbot</span><span class="p">():</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Your message: &quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;You: </span><span class="si">{</span><span class="n">user_query</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">user_query</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;quit&quot;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Chatbot: Bye!&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">respond</span><span class="p">(</span><span class="n">user_query</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chatbot: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run_chatbot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>You: hi
Chatbot: It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.
You: what is the 3rd planet from the Sun?
Chatbot: The 3rd planet from the Sun is Earth.
You: and 4?
Chatbot: It seems like we just started our conversation, and I&#39;m not sure what &quot;and 4&quot; refers to. Could you please provide more context or clarify what you&#39;re asking? I&#39;m here to help and want to make sure I understand your question correctly.
You: quit
Chatbot: Bye!
</pre></div>
</div>
</div>
</div>
<p>As you can see, the chatbot has an access only to the last message you’re passing to it, so you cannot have an actual coherent <em>conversation</em>. An easy workaround would be to pass the entire message history to the chatbot so it is aware of the previous messages. Here’s when the distinction between the <code class="docutils literal notranslate"><span class="pre">HumanMessage</span></code> and <code class="docutils literal notranslate"><span class="pre">AIMessage</span></code> is crucial: the LLM needs to know what was generated by whom.</p>
<p>Let’s adjust our function to keep track of the entire message history. Since we will be keeping the entire history to the chatbot, it makes sense to add the system message only once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">respond</span><span class="p">(</span><span class="n">user_query</span><span class="p">,</span> <span class="n">previous_messages</span><span class="p">):</span>
    <span class="n">human_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">user_query</span>
    <span class="p">)</span>
    <span class="n">previous_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">human_message</span><span class="p">)</span> <span class="c1"># modify in place</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">previous_messages</span><span class="p">)</span>    <span class="c1"># history + user query</span>
    <span class="n">previous_messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>  <span class="c1"># modify in place</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">run_chatbot</span><span class="p">():</span>
    <span class="n">system_message</span> <span class="o">=</span> <span class="n">SystemMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful and honest assistant.&quot;</span> <span class="c1"># role</span>
    <span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">system_message</span><span class="p">]</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Your message: &quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;You: </span><span class="si">{</span><span class="n">user_query</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">user_query</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;quit&quot;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Chatbot: Bye!&quot;</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">respond</span><span class="p">(</span><span class="n">user_query</span><span class="p">,</span> <span class="n">messages</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chatbot: </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run_chatbot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>You: hi
Chatbot: It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.
You: what is the 3rd planet from the Sun?
Chatbot: The 3rd planet from the Sun is Earth.
You: and 4?
Chatbot: The 4th planet from the Sun is Mars.
You: What color is it?
Chatbot: Mars is often referred to as the &quot;Red Planet&quot; due to its reddish appearance, which is caused by iron oxide (or rust) in the planet&#39;s soil and rocks.
You: quit
Chatbot: Bye!
</pre></div>
</div>
</div>
</div>
<p>However, this solution is not scalable and robust: if you interact with the chatbot long enough, passing the whole message history becomes fairly (and unnecessary) expensive, the chatbot takes longer to respond, and the context window can be exceeded leading to errors. We will address that in <a class="reference internal" href="#memory"><span class="xref myst">Memory Enhancement</span></a>, and for now we’ll keep going with the basic variant.</p>
<h2 id="langgraph">3. Switching to LangGraph 🕸️</h2>
<p><a class="reference external" href="https://www.langchain.com/langgraph">LangGraph</a> is a powerful framework for building LLM-based applications in a graph-based manner. It extends LangChain by introducing graph-based workflows where each node can represent an agent, a tool, or a decision point. With support for branching logic, memory, backtracking, and more, LangGraph makes it easier to manage complex interactions and long-running processes. It’s especially useful for developers creating LLM-based multi-agent systems that need to reason, plan, or collaborate (both with and without human interaction).</p>
<p>While we are not building a complex system yet, there are a few reasons to switch to LangGraph already:</p>
<ol class="arabic simple">
<li><p>Easier data transfer. LangGraph comes with a builtin mechanism for managing messages, properties, metadata etc. – in one word, <em>state</em> of the system. For example, we will not have to add the messages to the history manually.</p></li>
<li><p>Persistence. LangGraph creates local snapshots of the system state, which allows it to pick up where it left off <strong>between</strong> the interactions.</p></li>
<li><p>Graph structure. We can already use the graph fashion LangGraph provides to easily manage the workflow. We won’t be using the stupid <code class="docutils literal notranslate"><span class="pre">while</span></code> loop anymore!</p></li>
<li><p>Scalability and modularity. Even though our chatbot is basic yet, later we will expand it and build other complex pipelines, which LangGraph is just perfect for. Thus, if we build the chatbot with LangGraph now, we will be able to improve and scale it much much easier just <strong>connecting</strong> the necessary logic.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Annotated</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">TypedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.graph.message</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_messages</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.runnables.graph</span><span class="w"> </span><span class="kn">import</span> <span class="n">MermaidDrawMethod</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nest_asyncio</span>
<span class="n">nest_asyncio</span><span class="o">.</span><span class="n">apply</span><span class="p">()</span>  <span class="c1"># this is needed to draw the PNG in Jupyter</span>
</pre></div>
</div>
</div>
</div>
<p>The first concept you should get familiar with is the <em>state</em> of the system. LangGraph builds pipelines as <em>state machines</em>, where at each given moment of the time, the system is at a certain <em>node</em>, has a certain <em>state</em>, and makes a <em>transition</em> based on the defined <em>edges</em>. As any state machine, a LangGraph pipeline has a <em>start node</em>, <em>intermediate nodes</em>, and an <em>end node</em>. When you pass the input to the system, it comes from the start node through the intermediate nodes to the end node, after which the system exits. At any transition, LangGraph transfers the state between the nodes. The state contains all the information you configured it to store: messages, properties etc. <strong>Each node receives the current state and returns the updated state</strong>. Thus, the system is always aware of what the current situation is.</p>
<p>A state is defined as a <code class="docutils literal notranslate"><span class="pre">TypedDict</span></code> with all the fields you want it to have (you can add extra fields later in workflow). If you add a function to the type declaration within the <code class="docutils literal notranslate"><span class="pre">Annotated</span></code> class, then instead of rewriting the state at each graph update, LangGraph will update it in correspondence with this function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
    <span class="c1"># `messages` is a list of messages of any kind. The `add_messages` function</span>
    <span class="c1"># in the annotation defines how this state key should be updated</span>
    <span class="c1"># (in this case, it appends messages to the list, rather than overwriting them)</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">BaseMessage</span><span class="p">],</span> <span class="n">add_messages</span><span class="p">]</span>
    <span class="c1"># Since we didn&#39;t define a function to update it, it will be rewritten at each transition</span>
    <span class="c1"># with the value you provide</span>
    <span class="n">n_turns</span><span class="p">:</span> <span class="nb">int</span>    <span class="c1"># just for demonstration</span>
    <span class="n">language</span><span class="p">:</span> <span class="nb">str</span>    <span class="c1"># just for demonstration</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">StateGraph</span></code> is the frame of the system, it will bear all the nodes and transitions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s define the nodes for our chatbot. In our case, we need three nodes:</p>
<ol class="arabic simple">
<li><p>The input receival node. It will prompt the user for the input and store it in the messages for further interaction with the LLM.</p></li>
<li><p>The router node. It performs the check whether the user wants to exit.</p></li>
<li><p>The chatbot node. It will receive the input if the user has not quit input, pass it to the LLM, and return the generation.</p></li>
</ol>
<p>Each node is a Python function that (typically) accepts the single argument: the state. To update the state, the function should return a <code class="docutils literal notranslate"><span class="pre">dict</span></code> with the keys corresponding to the state keys, with the <strong>updated</strong> values. That is, if you for example need to update only a single property in the state while the rest should remain the same, you only need to return a dict with this specific key and leave the rest out. Also remember that the update behavior depends on how you defined your state class (will be rewritten by default or processed by a function if given in <code class="docutils literal notranslate"><span class="pre">Annotated</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">input_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="n">user_query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Your message: &quot;</span><span class="p">)</span>
    <span class="n">human_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_query</span><span class="p">)</span>
    <span class="n">n_turns</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;n_turns&quot;</span><span class="p">]</span>
    <span class="c1"># add the input to the messages</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">human_message</span><span class="p">,</span>   <span class="c1"># this will append the response to the messages</span>
        <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="n">n_turns</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># and this will rewrite the number of turns</span>
        <span class="c1"># &quot;language&quot;: ...  # we don&#39;t update this field so we just leave it out</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>After we defined the node, we can hang it onto our state. To do so, we need to bind it to the graph builder with an arbitrary name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">input_node</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;langgraph.graph.state.StateGraph at 0x10953ce30&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">respond_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>    <span class="c1"># will already contain the user query</span>
    <span class="n">n_turns</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;n_turns&quot;</span><span class="p">]</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="c1"># add the response to the messages</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">,</span>   <span class="c1"># this will append the response to the messages</span>
        <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="n">n_turns</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># and this will rewrite the number of turns</span>
        <span class="c1"># &quot;language&quot;: ...  # we don&#39;t update this field so we just leave it out</span>
    <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="n">respond_node</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;langgraph.graph.state.StateGraph at 0x10953ce30&gt;
</pre></div>
</div>
</div>
</div>
<p>Now decision nodes – those responsible for branching – work a bit differently. They also receive the state of the system, but instead of the updated state they return the <strong>destination</strong> – meaning the node that should be executed next based on the logic implemented in this router node. The destination should be either a name we have given to a node (as <code class="docutils literal notranslate"><span class="pre">&quot;respond&quot;</span></code> in our case), or a LangGraph-predefined start or end state: <code class="docutils literal notranslate"><span class="pre">START</span></code>, <code class="docutils literal notranslate"><span class="pre">END</span></code>, respectively. Alternatively, you can return arbitrary values, but then you will have to map them to the actual destinations when defining the conditional edges.</p>
<p>The decision nodes are not added to the graph builder but are used for branching when defying edges (below).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">is_quitting_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># check if the user wants to quit</span>
    <span class="n">user_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
    <span class="k">return</span> <span class="n">user_message</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;quit&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>We now have all the building blocks for our chatbot. The only thing that is left is to assemble the system. For that, we should link the start node, the intermediate nodes, and the end node with edges.</p>
<p>There are two basic types of edges:</p>
<ol class="arabic simple">
<li><p>Direct edges. Just link two states unconditionally.</p></li>
<li><p>Conditional edges. Link the source edge to the destination based on a condition implemented in a decision node.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># that says: when you start, go straight to the &quot;input&quot; node to receive the first message</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span> <span class="c1"># equivalent to `graph_builder.set_entry_point(&quot;input&quot;)`</span>
<span class="c1"># that says: after you have received the first message, check if the user wants to quit</span>
<span class="c1"># and then go either to the &quot;respond&quot; node if you the function returns `False``</span>
<span class="c1"># or to the END node if the function returns `True`;</span>
<span class="c1"># depending on what the decision node returns;</span>
<span class="c1"># note that since it is a decision node, we didn&#39;t add it to the graph builder</span>
<span class="c1"># and we do not refer to it by its name and just pass it as a function</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
<span class="c1"># `is_quitting_node` will create edges to the possible destinations,</span>
<span class="c1"># so we don&#39;t have to specify those;</span>
<span class="c1"># finally, after the response, we go back to the &quot;input&quot; node</span>
<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
<span class="c1"># since the decision node decides when to quit, we don&#39;t need to specify the end node</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;langgraph.graph.state.StateGraph at 0x10953ce30&gt;
</pre></div>
</div>
</div>
</div>
<p>Finally, we can compile the graph and see how it looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chatbot</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># unstable</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">display</span><span class="p">(</span>
        <span class="n">Image</span><span class="p">(</span>
            <span class="n">chatbot</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">(</span>
                <span class="n">draw_method</span><span class="o">=</span><span class="n">MermaidDrawMethod</span><span class="o">.</span><span class="n">PYPPETEER</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/6afed1db6f3ec37be9dc435bab67227ee19e793168fb64ce5132ed6827faec54.png" src="../../../../_images/6afed1db6f3ec37be9dc435bab67227ee19e793168fb64ce5132ed6827faec54.png" />
</div>
</div>
<p>In a real-life development, you are more likely to want to make a class for the chatbot that will handle all the building at once. Here, we also add a convenient function to run the chatbot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Chatbot</span><span class="p">:</span>

    <span class="n">_graph_path</span> <span class="o">=</span> <span class="s2">&quot;./graph.png&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">llm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_build</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_display_graph</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># graph builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
        <span class="c1"># add the nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond_node</span><span class="p">)</span>
        <span class="c1"># define edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># compile the graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compile</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_input_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">user_query</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;Your message: &quot;</span><span class="p">)</span>
        <span class="n">human_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_query</span><span class="p">)</span>
        <span class="n">n_turns</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;n_turns&quot;</span><span class="p">]</span>
        <span class="c1"># add the input to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">human_message</span><span class="p">,</span>   <span class="c1"># this will append the input to the messages</span>
            <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="n">n_turns</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># and this will rewrite the number of turns</span>
            <span class="c1"># &quot;language&quot;: ...  # we don&#39;t update this field so we just leave it out</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_respond_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">]</span>    <span class="c1"># will already contain the user query</span>
        <span class="n">n_turns</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;n_turns&quot;</span><span class="p">]</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
        <span class="c1"># add the response to the messages</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">response</span><span class="p">,</span>   <span class="c1"># this will append the response to the messages</span>
            <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="n">n_turns</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># and this will rewrite the number of turns</span>
            <span class="c1"># &quot;language&quot;: ...  # we don&#39;t update this field so we just leave it out</span>
        <span class="p">}</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_is_quitting_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># check if the user wants to quit</span>
        <span class="n">user_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>
        <span class="k">return</span> <span class="n">user_message</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;quit&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_display_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># unstable</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">(</span>
                <span class="n">draw_method</span><span class="o">=</span><span class="n">MermaidDrawMethod</span><span class="o">.</span><span class="n">PYPPETEER</span><span class="p">,</span>
                <span class="n">output_file_path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph_path</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="c1"># add the run method</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">SystemMessage</span><span class="p">(</span>
                    <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful and honest assistant.&quot;</span> <span class="c1"># role</span>
                <span class="p">)</span>
            <span class="p">],</span>
            <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;language&quot;</span><span class="p">:</span> <span class="s2">&quot;some_value&quot;</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>   <span class="c1">#stream_mode=&quot;updates&quot;):</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">:</span><span class="se">\t</span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chatbot</span> <span class="o">=</span> <span class="n">Chatbot</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we are ready to interact with the chatbot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;)]
n_turns:	0
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;)]
n_turns:	1
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;), AIMessage(content=&quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;total_tokens&#39;: 63, &#39;completion_tokens&#39;: 39}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-2a4c751d-2f6c-49bc-b69c-0ad7a6b43473-0&#39;, usage_metadata={&#39;input_tokens&#39;: 24, &#39;output_tokens&#39;: 39, &#39;total_tokens&#39;: 63}, role=&#39;assistant&#39;)]
n_turns:	2
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;), AIMessage(content=&quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;total_tokens&#39;: 63, &#39;completion_tokens&#39;: 39}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-2a4c751d-2f6c-49bc-b69c-0ad7a6b43473-0&#39;, usage_metadata={&#39;input_tokens&#39;: 24, &#39;output_tokens&#39;: 39, &#39;total_tokens&#39;: 63}, role=&#39;assistant&#39;), HumanMessage(content=&#39;what is the 3rd platen from the Sun?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;6b7e7697-3b67-4deb-b964-03715899940c&#39;)]
n_turns:	3
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;), AIMessage(content=&quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;total_tokens&#39;: 63, &#39;completion_tokens&#39;: 39}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-2a4c751d-2f6c-49bc-b69c-0ad7a6b43473-0&#39;, usage_metadata={&#39;input_tokens&#39;: 24, &#39;output_tokens&#39;: 39, &#39;total_tokens&#39;: 63}, role=&#39;assistant&#39;), HumanMessage(content=&#39;what is the 3rd platen from the Sun?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;6b7e7697-3b67-4deb-b964-03715899940c&#39;), AIMessage(content=&#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 85, &#39;total_tokens&#39;: 179, &#39;completion_tokens&#39;: 94}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-848bc634-785a-4f22-8bdb-9412e353846b-0&#39;, usage_metadata={&#39;input_tokens&#39;: 85, &#39;output_tokens&#39;: 94, &#39;total_tokens&#39;: 179}, role=&#39;assistant&#39;)]
n_turns:	4
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;), AIMessage(content=&quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;total_tokens&#39;: 63, &#39;completion_tokens&#39;: 39}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-2a4c751d-2f6c-49bc-b69c-0ad7a6b43473-0&#39;, usage_metadata={&#39;input_tokens&#39;: 24, &#39;output_tokens&#39;: 39, &#39;total_tokens&#39;: 63}, role=&#39;assistant&#39;), HumanMessage(content=&#39;what is the 3rd platen from the Sun?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;6b7e7697-3b67-4deb-b964-03715899940c&#39;), AIMessage(content=&#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 85, &#39;total_tokens&#39;: 179, &#39;completion_tokens&#39;: 94}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-848bc634-785a-4f22-8bdb-9412e353846b-0&#39;, usage_metadata={&#39;input_tokens&#39;: 85, &#39;output_tokens&#39;: 94, &#39;total_tokens&#39;: 179}, role=&#39;assistant&#39;), HumanMessage(content=&#39;and 4?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d4a36f16-cab2-4db9-be78-1783b4acba8a&#39;)]
n_turns:	5
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;), AIMessage(content=&quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;total_tokens&#39;: 63, &#39;completion_tokens&#39;: 39}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-2a4c751d-2f6c-49bc-b69c-0ad7a6b43473-0&#39;, usage_metadata={&#39;input_tokens&#39;: 24, &#39;output_tokens&#39;: 39, &#39;total_tokens&#39;: 63}, role=&#39;assistant&#39;), HumanMessage(content=&#39;what is the 3rd platen from the Sun?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;6b7e7697-3b67-4deb-b964-03715899940c&#39;), AIMessage(content=&#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 85, &#39;total_tokens&#39;: 179, &#39;completion_tokens&#39;: 94}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-848bc634-785a-4f22-8bdb-9412e353846b-0&#39;, usage_metadata={&#39;input_tokens&#39;: 85, &#39;output_tokens&#39;: 94, &#39;total_tokens&#39;: 179}, role=&#39;assistant&#39;), HumanMessage(content=&#39;and 4?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d4a36f16-cab2-4db9-be78-1783b4acba8a&#39;), AIMessage(content=&quot;The 4th planet from the Sun is Mars! The Red Planet is a fascinating world that has captivated human imagination for centuries. It&#39;s a rocky planet with a thin atmosphere, and scientists believe it may have had water on its surface in the past.\n\nSo, to recap:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n\nLet me know if you have any other questions or if you&#39;d like to explore more about our solar system!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;The 4th planet from the Sun is Mars! The Red Planet is a fascinating world that has captivated human imagination for centuries. It&#39;s a rocky planet with a thin atmosphere, and scientists believe it may have had water on its surface in the past.\n\nSo, to recap:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n\nLet me know if you have any other questions or if you&#39;d like to explore more about our solar system!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 193, &#39;total_tokens&#39;: 288, &#39;completion_tokens&#39;: 95}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-02d77c86-1932-4785-83ec-23bcf18c6808-0&#39;, usage_metadata={&#39;input_tokens&#39;: 193, &#39;output_tokens&#39;: 95, &#39;total_tokens&#39;: 288}, role=&#39;assistant&#39;)]
n_turns:	6
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d6729705-db7a-41bf-8d42-bc45055bc17e&#39;), HumanMessage(content=&#39;hi&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03bdb513-e5bd-4311-b1a3-9e20472b4701&#39;), AIMessage(content=&quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m here to assist you with any questions or topics you&#39;d like to discuss.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 24, &#39;total_tokens&#39;: 63, &#39;completion_tokens&#39;: 39}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-2a4c751d-2f6c-49bc-b69c-0ad7a6b43473-0&#39;, usage_metadata={&#39;input_tokens&#39;: 24, &#39;output_tokens&#39;: 39, &#39;total_tokens&#39;: 63}, role=&#39;assistant&#39;), HumanMessage(content=&#39;what is the 3rd platen from the Sun?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;6b7e7697-3b67-4deb-b964-03715899940c&#39;), AIMessage(content=&#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &#39;I think you meant to ask &quot;What is the 3rd planet from the Sun?&quot;\n\nThe answer is Earth! Our home planet is the third planet from the Sun in our solar system. The order of the planets, starting from the Sun, is:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n5. Jupiter\n6. Saturn\n7. Uranus\n8. Neptune\n\nLet me know if you have any other questions!&#39;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 85, &#39;total_tokens&#39;: 179, &#39;completion_tokens&#39;: 94}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-848bc634-785a-4f22-8bdb-9412e353846b-0&#39;, usage_metadata={&#39;input_tokens&#39;: 85, &#39;output_tokens&#39;: 94, &#39;total_tokens&#39;: 179}, role=&#39;assistant&#39;), HumanMessage(content=&#39;and 4?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;d4a36f16-cab2-4db9-be78-1783b4acba8a&#39;), AIMessage(content=&quot;The 4th planet from the Sun is Mars! The Red Planet is a fascinating world that has captivated human imagination for centuries. It&#39;s a rocky planet with a thin atmosphere, and scientists believe it may have had water on its surface in the past.\n\nSo, to recap:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n\nLet me know if you have any other questions or if you&#39;d like to explore more about our solar system!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;The 4th planet from the Sun is Mars! The Red Planet is a fascinating world that has captivated human imagination for centuries. It&#39;s a rocky planet with a thin atmosphere, and scientists believe it may have had water on its surface in the past.\n\nSo, to recap:\n\n1. Mercury\n2. Venus\n3. Earth\n4. Mars\n\nLet me know if you have any other questions or if you&#39;d like to explore more about our solar system!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 193, &#39;total_tokens&#39;: 288, &#39;completion_tokens&#39;: 95}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-02d77c86-1932-4785-83ec-23bcf18c6808-0&#39;, usage_metadata={&#39;input_tokens&#39;: 193, &#39;output_tokens&#39;: 95, &#39;total_tokens&#39;: 288}, role=&#39;assistant&#39;), HumanMessage(content=&#39;quit&#39;, additional_kwargs={}, response_metadata={}, id=&#39;cdf2b6a3-b0d5-4816-8c99-f9578a544faa&#39;)]
n_turns:	7
language:	some_value
</pre></div>
</div>
</div>
</div>
<h2 id="checkpointing">4. Checkpointing 📍</h2>
<p>Even though our chatbot now conveniently stores and updates the state throughout <strong>one session</strong>, the final state is erased one the system exits. That does not allow for the <strong>repeated</strong> interaction with it. However, in real life, you want to be able to return to the chatbot in some time and be able to proceed where you left off.</p>
<p>To enable that, LangGraph provides a checkpointer for saving the memory. It creates a snapshot of the state locally stored under a unique id. All you need to do is to compile the graph with this memory and pass the id in the config when running the chatbot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langgraph.checkpoint.memory</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemorySaver</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ChatbotWithMemory</span><span class="p">(</span><span class="n">Chatbot</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_compile</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">MemorySaver</span><span class="p">())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_id</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
                <span class="n">SystemMessage</span><span class="p">(</span>
                    <span class="n">content</span><span class="o">=</span><span class="s2">&quot;You are a helpful and honest assistant.&quot;</span>
                <span class="p">)</span>
            <span class="p">],</span>
            <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;dummy_field&quot;</span><span class="p">:</span> <span class="s2">&quot;some_value&quot;</span>
        <span class="p">}</span>
        <span class="c1"># add config</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configurable&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="n">user_id</span><span class="p">}}</span>
        <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">chatbot</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">&quot;values&quot;</span><span class="p">):</span>
            <span class="c1"># change the output format</span>
            <span class="n">event</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chatbot_with_memory</span> <span class="o">=</span> <span class="n">ChatbotWithMemory</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now compare: first, we run the simple chatbot twice: it doesn’t remember the previous session.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first run</span>
<span class="n">chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;910a17a4-24dc-4ebc-9606-426c725429fc&#39;)]
n_turns:	0
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;910a17a4-24dc-4ebc-9606-426c725429fc&#39;), HumanMessage(content=&quot;hi, I&#39;m Max&quot;, additional_kwargs={}, response_metadata={}, id=&#39;c7ce02fe-32ef-4ee6-86ab-47871e5c83e7&#39;)]
n_turns:	1
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;910a17a4-24dc-4ebc-9606-426c725429fc&#39;), HumanMessage(content=&quot;hi, I&#39;m Max&quot;, additional_kwargs={}, response_metadata={}, id=&#39;c7ce02fe-32ef-4ee6-86ab-47871e5c83e7&#39;), AIMessage(content=&quot;Hi Max! It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m all ears!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;Hi Max! It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m all ears!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 28, &#39;total_tokens&#39;: 58, &#39;completion_tokens&#39;: 30}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-a0ddf0e4-6ae1-44cf-b870-cb86c86b9912-0&#39;, usage_metadata={&#39;input_tokens&#39;: 28, &#39;output_tokens&#39;: 30, &#39;total_tokens&#39;: 58}, role=&#39;assistant&#39;)]
n_turns:	2
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;910a17a4-24dc-4ebc-9606-426c725429fc&#39;), HumanMessage(content=&quot;hi, I&#39;m Max&quot;, additional_kwargs={}, response_metadata={}, id=&#39;c7ce02fe-32ef-4ee6-86ab-47871e5c83e7&#39;), AIMessage(content=&quot;Hi Max! It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m all ears!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;Hi Max! It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m all ears!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 28, &#39;total_tokens&#39;: 58, &#39;completion_tokens&#39;: 30}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-a0ddf0e4-6ae1-44cf-b870-cb86c86b9912-0&#39;, usage_metadata={&#39;input_tokens&#39;: 28, &#39;output_tokens&#39;: 30, &#39;total_tokens&#39;: 58}, role=&#39;assistant&#39;), HumanMessage(content=&#39;quit&#39;, additional_kwargs={}, response_metadata={}, id=&#39;2a3b7b8c-d962-4deb-baf3-fdff99f1a4e9&#39;)]
n_turns:	3
language:	some_value
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># second run</span>
<span class="n">chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03b8aea9-aa29-4a64-b5b6-230c35c50ea1&#39;)]
n_turns:	0
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03b8aea9-aa29-4a64-b5b6-230c35c50ea1&#39;), HumanMessage(content=&#39;remember me?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;4323f12c-5739-43d3-b045-ffee318a48d7&#39;)]
n_turns:	1
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03b8aea9-aa29-4a64-b5b6-230c35c50ea1&#39;), HumanMessage(content=&#39;remember me?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;4323f12c-5739-43d3-b045-ffee318a48d7&#39;), AIMessage(content=&quot;I&#39;m afraid I don&#39;t have personal memories, so I don&#39;t recall individual users or conversations. Each time you interact with me, it&#39;s a new conversation and I start from a blank slate. However, I&#39;m happy to chat with you again and help with any questions or topics you&#39;d like to discuss! How can I assist you today?&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;I&#39;m afraid I don&#39;t have personal memories, so I don&#39;t recall individual users or conversations. Each time you interact with me, it&#39;s a new conversation and I start from a blank slate. However, I&#39;m happy to chat with you again and help with any questions or topics you&#39;d like to discuss! How can I assist you today?&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 26, &#39;total_tokens&#39;: 96, &#39;completion_tokens&#39;: 70}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-537f2da3-9755-4dd5-bbb5-1a7ed13aeb7f-0&#39;, usage_metadata={&#39;input_tokens&#39;: 26, &#39;output_tokens&#39;: 70, &#39;total_tokens&#39;: 96}, role=&#39;assistant&#39;)]
n_turns:	2
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;03b8aea9-aa29-4a64-b5b6-230c35c50ea1&#39;), HumanMessage(content=&#39;remember me?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;4323f12c-5739-43d3-b045-ffee318a48d7&#39;), AIMessage(content=&quot;I&#39;m afraid I don&#39;t have personal memories, so I don&#39;t recall individual users or conversations. Each time you interact with me, it&#39;s a new conversation and I start from a blank slate. However, I&#39;m happy to chat with you again and help with any questions or topics you&#39;d like to discuss! How can I assist you today?&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;I&#39;m afraid I don&#39;t have personal memories, so I don&#39;t recall individual users or conversations. Each time you interact with me, it&#39;s a new conversation and I start from a blank slate. However, I&#39;m happy to chat with you again and help with any questions or topics you&#39;d like to discuss! How can I assist you today?&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 26, &#39;total_tokens&#39;: 96, &#39;completion_tokens&#39;: 70}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-537f2da3-9755-4dd5-bbb5-1a7ed13aeb7f-0&#39;, usage_metadata={&#39;input_tokens&#39;: 26, &#39;output_tokens&#39;: 70, &#39;total_tokens&#39;: 96}, role=&#39;assistant&#39;), HumanMessage(content=&#39;quit&#39;, additional_kwargs={}, response_metadata={}, id=&#39;80c99fc1-0784-4ffa-b540-bcffdd487c67&#39;)]
n_turns:	3
language:	some_value
</pre></div>
</div>
</div>
</div>
<p>The checkpointed chatbot will have the memories from the previous conversations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first run</span>
<span class="n">chatbot_with_memory</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;user_1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> System Message </span>================================

You are a helpful and honest assistant.


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

hi, I&#39;m Max


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

Hi Max! It&#39;s nice to meet you. Is there something I can help you with or would you like to chat? I&#39;m all ears!


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

quit
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># second run</span>
<span class="n">chatbot_with_memory</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;user_1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> System Message </span>================================

You are a helpful and honest assistant.


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

remember me?


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

You&#39;re Max, right? We just started chatting a little while ago. What&#39;s up?


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

quit
</pre></div>
</div>
</div>
</div>
<p>Note that this works as long as you use the same id! That is how you can maintain different conversation history for different users.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># third run</span>
<span class="n">chatbot_with_memory</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;user_2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>================================<span class=" -Color -Color-Bold"> System Message </span>================================

You are a helpful and honest assistant.


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

remember me?


==================================<span class=" -Color -Color-Bold"> Ai Message </span>==================================

I&#39;m afraid I don&#39;t have personal memories, so I don&#39;t recall individual users or conversations. Each time you interact with me, it&#39;s a new conversation and I start from scratch. However, I&#39;m happy to chat with you again and help with any questions or topics you&#39;d like to discuss! How can I assist you today?


================================<span class=" -Color -Color-Bold"> Human Message </span>=================================

quit
</pre></div>
</div>
</div>
</div>
<h2 id="memory">5. Memory Enhancement 💾</h2>
<p>As discussed in <a class="reference internal" href="#basic_chatbot"><span class="xref myst">Basic Chatbot</span></a>, passing the whole history to the chatbot is extremely inefficient. A simple way to handle it would be to set a memory window, e.g. pass only the last 5 messages.</p>
<p>Additionally, we can make stepwise summaries of the previous conversation to make the interaction more efficient while maintaining the reference to the previous chat history. To do so, we need to create an additional node that would check if the messages have piled up already, and one that would create summaries with LLMs and replace the chat history parts with it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemoveMessage</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># prompt template that will return the predefined system message</span>
<span class="c1"># and the additional messages you provide to it</span>
<span class="c1"># this will be covered in detail at the next lab</span>
<span class="n">summary_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># will always be returned</span>
        <span class="n">SystemMessage</span><span class="p">(</span><span class="s2">&quot;Make a summary of the following conversation. Return only the summary in 1-2 sentences.&quot;</span><span class="p">),</span>
        <span class="c1"># will be replaced by the messages you provide with the key &quot;messages&quot;</span>
        <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;messages&quot;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SummarizingChatbot</span><span class="p">(</span><span class="n">Chatbot</span><span class="p">):</span>

    <span class="n">_graph_path</span> <span class="o">=</span> <span class="s2">&quot;./summarizing_graph.png&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">llm</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">summary_template</span> <span class="o">=</span> <span class="n">summary_template</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># graph builder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
        <span class="c1"># add the nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond_node</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">&quot;summarize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summarize_node</span><span class="p">)</span>
        <span class="c1"># define edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_quitting_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">:</span> <span class="n">END</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">&quot;respond&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_summary_needed_node</span><span class="p">,</span> <span class="p">{</span><span class="kc">True</span><span class="p">:</span> <span class="s2">&quot;summarize&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">:</span> <span class="s2">&quot;input&quot;</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">&quot;summarize&quot;</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
        <span class="c1"># compile the graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_summary_needed_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">5</span>   <span class="c1"># system + 3 turns</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_summarize_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
        <span class="c1"># will pass the state to the prompt template;</span>
        <span class="c1"># the prompt template will match the key &quot;messages&quot;</span>
        <span class="c1"># with the messages in the state</span>
        <span class="c1"># and will return a sequence of messages</span>
        <span class="c1"># consisting of the summarization system message</span>
        <span class="c1"># and the sequence of previous messages in the state </span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary_template</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="c1"># now, mark all previous messages for deletion</span>
        <span class="n">messages</span> <span class="o">=</span>  <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;messages&quot;</span><span class="p">][</span><span class="mi">1</span><span class="p">:]]</span>  <span class="c1"># don&#39;t remove the system message</span>
        <span class="c1"># and add the summary instead</span>
        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
        <span class="n">n_turns</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;n_turns&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
            <span class="s2">&quot;n_turns&quot;</span><span class="p">:</span> <span class="n">n_turns</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summarizing_chatbot</span> <span class="o">=</span> <span class="n">SummarizingChatbot</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summarizing_chatbot</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;)]
n_turns:	0
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;), HumanMessage(content=&#39;what city are you from?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;ecb2ffac-656b-4668-a82c-c857793806c8&#39;)]
n_turns:	1
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;), HumanMessage(content=&#39;what city are you from?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;ecb2ffac-656b-4668-a82c-c857793806c8&#39;), AIMessage(content=&quot;I&#39;m not from a specific city, as I&#39;m a computer program designed to assist and communicate with users. I don&#39;t have a physical presence or a personal history, so I don&#39;t have a hometown or a city of origin. I exist solely to provide information and help with tasks, and I&#39;m available to assist you from anywhere in the world!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;I&#39;m not from a specific city, as I&#39;m a computer program designed to assist and communicate with users. I don&#39;t have a physical presence or a personal history, so I don&#39;t have a hometown or a city of origin. I exist solely to provide information and help with tasks, and I&#39;m available to assist you from anywhere in the world!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 29, &#39;total_tokens&#39;: 100, &#39;completion_tokens&#39;: 71}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-953ad878-294f-4b23-bb3a-f7554e32c493-0&#39;, usage_metadata={&#39;input_tokens&#39;: 29, &#39;output_tokens&#39;: 71, &#39;total_tokens&#39;: 100}, role=&#39;assistant&#39;)]
n_turns:	2
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;), HumanMessage(content=&#39;what city are you from?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;ecb2ffac-656b-4668-a82c-c857793806c8&#39;), AIMessage(content=&quot;I&#39;m not from a specific city, as I&#39;m a computer program designed to assist and communicate with users. I don&#39;t have a physical presence or a personal history, so I don&#39;t have a hometown or a city of origin. I exist solely to provide information and help with tasks, and I&#39;m available to assist you from anywhere in the world!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;I&#39;m not from a specific city, as I&#39;m a computer program designed to assist and communicate with users. I don&#39;t have a physical presence or a personal history, so I don&#39;t have a hometown or a city of origin. I exist solely to provide information and help with tasks, and I&#39;m available to assist you from anywhere in the world!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 29, &#39;total_tokens&#39;: 100, &#39;completion_tokens&#39;: 71}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-953ad878-294f-4b23-bb3a-f7554e32c493-0&#39;, usage_metadata={&#39;input_tokens&#39;: 29, &#39;output_tokens&#39;: 71, &#39;total_tokens&#39;: 100}, role=&#39;assistant&#39;), HumanMessage(content=&#39;I am from Tübingen&#39;, additional_kwargs={}, response_metadata={}, id=&#39;21d2f2a6-3815-43f8-8045-ccf70360f50b&#39;)]
n_turns:	3
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;), HumanMessage(content=&#39;what city are you from?&#39;, additional_kwargs={}, response_metadata={}, id=&#39;ecb2ffac-656b-4668-a82c-c857793806c8&#39;), AIMessage(content=&quot;I&#39;m not from a specific city, as I&#39;m a computer program designed to assist and communicate with users. I don&#39;t have a physical presence or a personal history, so I don&#39;t have a hometown or a city of origin. I exist solely to provide information and help with tasks, and I&#39;m available to assist you from anywhere in the world!&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;I&#39;m not from a specific city, as I&#39;m a computer program designed to assist and communicate with users. I don&#39;t have a physical presence or a personal history, so I don&#39;t have a hometown or a city of origin. I exist solely to provide information and help with tasks, and I&#39;m available to assist you from anywhere in the world!&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 29, &#39;total_tokens&#39;: 100, &#39;completion_tokens&#39;: 71}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-953ad878-294f-4b23-bb3a-f7554e32c493-0&#39;, usage_metadata={&#39;input_tokens&#39;: 29, &#39;output_tokens&#39;: 71, &#39;total_tokens&#39;: 100}, role=&#39;assistant&#39;), HumanMessage(content=&#39;I am from Tübingen&#39;, additional_kwargs={}, response_metadata={}, id=&#39;21d2f2a6-3815-43f8-8045-ccf70360f50b&#39;), AIMessage(content=&quot;Tübingen is a beautiful university town in southwestern Germany, known for its rich history, cultural heritage, and stunning architecture. The town is situated in the Neckar River valley and is famous for its well-preserved medieval old town, with its half-timbered houses, charming streets, and picturesque river views.\n\nTübingen is also home to one of Germany&#39;s oldest and most prestigious universities, the Eberhard Karls University of Tübingen, which was founded in 1477. The university has a strong reputation for academic excellence and has produced many notable alumni, including philosophers, theologians, and scientists.\n\nWhat do you like most about Tübingen? Is there a particular aspect of the town or its culture that you&#39;re fond of?&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;Tübingen is a beautiful university town in southwestern Germany, known for its rich history, cultural heritage, and stunning architecture. The town is situated in the Neckar River valley and is famous for its well-preserved medieval old town, with its half-timbered houses, charming streets, and picturesque river views.\n\nTübingen is also home to one of Germany&#39;s oldest and most prestigious universities, the Eberhard Karls University of Tübingen, which was founded in 1477. The university has a strong reputation for academic excellence and has produced many notable alumni, including philosophers, theologians, and scientists.\n\nWhat do you like most about Tübingen? Is there a particular aspect of the town or its culture that you&#39;re fond of?&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 117, &#39;total_tokens&#39;: 274, &#39;completion_tokens&#39;: 157}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-17849923-dbcd-42d2-91e9-5890d71d57ec-0&#39;, usage_metadata={&#39;input_tokens&#39;: 117, &#39;output_tokens&#39;: 157, &#39;total_tokens&#39;: 274}, role=&#39;assistant&#39;)]
n_turns:	4
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;), AIMessage(content=&quot;Here is a summary of our conversation in 1-2 sentences: We discussed my lack of a physical location and my purpose as a computer program, and then you mentioned that you are from Tübingen, a university town in southwestern Germany. I provided some information about Tübingen&#39;s history, culture, and university, and asked about your favorite aspects of the town.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;Here is a summary of our conversation in 1-2 sentences: We discussed my lack of a physical location and my purpose as a computer program, and then you mentioned that you are from Tübingen, a university town in southwestern Germany. I provided some information about Tübingen&#39;s history, culture, and university, and asked about your favorite aspects of the town.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 303, &#39;total_tokens&#39;: 380, &#39;completion_tokens&#39;: 77}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-e1b01344-61cb-42e8-91a1-48eca2b0b07a-0&#39;, usage_metadata={&#39;input_tokens&#39;: 303, &#39;output_tokens&#39;: 77, &#39;total_tokens&#39;: 380}, role=&#39;assistant&#39;)]
n_turns:	5
language:	some_value


messages:	[SystemMessage(content=&#39;You are a helpful and honest assistant.&#39;, additional_kwargs={}, response_metadata={}, id=&#39;dd1fa488-4449-4011-9191-62b84d5eb319&#39;), AIMessage(content=&quot;Here is a summary of our conversation in 1-2 sentences: We discussed my lack of a physical location and my purpose as a computer program, and then you mentioned that you are from Tübingen, a university town in southwestern Germany. I provided some information about Tübingen&#39;s history, culture, and university, and asked about your favorite aspects of the town.&quot;, additional_kwargs={}, response_metadata={&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: &quot;Here is a summary of our conversation in 1-2 sentences: We discussed my lack of a physical location and my purpose as a computer program, and then you mentioned that you are from Tübingen, a university town in southwestern Germany. I provided some information about Tübingen&#39;s history, culture, and university, and asked about your favorite aspects of the town.&quot;, &#39;token_usage&#39;: {&#39;prompt_tokens&#39;: 303, &#39;total_tokens&#39;: 380, &#39;completion_tokens&#39;: 77}, &#39;finish_reason&#39;: &#39;stop&#39;, &#39;model_name&#39;: &#39;meta/llama-3.3-70b-instruct&#39;}, id=&#39;run-e1b01344-61cb-42e8-91a1-48eca2b0b07a-0&#39;, usage_metadata={&#39;input_tokens&#39;: 303, &#39;output_tokens&#39;: 77, &#39;total_tokens&#39;: 380}, role=&#39;assistant&#39;), HumanMessage(content=&#39;quit&#39;, additional_kwargs={}, response_metadata={}, id=&#39;46b6d33b-584b-478c-bb6b-13e293b46a3d&#39;)]
n_turns:	6
language:	some_value
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./sessions/block2_core_topics/pt1_business/0805"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../1305.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">13.05. Virtual Assistants Pt. 2: RAG</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../0605.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">06.05. Virtual Assistants Pt. 1: Chatbots</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, Maksim Shmalts
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
          <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
</svg>
</a>
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants/issues/new" aria-label="Issues"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
  <path d="M8 15A7 7 0 1 0 8 1a7 7 0 0 0 0 14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"/>
  <path d="M7.002 11a1 1 0 1 1 2 0 1 1 0 0 1-2 0zm.1-6.995a.905.905 0 0 1 1.8 0l-.35 4.5a.55.55 0 0 1-1.1 0l-.35-4.5z"/>
</svg>
</a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">08.05. Basic LLM-based Chatbot 🤖</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    </body>
</html>