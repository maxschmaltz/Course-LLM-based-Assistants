<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html"><link rel="next" title="LLM Inference Guide" href="llm_inference_guide.html"><link rel="prev" title="LLM-based Assistants" href="../index.html">

    <!-- Generated with Sphinx 7.4.7 and Furo 2025.09.25 -->
        <title>Topics Overview - LLM-based Assistants</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">LLM-based Assistants</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">LLM-based Assistants</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Infos and Stuff</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Topics Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm_inference_guide.html">LLM Inference Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="debates.html">Debates</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">October: INTRO</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block1_intro/2210.html">22.10. Intro</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">November-December: CORE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block2_core/0511.html">05.11. Lecture: Virtual Assistants</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">January-February: PROJECTS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block3_projects/projects.html">Projects</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="topics-overview">
<h1>Topics Overview<a class="headerlink" href="#topics-overview" title="Link to this heading">¶</a></h1>
<ul class="simple">
<li><p>October: INTRO</p>
<ul>
<li><p>Week 1</p>
<ul>
<li><p><a class="reference internal" href="#2210"><span class="xref myst">22.10. Intro</span></a></p></li>
<li><p><a class="reference internal" href="#2310"><span class="xref myst">23.10. Lecture: Ontological Status of LLMs</span></a></p></li>
</ul>
</li>
<li><p>Week 2</p>
<ul>
<li><p><a class="reference internal" href="#2910"><span class="xref myst">29.10. Lecture: LLM &amp; Agent Basics</span></a></p></li>
<li><p><a class="reference internal" href="#3010"><span class="xref myst">30.10. Lab: Intro to LangChain</span></a></p></li>
</ul>
</li>
</ul>
</li>
<li><p>November–December: CORE</p>
<ul>
<li><p>Week 3</p>
<ul>
<li><p><a class="reference internal" href="#0511"><span class="xref myst">05.11. Lecture: Virtual Assistants</span></a></p></li>
<li><p><a class="reference internal" href="#0611"><span class="xref myst">06.11. Lab: LLM-based Chatbot</span></a></p></li>
</ul>
</li>
<li><p>Week 4</p>
<ul>
<li><p><a class="reference internal" href="#1211"><span class="xref myst">12.11 &amp; 13.11. Labs: RAG</span></a></p></li>
</ul>
</li>
<li><p>Week 5</p>
<ul>
<li><p><a class="reference internal" href="#1911"><span class="xref myst">19.11. Lecture: Multi-agent Environment</span></a></p></li>
<li><p><a class="reference internal" href="#2011"><span class="xref myst">20.11. Lab: Multi-agent Environment</span></a></p></li>
</ul>
</li>
<li><p>Week 6</p>
<ul>
<li><p><a class="reference internal" href="#2611"><span class="xref myst">26.11 &amp; 27.11. Labs: LLM-powered Website</span></a></p></li>
</ul>
</li>
<li><p>Week 7</p>
<ul>
<li><p><a class="reference internal" href="#0312"><span class="xref myst">03.12 &amp; 04.12. Labs: LLM-powered Research Assistant</span></a></p></li>
</ul>
</li>
<li><p>Week 8</p>
<ul>
<li><p><a class="reference internal" href="#1012"><span class="xref myst">10.12. Lecture: Role of AI in Recent Years</span></a></p></li>
<li><p><a class="reference internal" href="#1112"><span class="xref myst">11.12. Wrap-up</span></a></p></li>
</ul>
</li>
<li><p>Week 9</p>
<ul>
<li><p><a class="reference internal" href="#1712"><span class="xref myst">17.12. Debate: Role of AI in Recent Years</span></a></p></li>
<li><p><a class="reference internal" href="#1812"><span class="xref myst">18.12. Project Proposals</span></a></p></li>
</ul>
</li>
</ul>
</li>
<li><p>January–February: PROJECTS</p>
<ul>
<li><p>Weeks 10-13</p>
<ul>
<li><p><a class="reference internal" href="#project_weeks"><span class="xref myst">Project work</span></a></p></li>
</ul>
</li>
<li><p>Week 14</p>
<ul>
<li><p><a class="reference internal" href="#0402"><span class="xref myst">04.02 &amp; 05.02. Project Presentations</span></a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<p>See detailed descriptions if the sessions below.</p>
<div style="font-size:9pt">
    <div class="notes">
        <p><em>Notes</em>:</p>
        <ul>
            <li>This schedule may be changed, should the need arise.</li>
            <li>You are <strong>not</strong> required to read anything. However, you are strongly encouraged to read sources marked by pin emojis 📌: those are comprehensive overviews on the topics or important works that are beneficial for a better understanding of the key concepts.</li>
            <li>Sources marked with a popcorn emoji 🍿 is misc material you might want to take a look at: blog posts, GitHub repos, leaderboards etc.</li>
            <li>For the <em>labs</em>, you are provided with practical tutorials that the respective lab tasks will mostly derive from. The core tutorials are marked with a writing emoji ✍️; you are asked to inspect them in advance (better yet: try them out).</li>
        </ul>
    </div>
<p><em>Disclaimer</em>: the reading entries are no proper citations; detailed infos about the authors, publication date, venue etc. can be found under the entry links.</p>
</div>
<hr class="docutils" />
<section id="october-intro">
<h2>October: INTRO<a class="headerlink" href="#october-intro" title="Link to this heading">¶</a></h2>
<section id="week-1">
<h3>Week 1<a class="headerlink" href="#week-1" title="Link to this heading">¶</a></h3>
<p><a name="2210"></a></p>
<section id="intro">
<h4>22.10. Intro<a class="headerlink" href="#intro" title="Link to this heading">¶</a></h4>
<p>That is an introductory meeting, in which I we will cover the contents and the schedule of the course, the class formats and the formalia, and where all your questions, suggestions etc. will be discussed.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Course introduction</p></li>
<li><p>Q&amp;A</p></li>
</ul>
<p><a name="2310"></a></p>
</section>
<section id="lecture-ontological-status-of-llms">
<h4>23.10. <em>Lecture</em>: Ontological Status of LLMs<a class="headerlink" href="#lecture-ontological-status-of-llms" title="Link to this heading">¶</a></h4>
<p>This lecture will suggest a warm-up discussion about different perspectives on LLM nature. We will focus on two prominent outlooks: LLM as as a complex statistical machine vs LLM as a form of intelligence. We’ll discuss differences of LLM and human intelligence and the degree to which LLMs exhibit (self-)awareness.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Different perspectives on the nature of LLMs</p></li>
<li><p>Similarities and differences between human and artificial intelligence</p></li>
<li><p>LLMs’ (self-)awareness</p></li>
</ul>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2210.13966">The Debate Over Understanding in AI’s Large Language Models</a> (pages 1-7), <code class="docutils literal notranslate"><span class="pre">Santa</span> <span class="pre">Fe</span> <span class="pre">Institute</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2208.02957">Meaning without reference in large language models</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">Berkeley</span> <span class="pre">&amp;</span> <span class="pre">DeepMind</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2301.06627">Dissociating language and thought in large language models</a>, <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Texas</span> <span class="pre">at</span> <span class="pre">Austin</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://direct.mit.edu/daed/article/151/2/183/110604/Do-Large-Language-Models-Understand-Us">Do Large Language Models Understand Us?</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2303.12712">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a> (chapters 1-8 &amp; 10), <code class="docutils literal notranslate"><span class="pre">Microsoft</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</a> (paragraphs 1, 5, 6.1), <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Washington</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.19671">Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding</a>, <code class="docutils literal notranslate"><span class="pre">Leiden</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Advanced</span> <span class="pre">Computer</span> <span class="pre">Science</span> <span class="pre">&amp;</span> <span class="pre">Leiden</span> <span class="pre">University</span> <span class="pre">Medical</span> <span class="pre">Centre</span></code></p></li>
</ul>
</section>
</section>
<section id="week-2">
<h3>Week 2<a class="headerlink" href="#week-2" title="Link to this heading">¶</a></h3>
<p><a name="2910"></a></p>
<section id="lecture-lecture-llm-agent-basics">
<h4>29.10. <em>Lecture</em>: <em>Lecture</em>: LLM &amp; Agent Basics<a class="headerlink" href="#lecture-lecture-llm-agent-basics" title="Link to this heading">¶</a></h4>
<p>In this lecture, we’ll recap the basics of LLMs and LLM-based agents to make sure we’re on the same page.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>What makes an LLM</p></li>
<li><p>Instruction fine-tuning &amp; alignment</p></li>
<li><p>LLM-based agents</p></li>
<li><p>Structured output</p></li>
<li><p>Tool calling</p></li>
<li><p>Piping &amp; Planning</p></li>
</ul>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2303.18223">A Survey of Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Renmin</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research,</span> <span class="pre">Stanford,</span> <span class="pre">UNC</span> <span class="pre">Chapel</span> <span class="pre">Hill,</span> <span class="pre">DeepMind</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a>, <code class="docutils literal notranslate"><span class="pre">Meta</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Washington</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.03710">Agent Instructs Large Language Models to be General Zero-Shot Reasoners</a>, <code class="docutils literal notranslate"><span class="pre">Washington</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">UC</span> <span class="pre">Berkeley</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a>, <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2307.12966">Aligning Large Language Models with Human: A Survey</a> (pages 1-14), <code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Noah’s</span> <span class="pre">Ark</span> <span class="pre">Lab</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>, <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.05862">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a>, <code class="docutils literal notranslate"><span class="pre">Anthropic</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.07362">“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research</span> <span class="pre">&amp;</span> <span class="pre">Google</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://openai.com/index/introducing-structured-outputs-in-the-api/">Introducing Structured Outputs in the API</a>, <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2405.17935">Tool Learning with Large Language Models: A Survey</a>, <code class="docutils literal notranslate"><span class="pre">Renmin</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.00920">ToolACE: Winning the Points of LLM Function Calling</a>, <code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Noah’s</span> <span class="pre">Ark</span> <span class="pre">Lab</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a>, <code class="docutils literal notranslate"><span class="pre">Meta</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.00121">Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks</a>, <code class="docutils literal notranslate"><span class="pre">IBM</span> <span class="pre">Research</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">Berkeley</span></code> (leaderboard)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Princeton</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Google</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2306.13549">A Survey on Multimodal Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">Technology</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">&amp;</span> <span class="pre">Tencent</span> <span class="pre">YouTu</span> <span class="pre">Lab</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.19736">Evaluating Large Language Models. A Comprehensive Survey</a>, <code class="docutils literal notranslate"><span class="pre">Tianjin</span> <span class="pre">University</span></code></p></li>
</ul>
<p><a name="3010"></a></p>
</section>
<section id="lab-intro-to-langchain">
<h4>30.10. <em>Lab</em>: Intro to LangChain<a class="headerlink" href="#lab-intro-to-langchain" title="Link to this heading">¶</a></h4>
<p>This is the first lab which will guide you through the basic concepts of LangChain for the further practical sessions.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>✍️ <a class="reference external" href="https://docs.langchain.com/oss/python/langchain/models">Models</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/langchain/messages">Messages</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
</ul>
</section>
</section>
</section>
<section id="november-december-core">
<h2>November-December: CORE<a class="headerlink" href="#november-december-core" title="Link to this heading">¶</a></h2>
<section id="week-3">
<h3>Week 3<a class="headerlink" href="#week-3" title="Link to this heading">¶</a></h3>
<p><a name="0511"></a></p>
<section id="lecture-virtual-assistants">
<h4>05.11. <em>Lecture</em>: Virtual Assistants<a class="headerlink" href="#lecture-virtual-assistants" title="Link to this heading">¶</a></h4>
<p>The first core topic addresses single-LLM virtual assistants such as chatbots and RAG systems. We’ll discuss how these systems are built and how you can tune them for your use case.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Prompting techniques</p></li>
<li><p>Memory</p></li>
<li><p>RAG workflow &amp; techniques</p></li>
<li><p>RAG vs long-context LLMs</p></li>
</ul>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.07927">A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications</a>, <code class="docutils literal notranslate"><span class="pre">Indian</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">Patna,</span> <span class="pre">Stanford</span> <span class="pre">&amp;</span> <span class="pre">Amazon</span> <span class="pre">AI</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> (pages 1-9), <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.13501">A Survey on the Memory Mechanism of Large Language Model based Agents</a>, <code class="docutils literal notranslate"><span class="pre">Renmin</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">&amp;</span> <span class="pre">Huawei</span> <span class="pre">Noah’s</span> <span class="pre">Ark</span> <span class="pre">Lab</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2306.07174">Augmenting Language Models with Long-Term Memory</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">Santa</span> <span class="pre">Barbara</span> <span class="pre">&amp;</span> <span class="pre">Microsoft</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2401.02777">From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Beike</span> <span class="pre">Inc.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.02717">Automatic Prompt Selection for Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Cinnamon</span> <span class="pre">AI,</span> <span class="pre">Hung</span> <span class="pre">Yen</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">and</span> <span class="pre">Education</span> <span class="pre">&amp;</span> <span class="pre">Deakin</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://aclanthology.org/2022.findings-naacl.3/">PromptGen: Automatically Generate Prompts using Generative Models</a>, <code class="docutils literal notranslate"><span class="pre">Baidu</span> <span class="pre">Research</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2407.16833">Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</a> (pages 1-7), <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">DeepMind</span> <span class="pre">&amp;</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Michigan</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.10981">A Survey on Retrieval-Augmented Text Generation for Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">York</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2412.15605">Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks</a>, <code class="docutils literal notranslate"><span class="pre">National</span> <span class="pre">Chengchi</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Academia</span> <span class="pre">Sinica</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.11511">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Washington,</span> <span class="pre">Allen</span> <span class="pre">Institute</span> <span class="pre">for</span> <span class="pre">AI</span> <span class="pre">&amp;</span> <span class="pre">IBM</span> <span class="pre">Research</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2411.19443">Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Chinese</span> <span class="pre">Academy</span> <span class="pre">of</span> <span class="pre">Sciences</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.14403">Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</a>, <code class="docutils literal notranslate"><span class="pre">Korea</span> <span class="pre">Advanced</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2502.00032">Querying Databases with Function Calling</a>, <code class="docutils literal notranslate"><span class="pre">Weaviate,</span> <span class="pre">Contextual</span> <span class="pre">AI</span> <span class="pre">&amp;</span> <span class="pre">Morningstar</span></code></p></li>
</ul>
<p><a name="0611"></a></p>
</section>
<section id="lab-llm-based-chatbot">
<h4>06.11. <em>Lab</em>: LLM-based Chatbot<a class="headerlink" href="#lab-llm-based-chatbot" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0511"><span class="xref myst"><em>Lecture</em>: Virtual Assistants</span></a></p>
</div></blockquote>
<p>In this lab, we’ll build a chatbot and try different prompts and settings to see how it affects the output.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>✍️ <a class="reference external" href="https://docs.langchain.com/oss/python/langgraph/graph-api">Graph API overview</a>, <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/langgraph/use-graph-api">Use the graph API</a>, <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
</ul>
</section>
</section>
<section id="week-4">
<h3>Week 4<a class="headerlink" href="#week-4" title="Link to this heading">¶</a></h3>
<p><a name="1211"></a></p>
<section id="labs-rag">
<h4>12.11 &amp; 13.11. <em>Labs</em>: RAG<a class="headerlink" href="#labs-rag" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0511"><span class="xref myst"><em>Lecture</em>: Virtual Assistants</span></a></p>
</div></blockquote>
<p>In this lab, we’ll start expanding the functionality of the chatbot built at the last lab to connect it to our user-specific information. On the first day, we’ll preprocess our custom data for further retrieval. The following day we’ll complete move from data preprocessing to implementing the RAG workflow.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>✍️ <a class="reference external" href="https://docs.langchain.com/oss/python/langchain/retrieval">Retrieval</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/integrations/document_loaders">Document loaders</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/integrations/splitters">Text splitters</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/integrations/text_embedding">Embedding models</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/integrations/vectorstores">Vector stores</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/integrations/retrievers">Retrievers</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://docs.langchain.com/oss/python/langchain/rag#build-a-rag-agent-with-langchain">Build a RAG agent with LangChain</a>, <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/">Adaptive RAG</a> (deprecated), <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
</ul>
</section>
</section>
<section id="week-5">
<h3>Week 5<a class="headerlink" href="#week-5" title="Link to this heading">¶</a></h3>
<p><a name="1911"></a></p>
<section id="lecture-multi-agent-environment">
<h4>19.11. <em>Lecture</em>: Multi-agent Environment<a class="headerlink" href="#lecture-multi-agent-environment" title="Link to this heading">¶</a></h4>
<p>This lectures directs its attention to automating everyday / business operations in a multi-agent environment. We’ll look at how agents communicate with each other, how their communication can be guided (both with and without involvement of a human), and how this is used in real applications.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Multi-agent environment</p></li>
<li><p>Agents communication</p></li>
<li><p>Examples of pipelines for business operations</p></li>
</ul>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2411.14033">LLM-based Multi-Agent Systems: Techniques and Business Perspectives</a> (pages 1-8), <code class="docutils literal notranslate"><span class="pre">Shanghai</span> <span class="pre">Jiao</span> <span class="pre">Tong</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">OPPO</span> <span class="pre">Research</span> <span class="pre">Institute</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2304.03442">Generative Agents: Interactive Simulacra of Human Behavior</a>, <code class="docutils literal notranslate"><span class="pre">Stanford,</span> <span class="pre">Google</span> <span class="pre">Research</span> <span class="pre">&amp;</span> <span class="pre">DeepMind</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2305.14325">Improving Factuality and Reasoning in Language Models through Multiagent Debate</a>, <code class="docutils literal notranslate"><span class="pre">MIT</span> <span class="pre">&amp;</span> <span class="pre">Google</span> <span class="pre">Brain</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.02124">Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</a>, <code class="docutils literal notranslate"><span class="pre">Zhejiang</span> <span class="pre">University,</span> <span class="pre">National</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Singapore</span> <span class="pre">&amp;</span> <span class="pre">DeepMind</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2308.08155">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span> <span class="pre">Research</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://blogs.microsoft.com/blog/2025/03/10/https-blogs-microsoft-com-blog-2024-11-12-how-real-world-businesses-are-transforming-with-ai/">How real-world businesses are transforming with AI — with more than 140 new stories</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span></code> (blog post)</p></li>
<li><p>🍿 <a class="reference external" href="https://www.langchain.com/built-with-langgraph">Built with LangGraph</a>, <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code> (website page)</p></li>
<li><p>🍿 <a class="reference external" href="https://blogs.microsoft.com/blog/2025/04/04/your-ai-companion/">Your AI Companion</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span></code> (blog post)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2502.01390">Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant</a>, <code class="docutils literal notranslate"><span class="pre">Delft</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">&amp;</span> <span class="pre">The</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Queensland</span></code></p></li>
</ul>
<p><a name="2011"></a></p>
</section>
<section id="lab-multi-agent-environment">
<h4>20.11. <em>Lab</em>: Multi-agent Environment<a class="headerlink" href="#lab-multi-agent-environment" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#1911"><span class="xref myst"><em>Lecture</em>: Multi-agent Environment</span></a></p>
</div></blockquote>
<p>This lab will introduce a short walkthrough to creation of a multi-agent environment for automated meeting scheduling and preparation. We will see how the coordinator agent will communicate with two auxiliary agents to check time availability and prepare an agenda for the meeting.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>✍️ <a class="reference external" href="https://docs.langchain.com/oss/python/langchain/multi-agent#multi-agent">Multi-agent</a>, <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
<li><p><a class="reference external" href="https://docs.langchain.com/oss/python/langchain/human-in-the-loop">Human-in-the-loop</a>, <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute/">Plan-and-Execute</a> (deprecated), <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/reflexion/reflexion/">Reflection</a> (deprecated), <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/">Multi-agent supervisor</a> (deprecated), <code class="docutils literal notranslate"><span class="pre">LangGraph</span></code></p></li>
</ul>
</section>
</section>
<section id="week-6">
<h3>Week 6<a class="headerlink" href="#week-6" title="Link to this heading">¶</a></h3>
<p><a name="2611"></a></p>
<section id="labs-llm-powered-website">
<h4>26.11 &amp; 27.11. <em>Labs</em>: LLM-powered Website<a class="headerlink" href="#labs-llm-powered-website" title="Link to this heading">¶</a></h4>
<p>This lab open a mini-cycle of labs, where you will individually build a couple of smaller multi-agent systems. These labs are needed for you to practice the technical implementation of such systems, identify and work on your weak spots, as well as discuss all the doubts and difficulties you encounter. Consider it preparation for the final project. During the first two labs, you will create a workflow to generate websites with LLMs. The LLMs will generate both the contents and the code required for rendering, styling and navigation.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>see <a class="reference internal" href="#2011"><span class="xref myst"><em>Lab</em>: Multi-agent Environment</span></a></p></li>
<li><p>✍️ <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Getting_started/Your_first_website/Creating_the_content">HTML: Creating the content</a>, <code class="docutils literal notranslate"><span class="pre">MDN</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Getting_started">Getting started with CSS</a>, <code class="docutils literal notranslate"><span class="pre">MDN</span></code></p></li>
</ul>
</section>
</section>
<section id="week-7">
<h3>Week 7<a class="headerlink" href="#week-7" title="Link to this heading">¶</a></h3>
<p><a name="0312"></a></p>
<section id="labs-llm-powered-research-assistant">
<h4>03.12 &amp; 04.12. <em>Labs</em>: LLM-powered Research Assistant<a class="headerlink" href="#labs-llm-powered-research-assistant" title="Link to this heading">¶</a></h4>
<p>The second system you will build individually will be a multi-agent research assistant. It will facilitate in planning the research, generating and evaluating hypotheses, and finding the literature for a given scientific problem.</p>
<p><strong>Sources</strong>: see <a class="reference internal" href="#2011"><span class="xref myst"><em>Lab</em>: Multi-agent Environment</span></a></p>
</section>
</section>
<section id="week-8">
<h3>Week 8<a class="headerlink" href="#week-8" title="Link to this heading">¶</a></h3>
<p><a name="1012"></a></p>
<section id="lecture-role-of-ai-in-recent-years">
<h4>10.12. <em>Lecture</em>: Role of AI in Recent Years<a class="headerlink" href="#lecture-role-of-ai-in-recent-years" title="Link to this heading">¶</a></h4>
<p>The last <em>lecture</em> of the course will turn to societal considerations regarding LLMs and AI in general and will investigate its role and influence on the humanity nowadays.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Studies on influence of AI in the recent years</p></li>
<li><p>Studies on AI integration rate</p></li>
<li><p>Ethical, legal &amp; environmental aspects</p></li>
</ul>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2502.12447">Protecting Human Cognition in the Age of AI</a> (pages 1-5), The University of Texas at Austin et al.</p></li>
<li><p>📌 <a class="reference external" href="https://onlinelibrary.wiley.com/doi/full/10.1111/exsy.13406">Artificial intelligence governance: Ethical considerations and implications for social responsibility</a> (pages 1-12), <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Malta</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2412.03963">Augmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI’s Impact on Creative Tasks</a>, <code class="docutils literal notranslate"><span class="pre">Tsinghua</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Wuhan</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2410.03703">Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Toronto</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.01754v1">Empirical evidence of Large Language Model’s influence on human spoken communication</a>, <code class="docutils literal notranslate"><span class="pre">Max-Planck</span> <span class="pre">Institute</span> <span class="pre">for</span> <span class="pre">Human</span> <span class="pre">Development</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://hai.stanford.edu/ai-index/2025-ai-index-report">The 2025 AI Index Report: Top Takeaways</a>, <code class="docutils literal notranslate"><span class="pre">Stanford</span></code></p></li>
<li><p><a class="reference external" href="https://ai.wharton.upenn.edu/focus-areas/human-technology-interaction/2024-ai-adoption-report/">Growing Up: Navigating Generative AI’s Early Years – AI Adoption Report: Executive Summary</a>, <code class="docutils literal notranslate"><span class="pre">AI</span> <span class="pre">at</span> <span class="pre">Wharton</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2503.14539">Ethical Implications of AI in Data Collection: Balancing Innovation with Privacy</a>, <code class="docutils literal notranslate"><span class="pre">AI</span> <span class="pre">Data</span> <span class="pre">Chronicles</span></code></p></li>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007/s43681-024-00644-x">Legal and ethical implications of AI-based crowd analysis: the AI Act and beyond</a>, <code class="docutils literal notranslate"><span class="pre">Vrije</span> <span class="pre">Universiteit</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2412.04782v1">A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges</a>, <code class="docutils literal notranslate"><span class="pre">Cleveland</span> <span class="pre">State</span> <span class="pre">University</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
</ul>
<p><a name="1112"></a></p>
</section>
<section id="wrap-up">
<h4>11.12. Wrap-up<a class="headerlink" href="#wrap-up" title="Link to this heading">¶</a></h4>
<p>This informal meeting will give a small summary with key takeaways from the course. We will also discuss the next steps such as project requirements, proposal procedure etc.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Summary</p></li>
<li><p>Project discussion</p></li>
<li><p>Q&amp;A</p></li>
</ul>
</section>
</section>
<section id="week-9">
<h3>Week 9<a class="headerlink" href="#week-9" title="Link to this heading">¶</a></h3>
<p><a name="1712"></a></p>
<section id="debate-role-of-ai-in-recent-years">
<h4>17.12. <em>Debate</em>: Role of AI in Recent Years<a class="headerlink" href="#debate-role-of-ai-in-recent-years" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#1012"><span class="xref myst"><em>Lecture</em>: Role of AI in Recent Years</span></a></p>
</div></blockquote>
<p>The core block of the course will be concluded by the final debates about the role of AI in recent years. Debate topics well be announced on 10.12.</p>
<p><strong>Sources</strong>: see <a class="reference internal" href="#1012"><span class="xref myst"><em>Lecture</em>: Role of AI in Recent Years</span></a></p>
<p><a name="1812"></a></p>
</section>
<section id="project-proposals">
<h4>18.12. Project Proposals<a class="headerlink" href="#project-proposals" title="Link to this heading">¶</a></h4>
<p>In this meeting, you will introduce your project proposals. The goal of this session is to receive feedback on your idea from your peer students and me in order to adjust the idea if necessary. Additionally, the intermediate consultations for the project groups will be scheduled.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Project proposals</p></li>
<li><p>Consultation scheduling</p></li>
</ul>
</section>
</section>
</section>
<section id="january-february-projects">
<h2>January-February: PROJECTS<a class="headerlink" href="#january-february-projects" title="Link to this heading">¶</a></h2>
<p><a name="project_weeks"></a></p>
<section id="weeks-10-13">
<h3>Weeks 10-13<a class="headerlink" href="#weeks-10-13" title="Link to this heading">¶</a></h3>
<p>This time is given to you to implement the projects as well as to prepare a short presentation for the final week. During this time, there will be a few consultations for the project groups, where we will be inspecting the intermediate progress and addressing the issues.</p>
</section>
<section id="week-14">
<h3>Week 14<a class="headerlink" href="#week-14" title="Link to this heading">¶</a></h3>
<p><a name="0402"></a></p>
<section id="project-presentations">
<h4>04.02 &amp; 05.02. Project Presentations<a class="headerlink" href="#project-presentations" title="Link to this heading">¶</a></h4>
<p>Finally, the last two sessions of the course will be dedicated to your project presentations.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./infos"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="llm_inference_guide.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">LLM Inference Guide</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../index.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Home</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, Maksim Shmalts
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
          <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
</svg>
</a>
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants/issues/new" aria-label="Issues"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
  <path d="M8 15A7 7 0 1 0 8 1a7 7 0 0 0 0 14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"/>
  <path d="M7.002 11a1 1 0 1 1 2 0 1 1 0 0 1-2 0zm.1-6.995a.905.905 0 0 1 1.8 0l-.35 4.5a.55.55 0 0 1-1.1 0l-.35-4.5z"/>
</svg>
</a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Topics Overview</a><ul>
<li><a class="reference internal" href="#october-intro">October: INTRO</a><ul>
<li><a class="reference internal" href="#week-1">Week 1</a><ul>
<li><a class="reference internal" href="#intro">22.10. Intro</a></li>
<li><a class="reference internal" href="#lecture-ontological-status-of-llms">23.10. <em>Lecture</em>: Ontological Status of LLMs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-2">Week 2</a><ul>
<li><a class="reference internal" href="#lecture-lecture-llm-agent-basics">29.10. <em>Lecture</em>: <em>Lecture</em>: LLM &amp; Agent Basics</a></li>
<li><a class="reference internal" href="#lab-intro-to-langchain">30.10. <em>Lab</em>: Intro to LangChain</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#november-december-core">November-December: CORE</a><ul>
<li><a class="reference internal" href="#week-3">Week 3</a><ul>
<li><a class="reference internal" href="#lecture-virtual-assistants">05.11. <em>Lecture</em>: Virtual Assistants</a></li>
<li><a class="reference internal" href="#lab-llm-based-chatbot">06.11. <em>Lab</em>: LLM-based Chatbot</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-4">Week 4</a><ul>
<li><a class="reference internal" href="#labs-rag">12.11 &amp; 13.11. <em>Labs</em>: RAG</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-5">Week 5</a><ul>
<li><a class="reference internal" href="#lecture-multi-agent-environment">19.11. <em>Lecture</em>: Multi-agent Environment</a></li>
<li><a class="reference internal" href="#lab-multi-agent-environment">20.11. <em>Lab</em>: Multi-agent Environment</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-6">Week 6</a><ul>
<li><a class="reference internal" href="#labs-llm-powered-website">26.11 &amp; 27.11. <em>Labs</em>: LLM-powered Website</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-7">Week 7</a><ul>
<li><a class="reference internal" href="#labs-llm-powered-research-assistant">03.12 &amp; 04.12. <em>Labs</em>: LLM-powered Research Assistant</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-8">Week 8</a><ul>
<li><a class="reference internal" href="#lecture-role-of-ai-in-recent-years">10.12. <em>Lecture</em>: Role of AI in Recent Years</a></li>
<li><a class="reference internal" href="#wrap-up">11.12. Wrap-up</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-9">Week 9</a><ul>
<li><a class="reference internal" href="#debate-role-of-ai-in-recent-years">17.12. <em>Debate</em>: Role of AI in Recent Years</a></li>
<li><a class="reference internal" href="#project-proposals">18.12. Project Proposals</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#january-february-projects">January-February: PROJECTS</a><ul>
<li><a class="reference internal" href="#weeks-10-13">Weeks 10-13</a></li>
<li><a class="reference internal" href="#week-14">Week 14</a><ul>
<li><a class="reference internal" href="#project-presentations">04.02 &amp; 05.02. Project Presentations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    </body>
</html>