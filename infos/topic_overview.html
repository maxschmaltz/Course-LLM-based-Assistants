<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Debates" href="formats/debates.html" /><link rel="prev" title="LLM-based Assistants" href="../index.html" />

    <!-- Generated with Sphinx 7.4.7 and Furo 2024.08.06 -->
        <title>Topics Overview - LLM-based Assistants</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    
    


<style>
  body {
    --color-code-background: #eeffcc;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">LLM-based Assistants</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">LLM-based Assistants</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Infos and Stuff</span></p>
<ul class="current">
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Topics Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/debates.html">Debates</a></li>
<li class="toctree-l1"><a class="reference internal" href="formats/pitches.html">Pitches</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm_inference_guide/README.html">LLM Inference Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 1: Intro</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block1_intro/under_development.html">Under development</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 2: Core Topics | Part 1: Business Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block2_core_topics/pt1_business/under_development.html">Under development</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 2: Core Topics | Part 2: Applications in Science</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block2_core_topics/pt2_science/under_development.html">Under development</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Block 3: Wrap-up</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sessions/block3_wrapup/under_development.html">Under development</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="topics-overview">
<h1>Topics Overview<a class="headerlink" href="#topics-overview" title="Link to this heading">¶</a></h1>
<p>The schedule is preliminary and subject to changes!</p>
<p>The reading for each <em>lecture</em> is given as references to the sources the respective lectures base on. You are <strong>not</strong> obliged to read anything. However, you are strongly <strong>encouraged</strong> to read references marked by pin emojis 📌: those are comprehensive overviews on the topics or important works that are beneficial for a better understanding of the key concepts. For the pinned papers, I also specify the pages span for you to focus on the most important fragments. Some of the sources are also marked with a popcorn emoji 🍿: that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboards etc. (also a couple of LLM-based games). For each of the sources, I also leave my <strong>subjective</strong> estimation of how important this work is for <strong>this specific</strong> topic: from yellow 🟡 <em>‘partially useful’</em> though orange 🟠 <em>‘useful’</em> to red 🔴 ‘<em>crucial findings / thoughts</em>’. These estimations will be continuously updated as I revise the materials.</p>
<p>For the <em>labs</em>, you are provided with practical tutorials that respective lab tasks will mostly derive from. The core tutorials are marked with a writing emoji ✍️; you are <strong>asked</strong> to inspect them <strong>in advance</strong> (better yet: try them out). On lab sessions, we will only <strong>briefly recap</strong> them so it is up to you to prepare in advance to keep up with the lab.</p>
<p><em>Disclaimer</em>: the reading entries are no proper citations; the bibtex references as well as detailed infos about the authors, publish date etc. can be found under the entry links.</p>
<hr class="docutils" />
<section id="block-1-intro">
<h2>Block 1: Intro<a class="headerlink" href="#block-1-intro" title="Link to this heading">¶</a></h2>
<section id="week-1">
<h3>Week 1<a class="headerlink" href="#week-1" title="Link to this heading">¶</a></h3>
<p><a name="2204"></a></p>
<section id="lecture-llms-as-a-form-of-intelligence-vs-llms-as-statistical-machines">
<h4>22.04. <em>Lecture</em>: LLMs as a Form of Intelligence vs LLMs as Statistical Machines<a class="headerlink" href="#lecture-llms-as-a-form-of-intelligence-vs-llms-as-statistical-machines" title="Link to this heading">¶</a></h4>
<p>That is an introductory lecture, in which I will briefly introduce the course and we’ll have a warming up discussion about different perspectives on LLMs’ nature. We will focus on two prominent outlooks: LLM is a form of intelligence and LLM is a complex statistical machine. We’ll discuss differences of LLMs with human intelligence and the degree to which LLMs exhibit (self-)awareness.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Course introduction</p></li>
<li><p>Different perspectives on the nature of LLMs</p></li>
<li><p>Similarities and differences between human and artificial intelligence</p></li>
<li><p>LLMs’ (self-)awareness</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2210.13966">The Debate Over Understanding in AI’s Large Language Models</a> (pages 1-7), <code class="docutils literal notranslate"><span class="pre">Santa</span> <span class="pre">Fe</span> <span class="pre">Institute</span></code> 🟠</p></li>
<li><p><a class="reference external" href="https://direct.mit.edu/daed/article/151/2/183/110604/Do-Large-Language-Models-Understand-Us">Do Large Language Models Understand Us?</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research</span></code> 🟠</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2303.12712">Sparks of Artificial General Intelligence: Early experiments with GPT-4</a> (chapters 1-8 &amp; 10), <code class="docutils literal notranslate"><span class="pre">Microsoft</span> <span class="pre">Research</span></code> 🟡</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2208.02957">Meaning without reference in large language models</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">Berkeley</span> <span class="pre">&amp;</span> <span class="pre">DeepMind</span></code> 🔴</p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜</a> (paragraphs 1, 5, 6.1), <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Washington</span> <span class="pre">et</span> <span class="pre">al.</span></code> 🟡</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2301.06627">Dissociating language and thought in large language models</a> (intro [right after the abstract, see more on the sectioning in this paper at the bottom of page 2], sections 1, 2.3 [<em>LLMs are predictive …</em>], 3-5), <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Texas</span> <span class="pre">at</span> <span class="pre">Austin</span> <span class="pre">et</span> <span class="pre">al.</span></code> 🔴</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.19671">Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding</a>, <code class="docutils literal notranslate"><span class="pre">Leiden</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Advanced</span> <span class="pre">Computer</span> <span class="pre">Science</span> <span class="pre">&amp;</span> <span class="pre">Leiden</span> <span class="pre">University</span> <span class="pre">Medical</span> <span class="pre">Centre</span></code> 🟡</p></li>
</ul>
<p><a name="2404"></a></p>
</section>
<section id="lecture-llm-agent-basics">
<h4>24.04. <em>Lecture</em>: LLM &amp; Agent Basics<a class="headerlink" href="#lecture-llm-agent-basics" title="Link to this heading">¶</a></h4>
<p>In this lecture, we’ll recap some basics about LLMs and LLM-based agents to make sure we’re on the same page.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>LLM architecture recap</p></li>
<li><p>Structured output</p></li>
<li><p>Tool calling</p></li>
<li><p>Piping</p></li>
<li><p>Reasoning</p></li>
<li><p>Multimodality</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2303.18223">A Survey of Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Renmin</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a>, <code class="docutils literal notranslate"><span class="pre">Meta</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.07362">“We Need Structured Output”: Towards User-centered Constraints on Large Language Model Output</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research</span> <span class="pre">&amp;</span> <span class="pre">Google</span></code></p></li>
<li><p><a class="reference external" href="https://openai.com/index/introducing-structured-outputs-in-the-api/">Introducing Structured Outputs in the API</a>, <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2405.17935">Tool Learning with Large Language Models: A Survey</a>, <code class="docutils literal notranslate"><span class="pre">Renmin</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Chin</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.00920">ToolACE: Winning the Points of LLM Function Calling</a>, <code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Noah’s</span> <span class="pre">Ark</span> <span class="pre">Lab</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a>, <code class="docutils literal notranslate"><span class="pre">Meta</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.00121">Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks</a>, <code class="docutils literal notranslate"><span class="pre">IBM</span> <span class="pre">Research</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">Berkeley</span></code> (leaderboard)</p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> (pages 1-9), <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.03710">Agent Instructs Large Language Models to be General Zero-Shot Reasoners</a>,<code class="docutils literal notranslate"><span class="pre">Washington</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">UC</span> <span class="pre">Berkeley</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2306.13549">A Survey on Multimodal Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">Technology</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">&amp;</span> <span class="pre">Tencent</span> <span class="pre">YouTu</span> <span class="pre">Lab</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="week-2">
<h3>Week 2<a class="headerlink" href="#week-2" title="Link to this heading">¶</a></h3>
<p><a name="2904"></a></p>
<section id="debates-llms-as-a-form-of-intelligence-vs-llms-as-statistical-machines">
<h4>29.04. <em>Debates</em>: LLMs as a Form of Intelligence vs LLMs as Statistical Machines<a class="headerlink" href="#debates-llms-as-a-form-of-intelligence-vs-llms-as-statistical-machines" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#2204"><span class="xref myst">session 22.04</span></a></p>
</div></blockquote>
<p>The first debates of the course will revolve around the topic raised in the respective lecture and will utilize concrete evidence in support of the two outlooks on LLMs. There will be two debate rounds (motions will be released on 22.04).</p>
<!-- * LLMs: a Form of Intelligence or a Complex Statistical Machine?
* LLM Behavior: Evidence of Awareness or Illusion of Understanding? -->
<p><strong>Reading</strong>: see <a class="reference internal" href="#2204"><span class="xref myst">session 22.04</span></a></p>
</section>
<section id="id1">
<h4>01.05.<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h4>
<p><em>Ausfalltermin</em></p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="block-2-core-topics">
<h2>Block 2: Core Topics<a class="headerlink" href="#block-2-core-topics" title="Link to this heading">¶</a></h2>
</section>
<section id="part-1-business-applications">
<h2>Part 1: Business Applications<a class="headerlink" href="#part-1-business-applications" title="Link to this heading">¶</a></h2>
<section id="week-3">
<h3>Week 3<a class="headerlink" href="#week-3" title="Link to this heading">¶</a></h3>
<p><a name="0605"></a></p>
<section id="lecture-virtual-assistants-pt-1-chatbots">
<h4>06.05. <em>Lecture</em>: Virtual Assistants Pt. 1: Chatbots<a class="headerlink" href="#lecture-virtual-assistants-pt-1-chatbots" title="Link to this heading">¶</a></h4>
<p>The first core topic concerns chatbots. We’ll discuss how chatbots are built, how they (should) handle harmful requests and you can tune it for your use case.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>LLMs under the hood: alignment, harmlessness, honesty</p></li>
<li><p>Prompting &amp; automated prompt generation</p></li>
<li><p>Chat memory</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2307.12966">Aligning Large Language Models with Human: A Survey</a> (pages 1-14), <code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Noah’s</span> <span class="pre">Ark</span> <span class="pre">Lab</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>, <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2204.05862">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a>, <code class="docutils literal notranslate"><span class="pre">Anthropic</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.07927">A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications</a>, <code class="docutils literal notranslate"><span class="pre">Indian</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">Patna,</span> <span class="pre">Stanford</span> <span class="pre">&amp;</span> <span class="pre">Amazon</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.02717">Automatic Prompt Selection for Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Cinnamon</span> <span class="pre">AI,</span> <span class="pre">Hung</span> <span class="pre">Yen</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">and</span> <span class="pre">Education</span> <span class="pre">&amp;</span> <span class="pre">Deakin</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://aclanthology.org/2022.findings-naacl.3/">PromptGen: Automatically Generate Prompts using Generative Models</a>, <code class="docutils literal notranslate"><span class="pre">Baidu</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.13501">A Survey on the Memory Mechanism of Large Language Model based Agents</a>, <code class="docutils literal notranslate"><span class="pre">Renmin</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">&amp;</span> <span class="pre">Huawei</span> <span class="pre">Noah’s</span> <span class="pre">Ark</span> <span class="pre">Lab</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2306.07174">Augmenting Language Models with Long-Term Memory</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">Santa</span> <span class="pre">Barbara</span> <span class="pre">&amp;</span> <span class="pre">Microsoft</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2401.02777">From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Beike</span> <span class="pre">Inc.</span></code></p></li>
</ul>
<p><a name="0805"></a></p>
</section>
<section id="lab-chatbot">
<h4>08.05. <em>Lab</em>: Chatbot<a class="headerlink" href="#lab-chatbot" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0605"><span class="xref myst">session 06.05</span></a></p>
</div></blockquote>
<p>In this lab, we’ll build a chatbot and try different prompts and settings to see how it affects the output.</p>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/chat_models/">Chat models</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://python.langchain.com/docs/concepts/prompt_templates/">Prompt Templates</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/few_shot_prompting/">Few-shot prompting</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/multimodality/">Multimodality</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://python.langchain.com/docs/concepts/structured_outputs/">Structured outputs</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/tools/">Tools</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://python.langchain.com/docs/concepts/tool_calling/">Tool calling</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/runnables/">Runnable interface</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://python.langchain.com/docs/concepts/lcel/#should-i-use-lcel">LangChain Expression Language (LCEL)</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/introduction/">LangGraph Quickstart Parts 1-3: Build a Basic Chatbot</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="week-4">
<h3>Week 4<a class="headerlink" href="#week-4" title="Link to this heading">¶</a></h3>
<p><a name="1305"></a></p>
<section id="lecture-virtual-assistants-pt-2-rag">
<h4>13.05. <em>Lecture</em>: Virtual Assistants Pt. 2: RAG<a class="headerlink" href="#lecture-virtual-assistants-pt-2-rag" title="Link to this heading">¶</a></h4>
<p>Continuing the first part, the second part will expand scope of chatbot functionality and will teach it to refer to custom knowledge base to retrieve and use user-specific information. Finally, the most widely used deployment methods will be briefly introduced.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>General knowledge vs context</p></li>
<li><p>Knowledge indexing, retrieval &amp; ranking</p></li>
<li><p>Retrieval tools</p></li>
<li><p>Agentic RAG</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.16833">Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">DeepMind</span> <span class="pre">&amp;</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Michigan</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2412.15605">Don’t Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks</a>, <code class="docutils literal notranslate"><span class="pre">National</span> <span class="pre">Chengchi</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Academia</span> <span class="pre">Sinica</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2404.10981">A Survey on Retrieval-Augmented Text Generation for Large Language Models</a> (pages 1-19), <code class="docutils literal notranslate"><span class="pre">York</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.14403">Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity</a>, <code class="docutils literal notranslate"><span class="pre">Korea</span> <span class="pre">Advanced</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2502.00032">Querying Databases with Function Calling</a>, <code class="docutils literal notranslate"><span class="pre">Weaviate,</span> <span class="pre">Contextual</span> <span class="pre">AI</span> <span class="pre">&amp;</span> <span class="pre">Morningstar</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2411.19443">Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Chinese</span> <span class="pre">Academy</span> <span class="pre">of</span> <span class="pre">Sciences</span></code></p></li>
</ul>
<p><a name="1505"></a></p>
</section>
<section id="lab-rag-chatbot">
<h4>15.05. <em>Lab</em>: RAG Chatbot<a class="headerlink" href="#lab-rag-chatbot" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#1305"><span class="xref myst">session 13.05</span></a></p>
</div></blockquote>
<p>In this lab, we’ll expand the functionality of the chatbot built at the last lab to connect it to user-specific information.</p>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://python.langchain.com/docs/how_to/document_loader_pdf/">How to load PDFs</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/text_splitters/">Text splitters</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/embedding_models/">Embedding models</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/vectorstores/">Vector stores</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://python.langchain.com/docs/concepts/retrievers/">Retrievers</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://python.langchain.com/docs/concepts/rag/">Retrieval augmented generation (RAG)</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/">Agentic RAG</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/">Adaptive RAG</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="week-5">
<h3>Week 5<a class="headerlink" href="#week-5" title="Link to this heading">¶</a></h3>
<p><a name="2005"></a></p>
<section id="lecture-virtual-assistants-pt-3-multi-agent-environment">
<h4>20.05. <em>Lecture</em>: Virtual Assistants Pt. 3: Multi-agent Environment<a class="headerlink" href="#lecture-virtual-assistants-pt-3-multi-agent-environment" title="Link to this heading">¶</a></h4>
<p>This lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday / business operations in a multi-agent environment. We’ll look at how agents communicate with each other, how their communication can be guided (both with and without involvement of a human), and this all is used in real applications.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Multi-agent environment</p></li>
<li><p>Human in the Loop</p></li>
<li><p>Examples of pipelines for business operations</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2411.14033">LLM-based Multi-Agent Systems: Techniques and Business Perspectives</a> (pages 1-8), <code class="docutils literal notranslate"><span class="pre">Shanghai</span> <span class="pre">Jiao</span> <span class="pre">Tong</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">OPPO</span> <span class="pre">Research</span> <span class="pre">Institute</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2304.03442">Generative Agents: Interactive Simulacra of Human Behavior</a>, <code class="docutils literal notranslate"><span class="pre">Stanford,</span> <span class="pre">Google</span> <span class="pre">Research</span> <span class="pre">&amp;</span> <span class="pre">DeepMind</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2305.14325">Improving Factuality and Reasoning in Language Models through Multiagent Debate</a>, <code class="docutils literal notranslate"><span class="pre">MIT</span> <span class="pre">&amp;</span> <span class="pre">Google</span> <span class="pre">Brain</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.02124">Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</a>, <code class="docutils literal notranslate"><span class="pre">Zhejiang</span> <span class="pre">University,</span> <span class="pre">National</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Singapore</span> <span class="pre">&amp;</span> <span class="pre">DeepMind</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2308.08155">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span> <span class="pre">Research</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://blogs.microsoft.com/blog/2025/03/10/https-blogs-microsoft-com-blog-2024-11-12-how-real-world-businesses-are-transforming-with-ai/">How real-world businesses are transforming with AI — with more than 140 new stories</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span></code> (blog post)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2502.01390">Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant</a>, <code class="docutils literal notranslate"><span class="pre">Delft</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">&amp;</span> <span class="pre">The</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Queensland</span></code></p></li>
</ul>
<p><a name="2205"></a></p>
</section>
<section id="lab-multi-agent-environment">
<h4>22.05. <em>Lab</em>: Multi-agent Environment<a class="headerlink" href="#lab-multi-agent-environment" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#2005"><span class="xref myst">session 20.05</span></a></p>
</div></blockquote>
<p>This lab will introduce a short walkthrough to creation of a multi-agent environment for automated meeting scheduling and preparation. We will see how the coordinator agent will communicate with two auxiliary agents to check time availability and prepare an agenda for the meeting.</p>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>✍️ <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/">Multi-agent network</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute/">Plan-and-Execute</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/reflexion/reflexion/">Reflexion</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/">Multi-agent supervisor</a>, <code class="docutils literal notranslate"><span class="pre">LangChain</span></code></p></li>
<li><p><a class="reference external" href="https://microsoft.github.io/autogen/stable//user-guide/core-user-guide/quickstart.html">Quick Start</a>, <code class="docutils literal notranslate"><span class="pre">AutoGen</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="week-6">
<h3>Week 6<a class="headerlink" href="#week-6" title="Link to this heading">¶</a></h3>
<p><a name="2705"></a></p>
<section id="lecture-software-development-pt-1-code-generation-evaluation-testing">
<h4>27.05. <em>Lecture</em>: Software Development Pt. 1: Code Generation, Evaluation &amp; Testing<a class="headerlink" href="#lecture-software-development-pt-1-code-generation-evaluation-testing" title="Link to this heading">¶</a></h4>
<p>This lectures opens a new lecture mini-cycle dedicated to software development. The first lecture overviews how LLMs are used to generate reliable code and how generated code is tested and improved to deal with the errors.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Code generation &amp; refining</p></li>
<li><p>Automated testing</p></li>
<li><p>Code evaluation &amp; benchmarks</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.02977">Large Language Model-Based Agents for Software Engineering: A Survey</a>, <code class="docutils literal notranslate"><span class="pre">Fudan</span> <span class="pre">University,</span> <span class="pre">Nanyang</span> <span class="pre">Technological</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Illinois</span> <span class="pre">at</span> <span class="pre">Urbana-Champaign</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2207.01780">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</a> (pages 1-20), <code class="docutils literal notranslate"><span class="pre">Salesforce</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2303.05510">Planning with Large Language Models for Code Generation</a>, <code class="docutils literal notranslate"><span class="pre">MIT-IBM</span> <span class="pre">Watson</span> <span class="pre">AI</span> <span class="pre">Lab</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2311.07961">The ART of LLM Refinement: Ask, Refine, and Trust</a>, <code class="docutils literal notranslate"><span class="pre">ETH</span> <span class="pre">Zurich</span> <span class="pre">&amp;</span> <span class="pre">Meta</span> <span class="pre">AI</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2405.17503">Code Repair with LLMs gives an Exploration-Exploitation Tradeoff</a>, <code class="docutils literal notranslate"><span class="pre">Cornell,</span> <span class="pre">Shanghai</span> <span class="pre">Jiao</span> <span class="pre">Tong</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Toronto</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2305.04764">ChatUniTest: A Framework for LLM-Based Test Generation</a>, <code class="docutils literal notranslate"><span class="pre">Zhejiang</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Hangzhou</span> <span class="pre">City</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2408.03095">TestART: Improving LLM-based Unit Testing via Co-evolution of Automated Generation and Repair Iteration</a>, <code class="docutils literal notranslate"><span class="pre">Nanjing</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Huawei</span> <span class="pre">Cloud</span> <span class="pre">Computing</span> <span class="pre">Technologies</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2408.16498">A Survey on Evaluating Large Language Models in Code Generation Tasks</a>, <code class="docutils literal notranslate"><span class="pre">Peking</span> <span class="pre">University,</span> <span class="pre">Microsoft</span> <span class="pre">Research</span> <span class="pre">&amp;</span> <span class="pre">Tokyo</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a>, `OpenAI</p></li>
<li><p>🍿 <a class="reference external" href="https://paperswithcode.com/sota/code-generation-on-humaneval">Code Generation on HumanEval</a>, <code class="docutils literal notranslate"><span class="pre">OpenAI</span></code> (leaderboard)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2406.15877">BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</a>, <code class="docutils literal notranslate"><span class="pre">Monash</span> <span class="pre">University</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2410.02184">CodeJudge: Evaluating Code Generation with Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Huazhong</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">Technology</span> <span class="pre">&amp;</span> <span class="pre">Purdue</span> <span class="pre">University</span></code></p></li>
</ul>
</section>
<section id="id2">
<h4>29.05.<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h4>
<p><em>Ausfalltermin</em></p>
</section>
</section>
<hr class="docutils" />
<section id="week-7">
<h3>Week 7<a class="headerlink" href="#week-7" title="Link to this heading">¶</a></h3>
<p><a name="0306"></a></p>
<section id="lecture-software-development-pt-2-copilots-llm-powered-websites">
<h4>03.06. <em>Lecture</em>: Software Development Pt. 2: Copilots, LLM-powered Websites<a class="headerlink" href="#lecture-software-development-pt-2-copilots-llm-powered-websites" title="Link to this heading">¶</a></h4>
<p>The second and the last lecture of the software development cycle focuses on practical application of LLM code generation, in particular, on widely-used copilots (real-time code generation assistants) and LLM-supported web development.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Copilots &amp; real-time hints</p></li>
<li><p>LLM-powered websites</p></li>
<li><p>LLM-supported deployment</p></li>
<li><p>Further considerations: reliability, sustainability etc.</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.09512">Design and evaluation of AI copilots – case studies of retail copilot templates</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://blogs.microsoft.com/blog/2025/04/04/your-ai-companion/">Your AI Companion</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span></code> (blog post)</p></li>
<li><p><a class="reference external" href="https://github.com/features/copilot">GitHub Copilot</a>, <code class="docutils literal notranslate"><span class="pre">GitHub</span></code> (product page)</p></li>
<li><p>🍿 <a class="reference external" href="https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/">Research: quantifying GitHub Copilot’s impact on developer productivity and happiness</a>, <code class="docutils literal notranslate"><span class="pre">GitHub</span></code> (blog post)</p></li>
<li><p><a class="reference external" href="https://www.cursor.com">Cursor: The AI Code Editor</a>, <code class="docutils literal notranslate"><span class="pre">Cursor</span></code> (product page)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.09171">Automated Unit Test Improvement using Large Language Models at Meta</a>, <code class="docutils literal notranslate"><span class="pre">Meta</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2411.12924">Human-In-the-Loop Software Development Agents</a>, <code class="docutils literal notranslate"><span class="pre">Monash</span> <span class="pre">University,</span> <span class="pre">The</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Melbourne</span> <span class="pre">&amp;</span> <span class="pre">Atlassian</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2404.14459">LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities and Limitations</a> (pages 1-11), <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Oslo</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2307.12856">A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">DeepMind</span> <span class="pre">&amp;</span> <span class="pre">The</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Tokyo</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2502.13681">An LLM-based Agent for Reliable Docker Environment Configuration</a>, <code class="docutils literal notranslate"><span class="pre">Harbin</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">&amp;</span> <span class="pre">ByteDance</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2308.10335">Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation</a>, <code class="docutils literal notranslate"><span class="pre">UC</span> <span class="pre">San</span> <span class="pre">Diego</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.03344">Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation</a>, <code class="docutils literal notranslate"><span class="pre">TWT</span> <span class="pre">GmbH</span> <span class="pre">Science</span> <span class="pre">&amp;</span> <span class="pre">Innovation</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2310.16263">Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation</a>, <code class="docutils literal notranslate"><span class="pre">South</span> <span class="pre">China</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span> <span class="pre">&amp;</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Innsbruck</span></code></p></li>
</ul>
<p><a name="0506"></a></p>
</section>
<section id="lab-llm-powered-website">
<h4>05.06 <em>Lab</em>: LLM-powered Website<a class="headerlink" href="#lab-llm-powered-website" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0306"><span class="xref myst">session 03.06</span></a></p>
</div></blockquote>
<p>In this lab, we’ll have the LLM make a website for us: it will both generate the contents of the website and generate all the code required for rendering, styling and navigation.</p>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>see <a class="reference internal" href="#2205"><span class="xref myst">session 22.05</span></a></p></li>
<li><p>✍️ <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Getting_started/Your_first_website/Creating_the_content">HTML: Creating the content</a>, <code class="docutils literal notranslate"><span class="pre">MDN</span></code></p></li>
<li><p>✍️ <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Styling_basics/Getting_started">Getting started with CSS</a>, <code class="docutils literal notranslate"><span class="pre">MDN</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="week-8-having-some-rest">
<h3>Week 8: Having Some Rest<a class="headerlink" href="#week-8-having-some-rest" title="Link to this heading">¶</a></h3>
<section id="id3">
<h4>10.06.<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h4>
<p><em>Ausfalltermin</em></p>
</section>
<section id="id4">
<h4>12.06.<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h4>
<p><em>Ausfalltermin</em></p>
</section>
</section>
<hr class="docutils" />
<section id="week-9">
<h3>Week 9<a class="headerlink" href="#week-9" title="Link to this heading">¶</a></h3>
<p><a name="1706"></a></p>
<section id="pitch-rag-chatbot">
<h4>17.06. <em>Pitch</em>: RAG Chatbot<a class="headerlink" href="#pitch-rag-chatbot" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0605"><span class="xref myst">session 06.05</span></a> and <a class="reference internal" href="#1305"><span class="xref myst">session 13.05</span></a></p>
</div></blockquote>
<p>The first pitch will be dedicated to a custom RAG chatbot that the <em>contractors</em> (the presenting students, see the <a class="reference internal" href="#./Formats/Pitches.md"><span class="xref myst">infos about Pitches</span></a>) will have prepared to present. The RAG chatbot will have to be able to retrieve specific information from the given documents (not from the general knowledge!) and use it in its responses. Specific requirements will be released on 22.05.</p>
<p><strong>Reading</strong>: see <a class="reference internal" href="#0605"><span class="xref myst">session 06.05</span></a>, <a class="reference internal" href="#0805"><span class="xref myst">session 08.05</span></a>, <a class="reference internal" href="#1305"><span class="xref myst">session 13.05</span></a>, and <a class="reference internal" href="#1505"><span class="xref myst">session 15.05</span></a></p>
</section>
<section id="id5">
<h4>19.06.<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h4>
<p><em>Ausfalltermin</em></p>
</section>
</section>
<hr class="docutils" />
<section id="week-10">
<h3>Week 10<a class="headerlink" href="#week-10" title="Link to this heading">¶</a></h3>
<p><a name="2406"></a></p>
<section id="pitch-handling-customer-requests-in-a-multi-agent-environment">
<h4>24.06. <em>Pitch</em>: Handling Customer Requests in a Multi-agent Environment<a class="headerlink" href="#pitch-handling-customer-requests-in-a-multi-agent-environment" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#2005"><span class="xref myst">session 20.05</span></a></p>
</div></blockquote>
<p>In the second pitch, the <em>contractors</em> will present their solution to automated handling of customer requests. The solution will have to introduce a multi-agent environment to take off working load from an imagined support team. The solution will have to read and categorize tickets, generate replies and (in case of need) notify the human that their interference is required. Specific requirements will be released on 27.05.</p>
<p><strong>Reading</strong>: see <a class="reference internal" href="#2005"><span class="xref myst">session 20.05</span></a> and <a class="reference internal" href="#2205"><span class="xref myst">session 22.05</span></a></p>
<p><a name="2606"></a></p>
</section>
<section id="lecture-other-business-applications-game-design-financial-analysis-etc">
<h4>26.06. <em>Lecture</em>: Other Business Applications: Game Design, Financial Analysis etc.<a class="headerlink" href="#lecture-other-business-applications-game-design-financial-analysis-etc" title="Link to this heading">¶</a></h4>
<p>This lecture will serve a small break and will briefly go over other business scenarios that the LLMs are used in.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Game design &amp; narrative games</p></li>
<li><p>Financial applications</p></li>
<li><p>Content creation</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.17027">Player-Driven Emergence in LLM-Driven Game Narrative</a>, <code class="docutils literal notranslate"><span class="pre">Microsoft</span> <span class="pre">Research</span></code></p></li>
<li><p><a class="reference external" href="https://aclanthology.org/2024.games-1.6/">Generating Converging Narratives for Games with Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">U.S.</span> <span class="pre">Army</span> <span class="pre">Research</span> <span class="pre">Laboratory</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.07442">Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Tokyo</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://play.aidungeon.com">AI Dungeon Games</a>, <code class="docutils literal notranslate"><span class="pre">AI</span> <span class="pre">Dungeon</span></code> (game catalogue)</p></li>
<li><p>🍿 <a class="reference external" href="https://www.convex.dev/ai-town">AI Town</a>, <code class="docutils literal notranslate"><span class="pre">Andreessen</span> <span class="pre">Horowitz</span> <span class="pre">&amp;</span> <span class="pre">Convex</span></code> (game)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/blog/npc-gigax-cubzh?utm_source=chatgpt.com">Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs</a>, <code class="docutils literal notranslate"><span class="pre">HuggingFace</span></code> (blog post)</p></li>
<li><p><a class="reference external" href="https://github.com/bliporg/blip">Blip</a>, <code class="docutils literal notranslate"><span class="pre">bliporg</span></code> (GitHub repo)</p></li>
<li><p><a class="reference external" href="https://github.com/GigaxGames/gigax">gigax</a>, <code class="docutils literal notranslate"><span class="pre">GigaxGames</span></code> (GitHub repo)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2311.10723">Large Language Models in Finance: A Survey</a>, <code class="docutils literal notranslate"><span class="pre">Columbia</span> <span class="pre">&amp;</span> <span class="pre">New</span> <span class="pre">York</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2403.12285">FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications</a>, <code class="docutils literal notranslate"><span class="pre">Imperial</span> <span class="pre">College</span> <span class="pre">London</span> <span class="pre">&amp;</span> <span class="pre">MIT</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2401.15328">Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance</a>, <code class="docutils literal notranslate"><span class="pre">Monash</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2401.12224">LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation</a>, <code class="docutils literal notranslate"><span class="pre">Shanghai</span> <span class="pre">Jiao</span> <span class="pre">Tong</span> <span class="pre">University</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2402.14207">Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Stanford</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.11891">Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools</a>, <code class="docutils literal notranslate"><span class="pre">MIT,</span> <span class="pre">Harvard</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">MIT-IBM</span> <span class="pre">Watson</span> <span class="pre">AI</span> <span class="pre">Lab</span></code></p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="part-2-applications-in-science">
<h2>Part 2: Applications in Science<a class="headerlink" href="#part-2-applications-in-science" title="Link to this heading">¶</a></h2>
<section id="week-11">
<h3>Week 11<a class="headerlink" href="#week-11" title="Link to this heading">¶</a></h3>
<p><a name="0107"></a></p>
<section id="lecture-llms-in-research-experiment-planning-hypothesis-generation">
<h4>01.07. <em>Lecture</em>: LLMs in Research: Experiment Planning &amp; Hypothesis Generation<a class="headerlink" href="#lecture-llms-in-research-experiment-planning-hypothesis-generation" title="Link to this heading">¶</a></h4>
<p>The first lecture dedicated to scientific applications shows how LLMs are used to plan experiments and generate hypothesis to accelerate research.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Experiment planning</p></li>
<li><p>Hypothesis generation</p></li>
<li><p>Predicting possible results</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2404.04326">Hypothesis Generation with Large Language Models</a> (pages 1-9), <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Chicago</span> <span class="pre">&amp;</span> <span class="pre">Toyota</span> <span class="pre">Technological</span> <span class="pre">Institute</span> <span class="pre">at</span> <span class="pre">Chicago</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2411.02382">Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Virginia</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2311.16733">LLMs for Science: Usage for Code Generation and Data Analysis</a> (pages 1-6), <code class="docutils literal notranslate"><span class="pre">TUM</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2304.05332">Emergent autonomous scientific research capabilities of large language models</a>, <code class="docutils literal notranslate"><span class="pre">Carnegie</span> <span class="pre">Mellon</span> <span class="pre">University</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.04593">Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Illinois</span> <span class="pre">at</span> <span class="pre">Urbana-Champaign,</span> <span class="pre">Carnegie</span> <span class="pre">Mellon</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Carleton</span> <span class="pre">College</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2408.15545">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">Technology</span> <span class="pre">of</span> <span class="pre">China</span> <span class="pre">&amp;</span> <span class="pre">DP</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.01268">Mapping the Increasing Use of LLMs in Scientific Papers</a>, <code class="docutils literal notranslate"><span class="pre">Stanford</span></code></p></li>
</ul>
<p><a name="0307"></a></p>
</section>
<section id="lab-experiment-planning-hypothesis-generation">
<h4>03.07: <em>Lab</em>: Experiment Planning &amp; Hypothesis Generation<a class="headerlink" href="#lab-experiment-planning-hypothesis-generation" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0107"><span class="xref myst">session 01.07</span></a></p>
</div></blockquote>
<p>In this lab, we’ll practice in facilitating researcher’s work with LLMs on the example of a toy scientific research.</p>
<p><strong>Reading</strong>: see <a class="reference internal" href="#2205"><span class="xref myst">session 22.05</span></a></p>
</section>
</section>
<hr class="docutils" />
<section id="week-12">
<h3>Week 12<a class="headerlink" href="#week-12" title="Link to this heading">¶</a></h3>
<p><a name="0807"></a></p>
<section id="pitch-agent-for-code-generation">
<h4>08.07: <em>Pitch</em>: Agent for Code Generation<a class="headerlink" href="#pitch-agent-for-code-generation" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#2705"><span class="xref myst">session 27.05</span></a></p>
</div></blockquote>
<p>This pitch will revolve around the <em>contractors’</em> implementation of a self-improving code generator. The code generator will have to generate both scripts and test cases for a problem given in the input prompt, run the tests and refine the code if needed. Specific requirements will be released on 17.06.</p>
<p><strong>Reading</strong>: see <a class="reference internal" href="#2705"><span class="xref myst">session 27.05</span></a> and <a class="reference internal" href="#0506"><span class="xref myst">session 05.06</span></a></p>
<p><a name="1007"></a></p>
</section>
<section id="lecture-other-applications-in-science-drug-discovery-math-etc-scientific-reliability">
<h4>10.07. <em>Lecture</em>: Other Applications in Science: Drug Discovery, Math etc. &amp; Scientific Reliability<a class="headerlink" href="#lecture-other-applications-in-science-drug-discovery-math-etc-scientific-reliability" title="Link to this heading">¶</a></h4>
<p>The final core topic will mention other scientific applications of LLMs that were not covered in the previous lectures and address the question of reliability of the results obtained with LLMs.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Drug discovery, math &amp; other applications</p></li>
<li><p>Scientific confidence &amp; reliability</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2406.10833">A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Illinois</span> <span class="pre">at</span> <span class="pre">Urbana-Champaign</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.04481">Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials</a>, <code class="docutils literal notranslate"><span class="pre">Department</span> <span class="pre">of</span> <span class="pre">Data</span> <span class="pre">Science</span> <span class="pre">and</span> <span class="pre">AI,</span> <span class="pre">Monash</span> <span class="pre">University</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2404.18400">LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</a>, <code class="docutils literal notranslate"><span class="pre">Virginia</span> <span class="pre">Tech</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models">Awesome Scientific Language Models</a>, <code class="docutils literal notranslate"><span class="pre">yuzhimanhua</span></code> (GitHub repo)</p></li>
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2409.14037">Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators</a> (pages 1-9), <code class="docutils literal notranslate"><span class="pre">Indian</span> <span class="pre">Institute</span> <span class="pre">of</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2503.13517">CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning</a>, <code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2501.09775">Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong</a>, <code class="docutils literal notranslate"><span class="pre">Nanjing</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Aeronautics</span> <span class="pre">and</span> <span class="pre">Astronautics</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="block-3-wrap-up">
<h2>Block 3: Wrap-up<a class="headerlink" href="#block-3-wrap-up" title="Link to this heading">¶</a></h2>
<section id="week-13">
<h3>Week 13<a class="headerlink" href="#week-13" title="Link to this heading">¶</a></h3>
<p><a name="1507"></a></p>
<section id="pitch-agent-for-web-development">
<h4>15.07. <em>Pitch</em>: Agent for Web Development<a class="headerlink" href="#pitch-agent-for-web-development" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0306"><span class="xref myst">session 03.06</span></a></p>
</div></blockquote>
<p>The <em>contractors</em> will present their agent that will have to generate full (minimalistic) websites by a prompt. For each website, the agent will have to generate its own style and a simple menu with working navigation as well as the contents. Specific requirements will be released on 24.06.</p>
<p><strong>Reading</strong>: see <a class="reference internal" href="#0306"><span class="xref myst">session 03.06</span></a> and <a class="reference internal" href="#0506"><span class="xref myst">session 05.06</span></a></p>
<p><a name="1707"></a></p>
</section>
<section id="lecture-role-of-ai-in-recent-years">
<h4>17.07. <em>Lecture</em>: Role of AI in Recent Years<a class="headerlink" href="#lecture-role-of-ai-in-recent-years" title="Link to this heading">¶</a></h4>
<p>The last lecture of the course will turn to societal considerations regarding LLMs and AI in general and will investigate its role and influence on the humanity nowadays.</p>
<p><strong>Key points</strong>:</p>
<ul class="simple">
<li><p>Studies on influence of AI in the recent years</p></li>
<li><p>Studies on AI integration rate</p></li>
<li><p>Ethical, legal &amp; environmental aspects</p></li>
</ul>
<p><strong>Reading</strong>:</p>
<ul class="simple">
<li><p>📌 <a class="reference external" href="https://arxiv.org/abs/2502.12447">Protecting Human Cognition in the Age of AI</a> (pages 1-5), The University of Texas at Austin et al.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2412.03963">Augmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI’s Impact on Creative Tasks</a>, <code class="docutils literal notranslate"><span class="pre">Tsinghua</span> <span class="pre">University</span> <span class="pre">&amp;</span> <span class="pre">Wuhan</span> <span class="pre">University</span> <span class="pre">of</span> <span class="pre">Technology</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2410.03703">Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking</a>, <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Toronto</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2409.01754v1">Empirical evidence of Large Language Model’s influence on human spoken communication</a>, <code class="docutils literal notranslate"><span class="pre">Max-Planck</span> <span class="pre">Institute</span> <span class="pre">for</span> <span class="pre">Human</span> <span class="pre">Development</span></code></p></li>
<li><p>🍿 <a class="reference external" href="https://hai.stanford.edu/ai-index/2025-ai-index-report">The 2025 AI Index Report: Top Takeaways</a>, <code class="docutils literal notranslate"><span class="pre">Stanford</span></code></p></li>
<li><p><a class="reference external" href="https://ai.wharton.upenn.edu/focus-areas/human-technology-interaction/2024-ai-adoption-report/">Growing Up: Navigating Generative AI’s Early Years – AI Adoption Report: Executive Summary</a>, <code class="docutils literal notranslate"><span class="pre">AI</span> <span class="pre">at</span> <span class="pre">Wharton</span></code></p></li>
<li><p>📌 <a class="reference external" href="https://onlinelibrary.wiley.com/doi/full/10.1111/exsy.13406">Artificial intelligence governance: Ethical considerations and implications for social responsibility</a> (pages 1-12), <code class="docutils literal notranslate"><span class="pre">University</span> <span class="pre">of</span> <span class="pre">Malta</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2503.14539">Ethical Implications of AI in Data Collection: Balancing Innovation with Privacy</a>, <code class="docutils literal notranslate"><span class="pre">AI</span> <span class="pre">Data</span> <span class="pre">Chronicles</span></code></p></li>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007/s43681-024-00644-x">Legal and ethical implications of AI-based crowd analysis: the AI Act and beyond</a>, <code class="docutils literal notranslate"><span class="pre">Vrije</span> <span class="pre">Universiteit</span></code></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2412.04782v1">A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges</a>, <code class="docutils literal notranslate"><span class="pre">Cleveland</span> <span class="pre">State</span> <span class="pre">University</span> <span class="pre">et</span> <span class="pre">al.</span></code></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="week-14">
<h3>Week 14<a class="headerlink" href="#week-14" title="Link to this heading">¶</a></h3>
<p><a name="2207"></a></p>
<section id="pitch-llm-based-research-assistant">
<h4>22.07. <em>Pitch</em>: LLM-based Research Assistant<a class="headerlink" href="#pitch-llm-based-research-assistant" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#0107"><span class="xref myst">session 01.07</span></a></p>
</div></blockquote>
<p>The last pitch will introduce an agent that will have to plan the research, generate hypotheses, find the literature etc. for a given scientific problem. It will then have to introduce its results in form of a TODO or a guide for the researcher to start off of. Specific requirements will be released on 01.07.</p>
<p><strong>Reading</strong>: see <a class="reference internal" href="#0107"><span class="xref myst">session 01.07</span></a> and <a class="reference internal" href="#0307"><span class="xref myst">session 03.07</span></a></p>
<p><a name="2407"></a></p>
</section>
<section id="debate-role-of-ai-in-recent-years-wrap-up">
<h4>24.07. <em>Debate</em>: Role of AI in Recent Years + Wrap-up<a class="headerlink" href="#debate-role-of-ai-in-recent-years-wrap-up" title="Link to this heading">¶</a></h4>
<blockquote>
<div><p>On material of <a class="reference internal" href="#1707"><span class="xref myst">session 17.07</span></a></p>
</div></blockquote>
<p>The course will be concluded by the final debate (motions will be released on 17.07), after which a short course summary and a Q&amp;A session will be held.</p>
<!-- * Should We Limit the Usage of AI? -->
<p><strong>Reading</strong>: see <a class="reference internal" href="#1707"><span class="xref myst">session 17.07</span></a></p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./infos"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="formats/debates.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Debates</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../index.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Home</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, Maksim Shmalts
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants" aria-label="GitHub"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
          <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
</svg>
</a>
              <a class="muted-link footer-icon" href="https://github.com/maxschmaltz/Course-LLM-based-Assistants/issues/new" aria-label="Issues"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 18 18">
  <path d="M8 15A7 7 0 1 0 8 1a7 7 0 0 0 0 14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"/>
  <path d="M7.002 11a1 1 0 1 1 2 0 1 1 0 0 1-2 0zm.1-6.995a.905.905 0 0 1 1.8 0l-.35 4.5a.55.55 0 0 1-1.1 0l-.35-4.5z"/>
</svg>
</a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Topics Overview</a><ul>
<li><a class="reference internal" href="#block-1-intro">Block 1: Intro</a><ul>
<li><a class="reference internal" href="#week-1">Week 1</a><ul>
<li><a class="reference internal" href="#lecture-llms-as-a-form-of-intelligence-vs-llms-as-statistical-machines">22.04. <em>Lecture</em>: LLMs as a Form of Intelligence vs LLMs as Statistical Machines</a></li>
<li><a class="reference internal" href="#lecture-llm-agent-basics">24.04. <em>Lecture</em>: LLM &amp; Agent Basics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-2">Week 2</a><ul>
<li><a class="reference internal" href="#debates-llms-as-a-form-of-intelligence-vs-llms-as-statistical-machines">29.04. <em>Debates</em>: LLMs as a Form of Intelligence vs LLMs as Statistical Machines</a></li>
<li><a class="reference internal" href="#id1">01.05.</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#block-2-core-topics">Block 2: Core Topics</a></li>
<li><a class="reference internal" href="#part-1-business-applications">Part 1: Business Applications</a><ul>
<li><a class="reference internal" href="#week-3">Week 3</a><ul>
<li><a class="reference internal" href="#lecture-virtual-assistants-pt-1-chatbots">06.05. <em>Lecture</em>: Virtual Assistants Pt. 1: Chatbots</a></li>
<li><a class="reference internal" href="#lab-chatbot">08.05. <em>Lab</em>: Chatbot</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-4">Week 4</a><ul>
<li><a class="reference internal" href="#lecture-virtual-assistants-pt-2-rag">13.05. <em>Lecture</em>: Virtual Assistants Pt. 2: RAG</a></li>
<li><a class="reference internal" href="#lab-rag-chatbot">15.05. <em>Lab</em>: RAG Chatbot</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-5">Week 5</a><ul>
<li><a class="reference internal" href="#lecture-virtual-assistants-pt-3-multi-agent-environment">20.05. <em>Lecture</em>: Virtual Assistants Pt. 3: Multi-agent Environment</a></li>
<li><a class="reference internal" href="#lab-multi-agent-environment">22.05. <em>Lab</em>: Multi-agent Environment</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-6">Week 6</a><ul>
<li><a class="reference internal" href="#lecture-software-development-pt-1-code-generation-evaluation-testing">27.05. <em>Lecture</em>: Software Development Pt. 1: Code Generation, Evaluation &amp; Testing</a></li>
<li><a class="reference internal" href="#id2">29.05.</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-7">Week 7</a><ul>
<li><a class="reference internal" href="#lecture-software-development-pt-2-copilots-llm-powered-websites">03.06. <em>Lecture</em>: Software Development Pt. 2: Copilots, LLM-powered Websites</a></li>
<li><a class="reference internal" href="#lab-llm-powered-website">05.06 <em>Lab</em>: LLM-powered Website</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-8-having-some-rest">Week 8: Having Some Rest</a><ul>
<li><a class="reference internal" href="#id3">10.06.</a></li>
<li><a class="reference internal" href="#id4">12.06.</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-9">Week 9</a><ul>
<li><a class="reference internal" href="#pitch-rag-chatbot">17.06. <em>Pitch</em>: RAG Chatbot</a></li>
<li><a class="reference internal" href="#id5">19.06.</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-10">Week 10</a><ul>
<li><a class="reference internal" href="#pitch-handling-customer-requests-in-a-multi-agent-environment">24.06. <em>Pitch</em>: Handling Customer Requests in a Multi-agent Environment</a></li>
<li><a class="reference internal" href="#lecture-other-business-applications-game-design-financial-analysis-etc">26.06. <em>Lecture</em>: Other Business Applications: Game Design, Financial Analysis etc.</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#part-2-applications-in-science">Part 2: Applications in Science</a><ul>
<li><a class="reference internal" href="#week-11">Week 11</a><ul>
<li><a class="reference internal" href="#lecture-llms-in-research-experiment-planning-hypothesis-generation">01.07. <em>Lecture</em>: LLMs in Research: Experiment Planning &amp; Hypothesis Generation</a></li>
<li><a class="reference internal" href="#lab-experiment-planning-hypothesis-generation">03.07: <em>Lab</em>: Experiment Planning &amp; Hypothesis Generation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-12">Week 12</a><ul>
<li><a class="reference internal" href="#pitch-agent-for-code-generation">08.07: <em>Pitch</em>: Agent for Code Generation</a></li>
<li><a class="reference internal" href="#lecture-other-applications-in-science-drug-discovery-math-etc-scientific-reliability">10.07. <em>Lecture</em>: Other Applications in Science: Drug Discovery, Math etc. &amp; Scientific Reliability</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#block-3-wrap-up">Block 3: Wrap-up</a><ul>
<li><a class="reference internal" href="#week-13">Week 13</a><ul>
<li><a class="reference internal" href="#pitch-agent-for-web-development">15.07. <em>Pitch</em>: Agent for Web Development</a></li>
<li><a class="reference internal" href="#lecture-role-of-ai-in-recent-years">17.07. <em>Lecture</em>: Role of AI in Recent Years</a></li>
</ul>
</li>
<li><a class="reference internal" href="#week-14">Week 14</a><ul>
<li><a class="reference internal" href="#pitch-llm-based-research-assistant">22.07. <em>Pitch</em>: LLM-based Research Assistant</a></li>
<li><a class="reference internal" href="#debate-role-of-ai-in-recent-years-wrap-up">24.07. <em>Debate</em>: Role of AI in Recent Years + Wrap-up</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    </body>
</html>