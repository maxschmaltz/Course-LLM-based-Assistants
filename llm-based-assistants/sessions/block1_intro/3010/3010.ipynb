{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961d70de",
   "metadata": {},
   "source": [
    "# 30.10. Intro to LangChain ü¶úüîó\n",
    "\n",
    "üìç [Download notebook and session files](https://github.com/maxschmaltz/Course-LLM-based-Assistants/tree/main/llm-based-assistants/sessions/block1_intro/3010)\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/introduction/) is a powerful framework for **building** and **orchestration** of LLM-driven applications. It enables you to chain together language models, tools, and logic into flexible pipelines while maintaining the high level of abstraction. In other words, LangChain manages most of the engineering stuff for you so you can build LLM-based applications seamlessly.\n",
    "\n",
    "This tutorial covers the **basic concepts** you need to get started:\n",
    "\n",
    "* [Runnables](#runnables)\n",
    "* [LCEL (LangChain Expression Language)](#lcel)\n",
    "* [Messages](#messages)\n",
    "* [Chat Models](#chat_models)\n",
    "* [Structured Output](#structured_output)\n",
    "* [Tool Calling](#tool_calling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b740751",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To start with the tutorial, complete the steps [Prerequisites](../../../../infos/llm_inference_guide/README.md#prerequisites), [Environment Setup](../../../../infos/llm_inference_guide/README.md#environment-setup), and [Getting API Key](../../../../infos/llm_inference_guide/README.md#getting-api-key) from the [LLM Inference Guide](../../../../infos/llm_inference_guide/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266a22c",
   "metadata": {},
   "source": [
    "<h2 id=\"runnables\">1. Runnables üîÅ</h2>\n",
    "\n",
    "A `Runnable` is the foundational building block in LangChain. It is an abstraction for anything that can be _invoked_ ‚Äî meaning you can call it with an input and get an output. `Runnable`s share the same interface for the core functionality for you to be able to unify usage of components of different types under the same logic: **input in - output out**. This enables piping components for different purposes easily and intuitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d0dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f599b7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LANGCHAIN'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a simple function as a Runnable\n",
    "uppercase = RunnableLambda(lambda x: x.upper())\n",
    "\n",
    "uppercase.invoke(\"langchain\")  # output: LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08aa548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'niahcgnal'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define another simple function as a Runnable\n",
    "reverse = RunnableLambda(lambda x: x[::-1])\n",
    "\n",
    "reverse.invoke(\"langchain\")  # output: niahcgnal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e20ca",
   "metadata": {},
   "source": [
    "<h2 id=\"lcel\">2. LCEL (LangChain Expression Language) üîó</h2>\n",
    "\n",
    "_LCEL_ is a syntax for composing LangChain components (so `Runnables`s) using a `|` pipe operator ‚Äî similar to Unix pipes. Since LangChain components are (almost) all `Runnable`s, you can pipe them with LCEL and the output of the previous `Runnable` will become the input of the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf8ddd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NIAHCGNAL'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two Runnables into a single pipeline\n",
    "pipeline_c = uppercase | reverse\n",
    "\n",
    "pipeline_c.invoke(\"langchain\")  # output: NIAHCGNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93090b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(pipeline_c, Runnable)  # output: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a964b612",
   "metadata": {},
   "source": [
    "LCEL also support parallelization. If you pass a `dict` with `Runnable`s as values, LangChain will run them in parallel and return a `dict` with outputs under the corresponding keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393ab9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary: LANGCHAIN and niahcgnal'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {\n",
    "    \"upper\": uppercase,\n",
    "    \"rev\": reverse,\n",
    "}\n",
    "\n",
    "summarizer = RunnableLambda(lambda d: f\"Summary: {d['upper']} and {d['rev']}\")\n",
    "# this will 1) run `uppercase` and put the result in `upper` key\n",
    "# 2) run `reverse` and put the result in `rev` key\n",
    "# 3) pass this dict to summarizer for it to combine the results\n",
    "pipeline_p = mapping | summarizer\n",
    "\n",
    "pipeline_p.invoke(\"langchain\")  # output: Summary: LANGCHAIN and niahcgnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94dd41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(pipeline_p, Runnable)  # output: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e847e8b",
   "metadata": {},
   "source": [
    "<h2 id=\"messages\">3. Messages üó®Ô∏è</h2>\n",
    "\n",
    "Messages are needed to give LLMs instructions. Different types of messages improve the behavior of the model in multi-turn settings.\n",
    "\n",
    "There are 3 basic message types:\n",
    "* `SystemMessage`: sets LLM role and describes the desired behavior\n",
    "* `HumanMessage`: user input\n",
    "* `AIMessage`: model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6641cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c470195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a medieval French knight.\" # role\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Give me a summary of the Battle of Agincourt.\" # user request\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00279f81",
   "metadata": {},
   "source": [
    "Messages are no `Runnable`s! They are the data in the pipeline and not a part of it itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d2e4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(messages[0], Runnable)  # output: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d5f41",
   "metadata": {},
   "source": [
    "<h2 id=\"chat_models\">4. Chat Models üí¨</h2>\n",
    "\n",
    "A `ChatModel` is an LLM interface that lets you configure and call LLMs easily. It receives a list of messages and passes them to the underlying LLM for it to generate the output. In fact, it is common to use `ChatModel`s even for non-conversational settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ba3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "191089b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read system variables\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()    # that loads the .env file variables into os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e922aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any model, catalogue is available under https://build.nvidia.com/models\n",
    "MODEL_NAME = \"meta/llama-3.1-405b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04882c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this rate limiter will ensure we do not exceed the rate limit\n",
    "# of 40 RPM given by NVIDIA\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=35 / 60,  # 35 requests per minute to be sure\n",
    "    check_every_n_seconds=0.1,  # wake up every 100 ms to check whether allowed to make a request,\n",
    "    max_bucket_size=7,  # controls the maximum burst size\n",
    ")\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=MODEL_NAME,\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    temperature=0,   # ensure reproducibility,\n",
    "    rate_limiter=rate_limiter  # bind the rate limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da783cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(llm, Runnable)  # output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "172893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c57e71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)  # output: AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd359e5",
   "metadata": {},
   "source": [
    "In the standard case (no structured output or such), the generated text is stored under the `content` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4596f217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Bonjour! Ze Battle of Agincourt, eet ees a day that will be etched in my memory forever. 'Twas a day of great triumph for ze English, and a day of great shame for ze French.\\n\\nEet ees the year 1415, and our beloved France ees at war with ze English. Ze English king, Henry V, ees a cunning and ambitious man, and he ees determined to claim ze throne of France for himself.\\n\\nZe English army, eet ees a small force, no more than 6,000 men, but zay are well-trained and well-led. Ze French army, on ze other hand, ees a massive force, with over 20,000 men. We are confident of victory, no?\\n\\nBut ze English, zay have a secret weapon - ze longbow. Zay have brought with zem thousands of longbowmen, who can fire arrows at a rate of 12 per minute. Ze French knights, we are weighed down by our armor, and we are no match for ze English archers.\\n\\nZe battle begins, and ze English longbowmen unleash a hail of arrows upon us. Ze French knights, we charge forward, but we are cut down by ze arrows. Ze mud, eet ees thick and heavy, and our horses, zay are bogged down in ze mire.\\n\\nZe English, zay are well-positioned, and zay take advantage of ze terrain. Zay form a defensive line, with ze longbowmen at ze center, and ze men-at-arms on ze flanks. We, ze French, we are unable to break through zeir lines.\\n\\nZe battle rages on for hours, but in ze end, ze English emerge victorious. Ze French army, eet ees decimated, and many of our nobles, zay are killed or captured. I, myself, I am one of ze lucky ones, I escape with my life, but my honor, eet ees wounded.\\n\\nZe Battle of Agincourt, eet ees a great defeat for France, and a great victory for ze English. But I, I will not forget ze lessons of that day. Ze longbow, eet ees a powerful weapon, and ze English, zay are a force to be reckoned with. Vive la France!\", additional_kwargs={}, response_metadata={'role': 'assistant', 'content': \"Bonjour! Ze Battle of Agincourt, eet ees a day that will be etched in my memory forever. 'Twas a day of great triumph for ze English, and a day of great shame for ze French.\\n\\nEet ees the year 1415, and our beloved France ees at war with ze English. Ze English king, Henry V, ees a cunning and ambitious man, and he ees determined to claim ze throne of France for himself.\\n\\nZe English army, eet ees a small force, no more than 6,000 men, but zay are well-trained and well-led. Ze French army, on ze other hand, ees a massive force, with over 20,000 men. We are confident of victory, no?\\n\\nBut ze English, zay have a secret weapon - ze longbow. Zay have brought with zem thousands of longbowmen, who can fire arrows at a rate of 12 per minute. Ze French knights, we are weighed down by our armor, and we are no match for ze English archers.\\n\\nZe battle begins, and ze English longbowmen unleash a hail of arrows upon us. Ze French knights, we charge forward, but we are cut down by ze arrows. Ze mud, eet ees thick and heavy, and our horses, zay are bogged down in ze mire.\\n\\nZe English, zay are well-positioned, and zay take advantage of ze terrain. Zay form a defensive line, with ze longbowmen at ze center, and ze men-at-arms on ze flanks. We, ze French, we are unable to break through zeir lines.\\n\\nZe battle rages on for hours, but in ze end, ze English emerge victorious. Ze French army, eet ees decimated, and many of our nobles, zay are killed or captured. I, myself, I am one of ze lucky ones, I escape with my life, but my honor, eet ees wounded.\\n\\nZe Battle of Agincourt, eet ees a great defeat for France, and a great victory for ze English. But I, I will not forget ze lessons of that day. Ze longbow, eet ees a powerful weapon, and ze English, zay are a force to be reckoned with. Vive la France!\", 'token_usage': {'prompt_tokens': 35, 'total_tokens': 520, 'completion_tokens': 485}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-405b-instruct'}, id='run--fdc42134-1a35-4d1a-826c-776606ddec34-0', usage_metadata={'input_tokens': 35, 'output_tokens': 485, 'total_tokens': 520}, role='assistant')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bff35686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour! Ze Battle of Agincourt, eet ees a day that will be etched in my memory forever. 'Twas a day of great triumph for ze English, and a day of great shame for ze French.\n",
      "\n",
      "Eet ees the year 1415, and our beloved France ees at war with ze English. Ze English king, Henry V, ees a cunning and ambitious man, and he ees determined to claim ze throne of France for himself.\n",
      "\n",
      "Ze English army, eet ees a small force, no more than 6,000 men, but zay are well-trained and well-led. Ze French army, on ze other hand, ees a massive force, with over 20,000 men. We are confident of victory, no?\n",
      "\n",
      "But ze English, zay have a secret weapon - ze longbow. Zay have brought with zem thousands of longbowmen, who can fire arrows at a rate of 12 per minute. Ze French knights, we are weighed down by our armor, and we are no match for ze English archers.\n",
      "\n",
      "Ze battle begins, and ze English longbowmen unleash a hail of arrows upon us. Ze French knights, we charge forward, but we are cut down by ze arrows. Ze mud, eet ees thick and heavy, and our horses, zay are bogged down in ze mire.\n",
      "\n",
      "Ze English, zay are well-positioned, and zay take advantage of ze terrain. Zay form a defensive line, with ze longbowmen at ze center, and ze men-at-arms on ze flanks. We, ze French, we are unable to break through zeir lines.\n",
      "\n",
      "Ze battle rages on for hours, but in ze end, ze English emerge victorious. Ze French army, eet ees decimated, and many of our nobles, zay are killed or captured. I, myself, I am one of ze lucky ones, I escape with my life, but my honor, eet ees wounded.\n",
      "\n",
      "Ze Battle of Agincourt, eet ees a great defeat for France, and a great victory for ze English. But I, I will not forget ze lessons of that day. Ze longbow, eet ees a powerful weapon, and ze English, zay are a force to be reckoned with. Vive la France!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d3078",
   "metadata": {},
   "source": [
    "<h2 id=\"structured_output\">5. Structured Output üîå</h2>\n",
    "\n",
    "LLMs usually return text, but LangChain allows parsing that text into **structured data** like JSON. That enables **machine-readable** responses and compatibility of the components when connecting the LLMs to external stuff or have it do actions.\n",
    "\n",
    "JSON is the most widely-used structured output time, and `Pydantic` provides a Python interface to define schemas (using Python classes) that the model‚Äôs responses must conform to. That is an easy and intuitive way to provide the LLM with the instructions about how the output should be structured. `Pydantic` also takes care of parsing and validating the LLM output and is therefore a mediator between the LLM and the output JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d92e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d72d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Battle(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the battle\")\n",
    "    year: int = Field(..., description=\"Year of the battle\")\n",
    "    location: str = Field(..., description=\"Location of the battle\")\n",
    "    description: List[str] = Field(..., description=\"A list of verses to describe the battle, one string per verse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2ae7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Battle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e43e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a medieval French knight.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Give me a few verses (5 or more) about the Battle of Agincourt as well as information about its year and location.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = structured_llm.invoke(new_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97967e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Battle(name='Battle of Agincourt', year=1415, location='Agincourt, France', description=['In fourteen hundred and fifteen, a year of great renown', 'King Henry of England, with courage in his crown', 'Did lead his army forth, to claim the French throne', 'And at Agincourt, the battle was to be known', 'The English longbowmen, with arrows swift and true', 'Did cut down the French knights, like wheat in the dew', 'Their armor, no match for the hail of arrows bright', 'The French nobles fell, in the muddy field of fight', \"Their cries of 'Havoc!' and 'St. Denis!' did fill the air\", 'As the English lines, held firm, without a care', 'The battle raged on, through the muddy field of Agincourt', 'Where honor and glory, were won, and many a life was lost', 'The English emerged, victorious, their banners held high', 'And the French, defeated, did mourn, and wonder why', 'Their chivalry, and honor, had not been enough', \"To withstand, the English, and their longbow's deadly stuff\"])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be873fd",
   "metadata": {},
   "source": [
    "Note that now the response is now a `Pydantic` model and it will be structured exactly as the provided schema, so instead of `content`, you would need to refer to the actual keys you have provided in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb26ff4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(response, BaseModel)  # output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba7a4d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fourteen hundred and fifteen, a year of great renown\n",
      "King Henry of England, with courage in his crown\n",
      "Did lead his army forth, to claim the French throne\n",
      "And at Agincourt, the battle was to be known\n",
      "The English longbowmen, with arrows swift and true\n",
      "Did cut down the French knights, like wheat in the dew\n",
      "Their armor, no match for the hail of arrows bright\n",
      "The French nobles fell, in the muddy field of fight\n",
      "Their cries of 'Havoc!' and 'St. Denis!' did fill the air\n",
      "As the English lines, held firm, without a care\n",
      "The battle raged on, through the muddy field of Agincourt\n",
      "Where honor and glory, were won, and many a life was lost\n",
      "The English emerged, victorious, their banners held high\n",
      "And the French, defeated, did mourn, and wonder why\n",
      "Their chivalry, and honor, had not been enough\n",
      "To withstand, the English, and their longbow's deadly stuff\n"
     ]
    }
   ],
   "source": [
    "for verse in response.description:  # output: List of verses describing the battle\n",
    "    print(verse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86168f47",
   "metadata": {},
   "source": [
    "To convert the model into a `dict`, use `model_dump` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b30d86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Battle of Agincourt',\n",
       " 'year': 1415,\n",
       " 'location': 'Agincourt, France',\n",
       " 'description': ['In fourteen hundred and fifteen, a year of great renown',\n",
       "  'King Henry of England, with courage in his crown',\n",
       "  'Did lead his army forth, to claim the French throne',\n",
       "  'And at Agincourt, the battle was to be known',\n",
       "  'The English longbowmen, with arrows swift and true',\n",
       "  'Did cut down the French knights, like wheat in the dew',\n",
       "  'Their armor, no match for the hail of arrows bright',\n",
       "  'The French nobles fell, in the muddy field of fight',\n",
       "  \"Their cries of 'Havoc!' and 'St. Denis!' did fill the air\",\n",
       "  'As the English lines, held firm, without a care',\n",
       "  'The battle raged on, through the muddy field of Agincourt',\n",
       "  'Where honor and glory, were won, and many a life was lost',\n",
       "  'The English emerged, victorious, their banners held high',\n",
       "  'And the French, defeated, did mourn, and wonder why',\n",
       "  'Their chivalry, and honor, had not been enough',\n",
       "  \"To withstand, the English, and their longbow's deadly stuff\"]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c6709",
   "metadata": {},
   "source": [
    "<h2 id=\"tool_calling\">6. Tool Calling üõ†Ô∏è</h2>\n",
    "\n",
    "Tools are Python functions (hence former name: function calling) that can be \"called\" by the model to expand its abilities. It makes sense to call tool to do stuff LLMs is incapable of: real-time search, doing actions via external APIs (reading emails, scheduling appointments etc.).\n",
    "\n",
    "An **LLM cannot actually call the function**. What it does is it returns the name of the function it thinks it is now necessary to call and and the arguments provided by the scheme of the function. These arguments can then be parsed for the tool to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6af18",
   "metadata": {},
   "source": [
    "The easiest way to convert a function into a tool is to use the `@tool` decorator. It will automatically create a tool scheme based on the docstring and the input and output types of the provided function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e208761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ad2e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_temperature(location: str, is_celcius: bool) -> int:\n",
    "    \"\"\"Get current weather.\"\"\"\n",
    "    # dummy function\n",
    "    temp = len(location) * 2\n",
    "    if not is_celcius:\n",
    "        temp = temp * 9 / 5 + 32\n",
    "    return temp\n",
    "\n",
    "# will be used to actually execute tools\n",
    "tools_index = {\n",
    "    \"get_temperature\": get_temperature,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef5f6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tool = llm.bind_tools([get_temperature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e003807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"What is the temperature in Paris?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "response = llm_with_tool.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f52df5e",
   "metadata": {},
   "source": [
    "If the model decides to call tools, the respective outputs will be stored in the `tool_calls` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f344a1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_temperature',\n",
       "  'args': {'location': 'Paris', 'is_celcius': True},\n",
       "  'id': 'chatcmpl-tool-8ea62c7bd2e045fcb92b4bf29ded117b',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d561f1",
   "metadata": {},
   "source": [
    "To proceed with the generation, we should configure our pipeline to call the tools based on the generated name and arguments and then give it back to the LLM. Tools are also `Runnable`s so they can be executed directly with the `invoke` method. It will return a new type of messages: a `ToolMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1319514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='10', name='get_temperature', tool_call_id='chatcmpl-tool-8ea62c7bd2e045fcb92b4bf29ded117b')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_outputs = []\n",
    "for tool_call in response.tool_calls:\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_output = tools_index[tool_name].invoke(\n",
    "        tool_call\n",
    "    )\n",
    "    tool_outputs.append(tool_output)\n",
    "\n",
    "tool_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a9ee9",
   "metadata": {},
   "source": [
    "Now this `ToolMessage` should be added to the rest of the messages and passed back to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8e27003",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages + tool_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "433db155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in Paris is 10 degrees Celsius.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307fd17c",
   "metadata": {},
   "source": [
    "## Summary üß©\n",
    "\n",
    "| Concept          | Description                                         | Used For                          |\n",
    "|------------------|-----------------------------------------------------|-----------------------------------|\n",
    "| **Runnables**     | Core executable units                              | Universality, piping logic       |\n",
    "| **LCEL**          | Pipe syntax for chaining components                | Easy, clean composition           |\n",
    "| **Messages**      | Human / System / AI messages for giving the context               | Providing instructions to the LLM          |\n",
    "| **Chat Models**   | LLMs designed for taking message input and generating a certain output  | Conversations, reasoning, tools   |\n",
    "| **Structured Output** | Parsing LLM text into JSON / Pydantic types       | Data extraction, validation       |\n",
    "| **Tool Calling**  | Calling external Python functions from withing the LLM-based pipeline   | Extend LLMs with external logic   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
