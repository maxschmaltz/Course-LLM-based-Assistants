{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c99fa75",
   "metadata": {},
   "source": [
    "# 12.11 & 13.11. RAG Chatbot üìö\n",
    "\n",
    "üìç [Download notebook](https://github.com/maxschmaltz/Course-LLM-based-Assistants/tree/main/llm-based-assistants/sessions/block2_core/1211_1311/1211_1311.ipynb)\n",
    "\n",
    "In today'l lab, we will be expanding the chatbot we created in our [previous session](../0611.ipynb). We'll implement a RAG functionality so that the chatbot has access to custom knowledge. In the first part (12.11), we'll preprocess our data for further retrieval. Next day (13.11), we will complete the RAG chatbot and use the data we have preprocessed in the first part to inject our custom knowledge to the LLM. \n",
    "\n",
    "Our plan for 12.11 & 13.11:\n",
    "\n",
    "* [Data Preprocessing](#data)\n",
    "* [Prompting al LangChain](#prompts)\n",
    "* [Simple RAG](#rag)\n",
    "* [Advanced RAG](#adv_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6d768",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To start with the tutorial, complete the steps [Prerequisites](../../../../infos/llm_inference_guide/README.md#prerequisites), [Environment Setup](../../../../infos/llm_inference_guide/README.md#environment-setup), and [Getting API Key](../../../../infos/llm_inference_guide/README.md#getting-api-key) from the [LLM Inference Guide](../../../../infos/llm_inference_guide/README.md).\n",
    "\n",
    "Today, we have more packages so we'll use the requirements file to install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c2df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   118  100   118    0     0   1244      0 --:--:-- --:--:-- --:--:--  1255\n",
      "Collecting langchain_community (from -r requirements.txt (line 1))\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain_nvidia_ai_endpoints (from -r requirements.txt (line 2))\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.19-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting langchain-huggingface (from -r requirements.txt (line 3))\n",
      "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-qdrant (from -r requirements.txt (line 4))\n",
      "  Using cached langchain_qdrant-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langgraph (from -r requirements.txt (line 5))\n",
      "  Using cached langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pypdf (from -r requirements.txt (line 6))\n",
      "  Using cached pypdf-6.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 7))\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached sqlalchemy-2.0.44-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting PyYAML<7.0.0,>=5.3.0 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langsmith<1.0.0,>=0.1.125 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langsmith-0.4.42-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting numpy>=2.1.0 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain_nvidia_ai_endpoints->-r requirements.txt (line 2))\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-nvidia-ai-endpoints to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_nvidia_ai_endpoints (from -r requirements.txt (line 2))\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.18-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.17-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.16-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.14-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.12-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-nvidia-ai-endpoints to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.11-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.9-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.8-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.7-py3-none-any.whl.metadata (11 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.5-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.4-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.3-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.2-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.3.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.2.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.2.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting langchain_community (from -r requirements.txt (line 1))\n",
      "  Using cached langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain_nvidia_ai_endpoints (from -r requirements.txt (line 2))\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.7-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.6-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.5-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.4-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.20-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.19-py3-none-any.whl.metadata (9.7 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.18-py3-none-any.whl.metadata (9.7 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.16-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.15-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.14-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.13-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.12-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.11-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.10-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.9-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.7-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.6-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.5-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.4-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.3-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "  Using cached langchain_nvidia_ai_endpoints-0.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting langchain_community (from -r requirements.txt (line 1))\n",
      "  Using cached langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Using cached langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain<2.0.0,>=0.3.27 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core<2.0.0,>=0.3.78 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langchain_core-0.3.79-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./.venv/lib/python3.13/site-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1)) (25.0)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-huggingface (from -r requirements.txt (line 3))\n",
      "  Using cached langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Using cached langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting huggingface-hub>=0.33.4 (from langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-qdrant to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-qdrant (from -r requirements.txt (line 4))\n",
      "  Using cached langchain_qdrant-1.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "  Using cached langchain_qdrant-0.2.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting qdrant-client<2.0.0,>=1.10.1 (from langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph->-r requirements.txt (line 5))\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph->-r requirements.txt (line 5))\n",
      "  Using cached langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph->-r requirements.txt (line 5))\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph->-r requirements.txt (line 5))\n",
      "  Using cached xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain<2.0.0,>=0.3.27 (from langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langchain-1.0.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain-1.0.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain-1.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain-1.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain<2.0.0,>=0.3.27->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph->-r requirements.txt (line 5))\n",
      "  Using cached ormsgpack-1.12.0-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.2 kB)\n",
      "INFO: pip is looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph->-r requirements.txt (line 5))\n",
      "  Using cached langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langgraph (from -r requirements.txt (line 5))\n",
      "  Using cached langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is still looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph->-r requirements.txt (line 5))\n",
      "  Using cached langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph->-r requirements.txt (line 5))\n",
      "  Using cached orjson-3.11.4-cp313-cp313-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.1.125->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting protobuf>=3.20.0 (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting urllib3<3,>=1.26.14 (from qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.32.5->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.32.5->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.32.5->langchain_community->-r requirements.txt (line 1))\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community->-r requirements.txt (line 1))\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.10.1->langchain-qdrant->-r requirements.txt (line 4))\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.33.4->langchain-huggingface->-r requirements.txt (line 3))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain_nvidia_ai_endpoints-0.3.19-py3-none-any.whl (46 kB)\n",
      "Using cached langchain_core-0.3.79-py3-none-any.whl (449 kB)\n",
      "Using cached langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Using cached langchain_qdrant-0.2.1-py3-none-any.whl (24 kB)\n",
      "Using cached langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached aiohttp-3.13.2-cp313-cp313-macosx_11_0_arm64.whl (489 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Using cached huggingface_hub-1.1.2-py3-none-any.whl (514 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Using cached langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
      "Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Using cached langsmith-0.4.42-py3-none-any.whl (401 kB)\n",
      "Using cached numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached sqlalchemy-2.0.44-cp313-cp313-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Using cached xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached grpcio-1.76.0-cp313-cp313-macosx_11_0_universal2.whl (11.8 MB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached orjson-3.11.4-cp313-cp313-macosx_15_0_arm64.whl (128 kB)\n",
      "Using cached ormsgpack-1.12.0-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (369 kB)\n",
      "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
      "Using cached zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl (640 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: filetype, zstandard, xxhash, urllib3, typing-extensions, tqdm, tenacity, sniffio, shellingham, PyYAML, python-dotenv, pypdf, protobuf, propcache, portalocker, ormsgpack, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, hyperframe, httpx-sse, hpack, hf-xet, h11, fsspec, frozenlist, filelock, click, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, typer-slim, SQLAlchemy, requests, pydantic-core, jsonpatch, httpcore, h2, grpcio, anyio, aiosignal, requests-toolbelt, pydantic, httpx, dataclasses-json, aiohttp, pydantic-settings, langsmith, langgraph-sdk, huggingface-hub, tokenizers, qdrant-client, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-qdrant, langchain_nvidia_ai_endpoints, langchain-huggingface, langgraph-prebuilt, langchain, langgraph, langchain_community\n",
      "Successfully installed PyYAML-6.0.3 SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 click-8.3.0 dataclasses-json-0.6.7 filelock-3.20.0 filetype-1.2.0 frozenlist-1.8.0 fsspec-2025.10.0 grpcio-1.76.0 h11-0.16.0 h2-4.3.0 hf-xet-1.2.0 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 httpx-sse-0.4.3 huggingface-hub-1.1.2 hyperframe-6.1.0 idna-3.11 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.79 langchain-huggingface-0.3.1 langchain-qdrant-0.2.1 langchain-text-splitters-0.3.11 langchain_community-0.3.31 langchain_nvidia_ai_endpoints-0.3.19 langgraph-1.0.1 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 langsmith-0.4.42 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 numpy-2.3.4 orjson-3.11.4 ormsgpack-1.12.0 portalocker-3.2.0 propcache-0.4.1 protobuf-6.33.0 pydantic-2.12.4 pydantic-core-2.41.5 pydantic-settings-2.12.0 pypdf-6.2.0 python-dotenv-1.2.1 qdrant-client-1.15.1 requests-2.32.5 requests-toolbelt-1.0.0 shellingham-1.5.4 sniffio-1.3.1 tenacity-9.1.2 tokenizers-0.22.1 tqdm-4.67.1 typer-slim-0.20.0 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 urllib3-2.5.0 xxhash-3.6.0 yarl-1.22.0 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!curl -o requirements.txt https://raw.githubusercontent.com/maxschmaltz/Course-LLM-based-Assistants/main/llm-based-assistants/sessions/block2_core/1211_1311/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d89f95",
   "metadata": {},
   "source": [
    "Finally, download the data we'll be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d388b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  423k  100  423k    0     0  1914k      0 --:--:-- --:--:-- --:--:-- 1917k\n"
     ]
    }
   ],
   "source": [
    "!curl -o topic_overview.pdf https://raw.githubusercontent.com/maxschmaltz/Course-LLM-based-Assistants/main/llm-based-assistants/sessions/block2_core/1211_1311/topic_overview.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36839b67",
   "metadata": {},
   "source": [
    "In the [last session](../0611.ipynb), we created a basic chatbot implemented with LangGraph. The chatbot was built as a graph-like system with the following components:\n",
    "1. The input receival node. It prompted the user for the input and stored it in the messages for further interaction with the LLM.\n",
    "2. The router node. It performed the check whether the user wants to exit.\n",
    "3. The chatbot node. It received the input if the user had not quit, passed it to the LLM, and returned the generation.\n",
    "\n",
    "Before we begin, let's pull this chatbot, as it will be used as the base class for the further RAG chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d542f87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "# read system variables\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()    # that loads the .env file variables into os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4426472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any model, catalogue is available under https://build.nvidia.com/models\n",
    "MODEL_NAME = \"meta/llama-3.3-70b-instruct\"\n",
    "\n",
    "# this rate limiter will ensure we do not exceed the rate limit\n",
    "# of 40 RPM given by NVIDIA\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=35 / 60,  # 35 requests per minute to be sure\n",
    "    check_every_n_seconds=0.1,  # wake up every 100 ms to check whether allowed to make a request,\n",
    "    max_bucket_size=7,  # controls the maximum burst size\n",
    ")\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=MODEL_NAME,\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    temperature=0,   # ensure reproducibility,\n",
    "    rate_limiter=rate_limiter  # bind the rate limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83f8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7a39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleState(TypedDict):\n",
    "    # `messages` is a list of messages of any kind. The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    # Since we didn't define a function to update it, it will be rewritten at each transition\n",
    "    # with the value you provide\n",
    "    n_turns: int    # just for demonstration\n",
    "    language: str   # new, for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694b0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "\n",
    "    _graph_path = \"./graph.png\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self._build()\n",
    "        self._display_graph()\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(SimpleState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"respond\", True: END})\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "\n",
    "    def _compile(self):\n",
    "        self.chatbot = self._graph_builder.compile()\n",
    "\n",
    "    def _input_node(self, state: SimpleState) -> dict:\n",
    "        user_query = input(\"Your message: \")\n",
    "        human_message = HumanMessage(content=user_query)\n",
    "        # add the input to the messages\n",
    "        return {\n",
    "            \"messages\": human_message   # this will append the input to the messages\n",
    "        }\n",
    "    \n",
    "    def _respond_node(self, state: SimpleState) -> dict:\n",
    "        messages = state[\"messages\"]    # will already contain the user query\n",
    "        n_turns = state[\"n_turns\"]\n",
    "        response = self.llm.invoke(messages)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response,   # this will append the response to the messages\n",
    "            \"n_turns\": n_turns + 1  # and this will rewrite the number of turns\n",
    "        }\n",
    "    \n",
    "    def _is_quitting_node(self, state: SimpleState) -> dict:\n",
    "        # check if the user wants to quit\n",
    "        user_message = state[\"messages\"][-1].content\n",
    "        return user_message.lower() == \"quit\"\n",
    "    \n",
    "    def _display_graph(self):\n",
    "        display(\n",
    "            Image(\n",
    "                self.chatbot.get_graph().draw_mermaid_png(\n",
    "                    output_file_path=self._graph_path\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # add the run method\n",
    "    def run(self, language=None):\n",
    "        input = {\n",
    "            \"messages\": [\n",
    "                SystemMessage(\n",
    "                    content=\"You are a helpful and honest assistant.\" # role\n",
    "                )\n",
    "            ],\n",
    "            \"n_turns\": 0,\n",
    "            \"language\": language or \"English\"    # new\n",
    "        }\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"values\"):   #stream_mode=\"updates\"):\n",
    "            for key, value in event.items():\n",
    "                print(f\"{key}:\\t{value}\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c7fbc",
   "metadata": {},
   "source": [
    "<h2 id=\"data\">1. Data Preprocessing üìï</h2>\n",
    "\n",
    "As you remember from the lecture, the first step to RAG is data preprocessing. That includes:\n",
    "1. Loading: load the source (document, website etc.) as a text.\n",
    "2. Chunking: chunk the loaded text onto smaller pieces.\n",
    "3. Converting to embeddings: embed the chunks into dense vector for further similarity search.\n",
    "4. Indexing: put the embeddings into a so-called index -- a special database for efficient storage and search of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea291c",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "We will take a PDF version of the Topic Overview for the last iteration of this course (the current version does not convert nicely). No LLM can know the contents of it, especially some highly specific facts such as dates or key points.\n",
    "\n",
    "One of ways to load a PDF is to use [`PyPDFLoader`](https://docs.langchain.com/oss/python/integrations/document_loaders/pypdfloader) that load simple textual PDFs and their metadata. In this tutorial, we focus on a simpler variant when there are no multimodal data in the PDF. You can find out more about advanced loading on the PyPDFLoader [LangChain page](https://docs.langchain.com/oss/python/integrations/document_loaders/pypdfloader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03d0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a566ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./topic_overview.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b4c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1000b5",
   "metadata": {},
   "source": [
    "This function returns a list of `Document` objects, each containing the text of the PDF and its metadata such as title, page, creation date etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332a8971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nTo p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!\\nThe reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis \\n: those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji \\n: that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboardsetc. (also a couple of LLM-based games). For each of the sources, I also leave my subjectiveestimation of how important this work is for this specific topic: from yellow \\n ‚Äòpartially useful‚Äô thoughorange \\n ‚Äòuseful‚Äô to red \\n ‚Äòcrucial findings / thoughts‚Äô.  T h e s e  e s t i m a t i o n s  w i l l  b e  c o n t i n u o u s l yupdated as I revise the materials.\\nFor the labs, you are provided with practical tutorials that respective lab tasks will mostly derive from.The core tutorials are marked with a writing emoji \\n; you are asked to inspect them in advance(better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to preparein advance to keep up with the lab.\\nDisclaimer: the reading entries are no proper citations; the bibtex references as well as detailed infosabout the authors, publish date etc. can be found under the entry links.\\nBlock 1: IntroWeek 122.04. Lecture: LLMs as a Form of Intelligence vs LLMs as Statistical MachinesThat is an introductory lecture, in which I will briefly introduce the course and we‚Äôll have a warming updiscussion about different perspectives on LLMs‚Äô nature. We will focus on two prominent outlooks: LLMis a form of intelligence and LLM is a complex statistical machine. We‚Äôll discuss differences of LLMswith human intelligence and the degree to which LLMs exhibit (self-)awareness.\\nKey points:\\nCourse introduction\\nDifferent perspectives on the nature of LLMs\\nSimilarities and differences between human and artificial intelligence\\nLLMs‚Äô (self-)awareness\\nCore Reading:\\n The Debate Over Understanding in AI‚Äôs Large Language Models (pages 1-7), Santa Fe\\nInstitute \\nMeaning without reference in large language models, UC Berkeley & DeepMind \\nDissociating language and thought in large language models (intro [right after the abstract, seemore on the sectioning in this paper at the bottom of page 2], sections 1, 2.3 [LLMs are predictive‚Ä¶], 3-5), The University of Texas at Austin et al. \\nAdditional Reading:\\nLLM-basedAssistants\\nINFOS AND STUFF\\nBLOCK 1: INTRO\\nBLOCK 2: CORE TOPICS | PART 1:BUSINESS APPLICATIONS\\nBLOCK 2: CORE TOPICS | PART 2:APPLICATIONS IN SCIENCE\\nBLOCK 3: WRAP-UP\\nTopics Overview\\nDebates\\nPitches\\nLLM Inference Guide\\n22.04. LLMs as a Form ofIntelligence vs LLMs asStatistical Machines\\n24.04. LLM & Agent Basics\\n29.04. Intro to LangChain \\n!\\n\"\\n06.05. Virtual Assistants Pt. 1:Chatbots\\n08.05. Basic LLM-basedChatbot \\n#\\nUnder development\\nUnder development\\nSearch'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 2 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nDo Large Language Models Understand Us?, Google Research \\nSparks of Artificial General Intelligence: Early experiments with GPT-4 (chapters 1-8 & 10),Microsoft Research \\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \\n  (paragraphs 1, 5, 6.1),University of Washington et al. \\nLarge Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective onUnderstanding, Leiden Institute of Advanced Computer Science & Leiden University Medical\\nCentre \\n24.04. Lecture: LLM & Agent BasicsIn this lecture, we‚Äôll recap some basics about LLMs and LLM-based agents to make sure we‚Äôre on thesame page.\\nKey points:\\nLLM recap\\nPrompting\\nStructured output\\nTool calling\\nPiping & Planning\\nCore Reading:\\nA Survey of Large Language Models, (sections 1, 2.1, 4.1, 4.2.1, 4.2.3-4.2.4, 4.3, 5.1.1-5.1.3, 5.2.1-5.2.4, 5.3.1, 6) Renmin University of China et al. \\nEmergent Abilities of Large Language Models, Google Research, Stanford, UNC Chapel Hill,\\nDeepMind\\n‚ÄúWe Need Structured Output‚Äù: Towards User-centered Constraints on Large Language ModelOutput, Google Research & Google\\n Agent Instructs Large Language Models to be General Zero-Shot Reasoners (pages 1-9),Washington University & UC Berkeley\\nAdditional Reading:\\nLanguage Models are Few-Shot Learners, OpenAI\\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models, Google Research\\nThe Llama 3 Herd of Models, Meta AI\\nIntroducing Structured Outputs in the API, OpenAI\\nTool Learning with Large Language Models: A Survey, Renmin University of China et al.\\nToolACE: Winning the Points of LLM Function Calling, Huawei Noah‚Äôs Ark Lab et al.\\nToolformer: Language Models Can Teach Themselves to Use Tools, Meta AI\\nGranite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning ofGranular Tasks, IBM Research\\n Berkeley Function-Calling Leaderboard, UC Berkeley (leaderboard)\\nA Survey on Multimodal Large Language Models, University of Science and Technology of China\\n& Tencent YouTu Lab\\nWeek 229.04. Lab: Intro to LangChainThe final introductory session will guide you through the most basic concepts of LangChain for thefurther practical sessions.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 3 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nReading:\\nRunnable interface, LangChain\\nLangChain Expression Language (LCEL), LangChain\\nMessages, LangChain\\nChat models, LangChain\\nStructured outputs, LangChain\\nTools, LangChain\\nTool calling, LangChain\\n01.05.Ausfalltermin\\nBlock 2: Core T opics\\nPart 1: Business ApplicationsWeek 306.05. Lecture: Virtual Assistants Pt. 1: ChatbotsThe first core topic concerns chatbots. We‚Äôll discuss how chatbots are built, how they (should) handleharmful requests and you can tune it for your use case.\\nKey points:\\nLLMs alignment\\nMemory\\nPrompting & automated prompt generation\\nEvaluation\\nCore Reading:\\n Aligning Large Language Models with Human: A Survey (pages 1-14), Huawei Noah‚Äôs Ark Lab\\nSelf-Instruct: Aligning Language Models with Self-Generated Instructions, University of\\nWashington et al.\\nA Systematic Survey of Prompt Engineering in Large Language Models: Techniques andApplications, Indian Institute of Technology Patna, Stanford & Amazon AI\\nAdditional Reading:\\nTraining language models to follow instructions with human feedback, OpenAI\\nTraining a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,Anthropic\\nA Survey on the Memory Mechanism of Large Language Model based Agents, Renmin University\\nof China & Huawei Noah‚Äôs Ark Lab\\nAugmenting Language Models with Long-Term Memory, UC Santa Barbara & Microsoft Research\\nFrom LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of LargeLanguage Models, Beike Inc.\\nAutomatic Prompt Selection for Large Language Models, Cinnamon AI, Hung Yen University of\\nTechnology and Education & Deakin University\\nPromptGen: Automatically Generate Prompts using Generative Models, Baidu Research\\nEvaluating Large Language Models. A Comprehensive Survey, Tianjin University'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 4 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\n08.05. Lab: Basic LLM-based Chatbot\\nOn material of session 06.05\\nIn this lab, we‚Äôll build a chatbot and try different prompts and settings to see how it affects the output.\\nReading:\\n Build a Chatbot, LangChain\\n LangGraph Quickstart: Build a Basic Chatbot (parts 1, 3), LangGraph\\n How to add summary of the conversation history, LangGraph\\nPrompt Templates, LangChain\\nFew-shot prompting, LangChain\\nWeek 413.05. Lecture: Virtual Assistants Pt. 2: RAGContinuing the first part, the second part will expand scope of chatbot functionality and will teach it torefer to custom knowledge base to retrieve and use user-specific information. Finally, the most widelyused deployment methods will be briefly introduced.\\nKey points:\\nGeneral knowledge vs context\\nKnowledge indexing, retrieval & ranking\\nRetrieval tools\\nAgentic RAG\\nCore Reading:\\n Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and HybridApproach (pages 1-7), Google DeepMind & University of Michigan \\nA Survey on Retrieval-Augmented Text Generation for Large Language Models (sections 1-7), York\\nUniversity \\nAdditional Reading:\\nDon‚Äôt Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National\\nChengchi University & Academia Sinica \\nSelf-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, University of\\nWashington, Allen Institute for AI & IBM Research AI\\nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through QuestionComplexity, Korea Advanced Institute of Science and Technology\\nAuto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese\\nAcademy of Sciences\\nQuerying Databases with Function Calling, Weaviate, Contextual AI & Morningstar\\n15.05. Lab: RAG Chatbot\\nOn material of session 13.05\\nIn this lab, we‚Äôll expand the functionality of the chatbot built at the last lab to connect it to user-specificinformation.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 5 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nReading:\\nHow to load PDFs, LangChain\\nText splitters, LangChain\\nEmbedding models, LangChain\\nVector stores, LangChain\\nRetrievers, LangChain\\n Retrieval augmented generation (RAG), LangChain\\n LangGraph Quickstart: Build a Basic Chatbot (part 2), LangGraph\\n Agentic RAG, LangGraph\\nAdaptive RAG, LangGraph\\nMultimodality, LangChain\\nWeek 520.05. Lecture: Virtual Assistants Pt. 3: Multi-agent EnvironmentThis lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday /business operations in a multi-agent environment. We‚Äôll look at how agents communicate with eachother, how their communication can be guided (both with and without involvement of a human), andthis all is used in real applications.\\nKey points:\\nMulti-agent environment\\nHuman in the loop\\nLLMs as evaluators\\nExamples of pipelines for business operations\\nCore Reading:\\n LLM-based Multi-Agent Systems: Techniques and Business Perspectives (pages 1-8), Shanghai\\nJiao Tong University & OPPO Research Institute\\nGenerative Agents: Interactive Simulacra of Human Behavior, Stanford, Google Research &\\nDeepMind\\nAdditional Reading:\\nImproving Factuality and Reasoning in Language Models through Multiagent Debate, MIT & Google\\nBrain\\nExploring Collaboration Mechanisms for LLM Agents: A Social Psychology View, Zhejiang\\nUniversity, National University of Singapore & DeepMind\\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, Microsoft Research\\net al.\\n How real-world businesses are transforming with AI ‚Äî with more than 140 new stories,Microsoft (blog post)\\n Built with LangGraph, LangGraph (website page)\\nPlan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLMAgents As A Daily Assistant, Delft University of Technology & The University of Queensland\\n22.05. Lab: Multi-agent Environment\\nOn material of session 20.05'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 6 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nThis lab will introduce a short walkthrough to creation of a multi-agent environment for automatedmeeting scheduling and preparation. We will see how the coordinator agent will communicate with twoauxiliary agents to check time availability and prepare an agenda for the meeting.\\nReading:\\n Multi-agent network, LangGraph\\n Human-in-the-loop, LangGraph\\nPlan-and-Execute, LangGraph\\nReflection, LangGraph\\n Multi-agent supervisor, LangGraph\\nQuick Start, AutoGen\\nWeek 627.05. Lecture: Software Development Pt. 1: Code Generation, Evaluation &TestingThis lectures opens a new lecture mini-cycle dedicated to software development. The first lectureoverviews how LLMs are used to generate reliable code and how generated code is tested andimproved to deal with the errors.\\nKey points:\\nCode generation & refining\\nAutomated testing\\nGenerated code evaluation\\nCore Reading:\\nLarge Language Model-Based Agents for Software Engineering: A Survey, Fudan University,\\nNanyang Technological University & University of Illinois at Urbana-Champaign\\n CodeRL: Mastering Code Generation through Pretrained Models and Deep ReinforcementLearning (pages 1-20), Salesforce Research\\nThe ART of LLM Refinement: Ask, Refine, and Trust, ETH Zurich & Meta AI\\nAdditional Reading:\\nPlanning with Large Language Models for Code Generation, MIT-IBM Watson AI Lab et al.\\nCode Repair with LLMs gives an Exploration-Exploitation Tradeoff, Cornell, Shanghai Jiao Tong\\nUniversity & University of Toronto\\nChatUniTest: A Framework for LLM-Based Test Generation, Zhejiang University & Hangzhou City\\nUniversity\\nTestART: Improving LLM-based Unit Testing via Co-evolution of Automated Generation and RepairIteration, Nanjing University & Huawei Cloud Computing Technologies\\nEvaluating Large Language Models Trained on Code, `OpenAI\\n Code Generation on HumanEval, OpenAI (leaderboard)\\nCodeJudge: Evaluating Code Generation with Large Language Models, Huazhong University of\\nScience and Technology & Purdue University\\n29.05.Ausfalltermin'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 7 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nWeek 703.06. Lecture: Software Development Pt. 2: Copilots, LLM-powered WebsitesThe second and the last lecture of the software development cycle focuses on practical application ofLLM code generation, in particular, on widely-used copilots (real-time code generation assistants) andLLM-supported web development.\\nKey points:\\nCopilots & real-time hints\\nLLM-powered websites\\nLLM-supported deployment\\nFurther considerations: reliability, sustainability etc.\\nCore Reading:\\n LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities andLimitations (pages 1-11), University of Oslo\\nA Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis,Google DeepMind & The University of Tokyo\\nCan ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large LanguageModel Code Generation, UC San Diego\\nAdditional Reading:\\nDesign and evaluation of AI copilots ‚Äì case studies of retail copilot templates, Microsoft\\n Your AI Companion, Microsoft (blog post)\\nGitHub Copilot, GitHub (product page)\\n Research: quantifying GitHub Copilot‚Äôs impact on developer productivity and happiness, GitHub\\n(blog post)\\n Cursor: The AI Code Editor, Cursor (product page)\\nAutomated Unit Test Improvement using Large Language Models at Meta, Meta\\nHuman-In-the-Loop Software Development Agents, Monash University, The University of\\nMelbourne & Atlassian\\nAn LLM-based Agent for Reliable Docker Environment Configuration, Harbin Institute of\\nTechnology & ByteDance\\nLearn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation, TWT GmbH\\nScience & Innovation et al.\\nEnhancing Large Language Models for Secure Code Generation: A Dataset-driven Study onVulnerability Mitigation, South China University of Technology & University of Innsbruck\\n05.06 Lab: LLM-powered Website\\nOn material of session 03.06\\nIn this lab, we‚Äôll have the LLM make a website for us: it will both generate the contents of the websiteand generate all the code required for rendering, styling and navigation.\\nReading:\\nsee session 22.05\\n HTML: Creating the content, MDN\\n Getting started with CSS, MDN'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 8 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nWeek 8: Having Some Rest10.06.Ausfalltermin\\n12.06.Ausfalltermin\\nWeek 917.06. Pitch: RAG Chatbot\\nOn material of session 06.05 and session 13.05\\nThe first pitch will be dedicated to a custom RAG chatbot that the contractors (the presentingstudents, see the infos about Pitches) will have prepared to present. The RAG chatbot will have to beable to retrieve specific information from the given documents (not from the general knowledge!) anduse it in its responses. Specific requirements will be released on 22.05.\\nReading: see session 06.05, session 08.05, session 13.05, and session 15.05\\n19.06.Ausfalltermin\\nWeek 1024.06. Pitch: Handling Customer Requests in a Multi-agent Environment\\nOn material of session 20.05\\nIn the second pitch, the contractors will present their solution to automated handling of customerrequests. The solution will have to introduce a multi-agent environment to take off working load froman imagined support team. The solution will have to read and categorize tickets, generate replies and(in case of need) notify the human that their interference is required. Specific requirements will bereleased on 27.05.\\nReading: see session 20.05 and session 22.05\\n26.06. Lecture: Other Business Applications: Game Design, Financial Analysisetc.This lecture will serve a small break and will briefly go over other business scenarios that the LLMs areused in.\\nKey points:\\nGame design & narrative games\\nFinancial applications\\nContent creation\\nAdditional Reading:'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 9 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nPlayer-Driven Emergence in LLM-Driven Game Narrative, Microsoft Research\\nGenerating Converging Narratives for Games with Large Language Models, U.S. Army Research\\nLaboratory\\nGame Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and BehaviorBranch, University of Tokyo\\n AI Dungeon Games, AI Dungeon (game catalogue)\\n AI Town, Andreessen Horowitz & Convex (game)\\nIntroducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs, HuggingFace\\n(blog post)\\nBlip, bliporg (GitHub repo)\\ngigax, GigaxGames (GitHub repo)\\nLarge Language Models in Finance: A Survey, Columbia & New York University\\nFinLlama: Financial Sentiment Classification for Algorithmic Trading Applications, Imperial College\\nLondon & MIT\\nEquipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance, Monash\\nUniversity\\nLLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation,Shanghai Jiao Tong University et al.\\nAssisting in Writing Wikipedia-like Articles From Scratch with Large Language Models, Stanford\\nLarge Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools,MIT, Harvard University & MIT-IBM Watson AI Lab\\nPart 2: Applications in ScienceWeek 1101.07. Lecture: LLMs in Research: Experiment Planning & HypothesisGenerationThe first lecture dedicated to scientific applications shows how LLMs are used to plan experiments andgenerate hypothesis to accelerate research.\\nKey points:\\nExperiment planning\\nHypothesis generation\\nPredicting possible results\\nCore Reading:\\n Hypothesis Generation with Large Language Models (pages 1-9), University of Chicago &\\nToyota Technological Institute at Chicago\\n LLMs for Science: Usage for Code Generation and Data Analysis (pages 1-6), TUM\\nEmergent autonomous scientific research capabilities of large language models, Carnegie Mellon\\nUniversity\\nAdditional Reading:'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 10 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nImproving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,University of Virginia\\nPaper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance,University of Illinois at Urbana-Champaign, Carnegie Mellon University & Carleton College\\nSciLitLLM: How to Adapt LLMs for Scientific Literature Understanding, University of Science and\\nTechnology of China & DP Technology\\nMapping the Increasing Use of LLMs in Scientific Papers, Stanford\\n03.07: Lab: Experiment Planning & Hypothesis Generation\\nOn material of session 01.07\\nIn this lab, we‚Äôll practice in facilitating researcher‚Äôs work with LLMs on the example of a toy scientificresearch.\\nReading: see session 22.05\\nWeek 1208.07: Pitch: Agent for Code Generation\\nOn material of session 27.05\\nThis pitch will revolve around the contractors‚Äô implementation of a self-improving code generator. Thecode generator will have to generate both scripts and test cases for a problem given in the inputprompt, run the tests and refine the code if needed. Specific requirements will be released on 17.06.\\nReading: see session 27.05 and session 05.06\\n10.07. Lecture: Other Applications in Science: Drug Discovery, Math etc. &Scientific ReliabilityThe final core topic will mention other scientific applications of LLMs that were not covered in theprevious lectures and address the question of reliability of the results obtained with LLMs.\\nKey points:\\nDrug discovery, math & other applications\\nScientific confidence & reliability\\nCore Reading:\\n Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as ScienceCommunicators (pages 1-9), Indian Institute of Technology\\nAdditional Reading:'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11'}, page_content='12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 11 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nA Comprehensive Survey of Scientific Large Language Models and Their Applications in ScientificDiscovery, University of Illinois at Urbana-Champaign et al.\\nLarge Language Models in Drug Discovery and Development: From Disease Mechanisms to ClinicalTrials, Department of Data Science and AI, Monash University et al.\\nLLM-SR: Scientific Equation Discovery via Programming with Large Language Models, Virginia\\nTech et al.\\n Awesome Scientific Language Models, yuzhimanhua (GitHub repo)\\nCURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning, Google\\net al.\\nMultiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-ConfidentEven When They Are Wrong, Nanjing University of Aeronautics and Astronautics et al.\\nBlock 3: Wrap-upWeek 1315.07. Pitch: Agent for Web Development\\nOn material of session 03.06\\nThe contractors will present their agent that will have to generate full (minimalistic) websites by aprompt. For each website, the agent will have to generate its own style and a simple menu with workingnavigation as well as the contents. Specific requirements will be released on 24.06.\\nReading: see session 03.06 and session 05.06\\n17.07. Lecture: Role of AI in Recent YearsThe last lecture of the course will turn to societal considerations regarding LLMs and AI in general andwill investigate its role and influence on the humanity nowadays.\\nKey points:\\nStudies on influence of AI in the recent years\\nStudies on AI integration rate\\nEthical, legal & environmental aspects\\nCore Reading:\\n Protecting Human Cognition in the Age of AI (pages 1-5), The University of Texas at Austin et al.\\n Artificial intelligence governance: Ethical considerations and implications for social responsibility(pages 1-12), University of Malta\\nAdditional Reading:'),\n",
       " Document(metadata={'producer': 'macOS Version 12.7.6 (Build 21H1320) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20250512152829Z00'00'\", 'title': 'Topics Overview - LLM-based Assistants', 'moddate': \"D:20250512152829Z00'00'\", 'source': './topic_overview.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12'}, page_content=\"12.05.25, 17:28Topics Overview - LLM-based Assistants\\nPage 12 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\\nAugmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI‚ÄôsImpact on Creative Tasks, Tsinghua University & Wuhan University of Technology\\nHuman Creativity in the Age of LLMs: Randomized Experiments on Divergent and ConvergentThinking, University of Toronto\\nEmpirical evidence of Large Language Model‚Äôs influence on human spoken communication, Max-\\nPlanck Institute for Human Development\\n The 2025 AI Index Report: Top Takeaways, Stanford\\nGrowing Up: Navigating Generative AI‚Äôs Early Years ‚Äì AI Adoption Report: Executive Summary, AI at\\nWharton\\nEthical Implications of AI in Data Collection: Balancing Innovation with Privacy, AI Data Chronicles\\nLegal and ethical implications of AI-based crowd analysis: the AI Act and beyond, Vrije\\nUniversiteit\\nA Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges,Cleveland State University et al.\\nWeek 1422.07. Pitch: LLM-based Research Assistant\\nOn material of session 01.07\\nThe last pitch will introduce an agent that will have to plan the research, generate hypotheses, find theliterature etc. for a given scientific problem. It will then have to introduce its results in form of a TODOor a guide for the researcher to start off of. Specific requirements will be released on 01.07.\\nReading: see session 01.07 and session 03.07\\n24.07. Debate: Role of AI in Recent Years + Wrap-up\\nOn material of session 17.07\\nThe course will be concluded by the final debates, after which a short Q&A session will be held.\\nDebate topics:\\nLLM Behavior: Evidence of Awareness or Illusion of Understanding?\\nShould We Limit the Usage of AI?\\nReading: see session 17.07\\nCopyright ¬© 2025, Maksim ShmaltsMade with Sphinx and @pradyunsg's Furo\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1122db9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.05.25, 17:28Topics Overview - LLM-based Assistants\n",
      "Page 1 of 12https://maxschmaltz.github.io/Course-LLM-based-Assistants/infos/topic_overview.html\n",
      "To p i c s  O v e r v i e wThe schedule is preliminary and subject to changes!\n",
      "The reading for each lecture is given as references to the sources the respective lectures base on. Youare not obliged to read anything. However, you are strongly encouraged to read references marked bypin emojis \n",
      ": those are comprehensive overviews on the topics or important works that are beneficialfor a better understanding of the key concepts. For the pinned papers, I also specify the pages span foryou to focus on the most important fragments. Some of the sources are also marked with a popcornemoji \n",
      ": that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboardsetc. (also a couple of LLM-based games). For each of the sources, I also leave my subjectiveestimation of how important this work is for this specific topic: from yellow \n",
      " ‚Äòpartially useful‚Äô thoughorange \n",
      " ‚Äòuseful‚Äô to red \n",
      " ‚Äòcrucial findings / thoughts‚Äô.  T h e s e  e s t i m a t i o n s  w i l l  b e  c o n t i n u o u s l yupdated as I revise the materials.\n",
      "For the labs, you are provided with practical tutorials that respective lab tasks will mostly derive from.The core tutorials are marked with a writing emoji \n",
      "; you are asked to inspect them in advance(better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to preparein advance to keep up with the lab.\n",
      "Disclaimer: the reading entries are no proper citations; the bibtex references as well as detailed infosabout the authors, publish date etc. can be found under the entry links.\n",
      "Block 1: IntroWeek 122.04. Lecture: LLMs as a Form of Intelligence vs LLMs as Statistical MachinesThat is an introductory lecture, in which I will briefly introduce the course and we‚Äôll have a warming updiscussion about different perspectives on LLMs‚Äô nature. We will focus on two prominent outlooks: LLMis a form of intelligence and LLM is a complex statistical machine. We‚Äôll discuss differences of LLMswith human intelligence and the degree to which LLMs exhibit (self-)awareness.\n",
      "Key points:\n",
      "Course introduction\n",
      "Different perspectives on the nature of LLMs\n",
      "Similarities and differences between human and artificial intelligence\n",
      "LLMs‚Äô (self-)awareness\n",
      "Core Reading:\n",
      " The Debate Over Understanding in AI‚Äôs Large Language Models (pages 1-7), Santa Fe\n",
      "Institute \n",
      "Meaning without reference in large language models, UC Berkeley & DeepMind \n",
      "Dissociating language and thought in large language models (intro [right after the abstract, seemore on the sectioning in this paper at the bottom of page 2], sections 1, 2.3 [LLMs are predictive‚Ä¶], 3-5), The University of Texas at Austin et al. \n",
      "Additional Reading:\n",
      "LLM-basedAssistants\n",
      "INFOS AND STUFF\n",
      "BLOCK 1: INTRO\n",
      "BLOCK 2: CORE TOPICS | PART 1:BUSINESS APPLICATIONS\n",
      "BLOCK 2: CORE TOPICS | PART 2:APPLICATIONS IN SCIENCE\n",
      "BLOCK 3: WRAP-UP\n",
      "Topics Overview\n",
      "Debates\n",
      "Pitches\n",
      "LLM Inference Guide\n",
      "22.04. LLMs as a Form ofIntelligence vs LLMs asStatistical Machines\n",
      "24.04. LLM & Agent Basics\n",
      "29.04. Intro to LangChain \n",
      "!\n",
      "\"\n",
      "06.05. Virtual Assistants Pt. 1:Chatbots\n",
      "08.05. Basic LLM-basedChatbot \n",
      "#\n",
      "Under development\n",
      "Under development\n",
      "Search\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7561e",
   "metadata": {},
   "source": [
    "As you can see, the result is not satisfying because the PDF has a more complex structure than just one-paragraph text. To handle it's layout, we could use `UnstructuredLoader`-like OCR engines that will return a `Document` not for the whole page but for a single structure. One of the open-source solutions is [Docling](https://docs.langchain.com/oss/python/integrations/document_loaders/docling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d34f61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxschmaltz/Downloads/1213/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd803e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 12:33:24,123 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-12 12:33:24,182 - INFO - Going to convert document batch...\n",
      "2025-11-12 12:33:24,183 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-12 12:33:24,203 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-12 12:33:24,207 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-12 12:33:24,207 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-12 12:33:24,216 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-12 12:33:24,222 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-12 12:33:24,222 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-12 12:33:29,809 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-12 12:33:29,816 - INFO - Accelerator device: 'mps'\n",
      "2025-11-12 12:33:45,748 - INFO - Accelerator device: 'mps'\n",
      "2025-11-12 12:33:46,425 - INFO - Processing document topic_overview.pdf\n",
      "2025-11-12 12:33:55,289 - INFO - Finished converting document topic_overview.pdf in 31.17 sec.\n"
     ]
    }
   ],
   "source": [
    "loader = DoclingLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f4104",
   "metadata": {},
   "source": [
    "Look at how Docling parsed a single structure (left sidebar) into a separate `Document`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80791265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-based Assistants\n",
      "Search\n",
      "INFOS AND STUFF\n",
      "Topics Overview\n",
      "Debates\n",
      "Pitches\n",
      "LLM Inference Guide\n",
      "BLOCK 1: INTRO\n",
      "22.04. LLMs as a Form of Intelligence vs LLMs as Statistical Machines\n",
      "24.04. LLM & Agent Basics\n",
      "29.04. Intro to LangChain\n",
      "BLOCK 2: CORE TOPICS | PART 1: BUSINESS APPLICATIONS\n",
      "06.05. Virtual Assistants Pt. 1: Chatbots\n",
      "08.05. Basic LLM-based Chatbot\n",
      "BLOCK 2: CORE TOPICS | PART 2: APPLICATIONS IN SCIENCE\n",
      "Under development\n",
      "BLOCK 3: WRAP-UP\n",
      "Under development\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c845c6",
   "metadata": {},
   "source": [
    "However, it also parsed every paragraph from the body of the PDF into a separate `Document`, which destroyed the links between the paragraphs of the same sections: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0abddab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key points :\n",
      "- Course introduction\n",
      "- Different perspectives on the nature of LLMs\n",
      "- Similarities and differences between human and artificial intelligence\n",
      "- LLMs' (self-)awareness\n"
     ]
    }
   ],
   "source": [
    "print(docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964892ef",
   "metadata": {},
   "source": [
    "To avoid such a fine chunking, we ask the engine to output the whole PDF as a single MD file and then chunk it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d19bba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 12:35:16,464 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-12 12:35:16,469 - INFO - Going to convert document batch...\n",
      "2025-11-12 12:35:16,470 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-12 12:35:16,470 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-11-12 12:35:16,471 - INFO - Accelerator device: 'mps'\n",
      "2025-11-12 12:35:17,811 - INFO - Accelerator device: 'mps'\n",
      "2025-11-12 12:35:18,613 - INFO - Processing document topic_overview.pdf\n",
      "2025-11-12 12:35:26,337 - INFO - Finished converting document topic_overview.pdf in 9.88 sec.\n"
     ]
    }
   ],
   "source": [
    "loader = DoclingLoader(file_path, export_type=ExportType.MARKDOWN)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b64395d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d4bcc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LLM-based Assistants\n",
      "\n",
      "Search\n",
      "\n",
      "INFOS AND STUFF\n",
      "\n",
      "Topics Overview\n",
      "\n",
      "Debates\n",
      "\n",
      "Pitches\n",
      "\n",
      "LLM Inference Guide\n",
      "\n",
      "BLOCK 1: INTRO\n",
      "\n",
      "22.04. LLMs as a Form of Intelligence vs LLMs as Statistical Machines\n",
      "\n",
      "24.04. LLM &amp; Agent Basics\n",
      "\n",
      "29.04. Intro to LangChain\n",
      "\n",
      "BLOCK 2: CORE TOPICS | PART 1: BUSINESS APPLICATIONS\n",
      "\n",
      "06.05. Virtual Assistants Pt. 1: Chatbots\n",
      "\n",
      "08.05. Basic LLM-based Chatbot\n",
      "\n",
      "BLOCK 2: CORE TOPICS | PART 2: APPLICATIONS IN SCIENCE\n",
      "\n",
      "Under development\n",
      "\n",
      "BLOCK 3: WRAP-UP\n",
      "\n",
      "Under development\n",
      "\n",
      "## Topics Overview\n",
      "\n",
      "The schedule is preliminary and subject to changes !\n",
      "\n",
      "The reading for each lecture is given as references to the sources the respective lectures base on. You are not obliged to read anything. However, you are strongly encouraged to read references marked by pin emojis : those are comprehensive overviews on the topics or important works that are beneficial for a better understanding of the key concepts. For the pinned papers, I also specify the pages span for you to focus on the most important fragments. Some of the sources are also marked with a popcorn emoji : that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboards etc. (also a couple of LLM-based games). For each of the sources, I also leave my subjective estimation of how important this work is for this specific topic: from yellow 'partially useful' though orange 'useful' to red ' crucial findings / thoughts ' . These estimations will be continuously updated as I revise the materials.\n",
      "\n",
      "For the labs , you are provided with practical tutorials that respective lab tasks will mostly derive from. The core tutorials are marked with a writing emoji ; you are asked to inspect them in advance (better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to prepare in advance to keep up with the lab.\n",
      "\n",
      "Disclaimer : the reading entries are no proper citations; the bibtex references as well as detailed infos about the authors, publish date etc. can be found under the entry links.\n",
      "\n",
      "## Block 1: Intro\n",
      "\n",
      "Week 1\n",
      "\n",
      "## 22.04. Lecture : LLMs as a Form of Intelligence vs LLMs as Statistical Machines\n",
      "\n",
      "That is an introductory lecture, in which I will briefly introduce the course and we'll have a warming up discussion about different perspectives on LLMs' nature. We will focus on two prominent outlooks: LLM is a form of intelligence and LLM is a complex statistical machine. We'll discuss differences of LLMs with human intelligence and the degree to which LLMs exhibit (self-)awareness.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Course introduction\n",
      "- Different perspectives on the nature of LLMs\n",
      "- Similarities and differences between human and artificial intelligence\n",
      "- LLMs' (self-)awareness\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- The Debate Over Understanding in AI's Large Language Models (pages 1-7), Santa Fe Institute\n",
      "- Meaning without reference in large language models, UC Berkeley &amp; DeepMind\n",
      "- Dissociating language and thought in large language models (intro [right after the abstract, see more on the sectioning in this paper at the bottom of page 2], sections 1, 2.3 [ LLMs are predictive ‚Ä¶ ], 3-5), The University of Texas at Austin et al.\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Do Large Language Models Understand Us ? , Google Research\n",
      "- Sparks of Artificial General Intelligence: Early experiments with GPT-4 (chapters 1-8 &amp; 10), Microsoft Research\n",
      "- On the Dangers of Stochastic Parrots: Can Language Models Be Too Big ? (paragraphs 1, 5, 6.1), University of Washington et al.\n",
      "- Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding, Leiden Institute of Advanced Computer Science &amp; Leiden University Medical Centre\n",
      "\n",
      "## 24.04. Lecture : LLM &amp; Agent Basics\n",
      "\n",
      "In this lecture, we'll recap some basics about LLMs and LLM-based agents to make sure we're on the same page.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- LLM recap\n",
      "- Prompting\n",
      "- Structured output\n",
      "- Tool calling\n",
      "- Piping &amp; Planning\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- A Survey of Large Language Models, (sections 1, 2.1, 4.1, 4.2.1, 4.2.3-4.2.4, 4.3, 5.1.1-5.1.3, 5.2.15.2.4, 5.3.1, 6) Renmin University of China et al.\n",
      "- Emergent Abilities of Large Language Models, Google Research, Stanford, UNC Chapel Hill, DeepMind\n",
      "- 'We Need Structured Output': Towards User-centered Constraints on Large Language Model Output, Google Research &amp; Google\n",
      "- Agent Instructs Large Language Models to be General Zero-Shot Reasoners (pages 1-9), Washington University &amp; UC Berkeley\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Language Models are Few-Shot Learners, OpenAI\n",
      "- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Google Research\n",
      "- The Llama 3 Herd of Models, Meta AI\n",
      "- Introducing Structured Outputs in the API, OpenAI\n",
      "- Tool Learning with Large Language Models: A Survey, Renmin University of China et al.\n",
      "- ToolACE: Winning the Points of LLM Function Calling, Huawei Noah's Ark Lab et al.\n",
      "- Toolformer: Language Models Can Teach Themselves to Use Tools, Meta AI\n",
      "- Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks, IBM Research\n",
      "- Berkeley Function-Calling Leaderboard, UC Berkeley (leaderboard)\n",
      "- A Survey on Multimodal Large Language Models, University of Science and Technology of China &amp; Tencent YouTu Lab\n",
      "\n",
      "## Week 2\n",
      "\n",
      "## 29.04. Lab : Intro to LangChain\n",
      "\n",
      "The final introductory session will guide you through the most basic concepts of LangChain for the further practical sessions.\n",
      "\n",
      "## Reading :\n",
      "\n",
      "- Runnable interface, LangChain\n",
      "- LangChain Expression Language (LCEL), LangChain\n",
      "- Messages, LangChain\n",
      "- Chat models, LangChain\n",
      "- Structured outputs, LangChain\n",
      "- Tools, LangChain\n",
      "- Tool calling, LangChain\n",
      "\n",
      "## 01.05.\n",
      "\n",
      "Ausfalltermin\n",
      "\n",
      "## Block 2: Core Topics\n",
      "\n",
      "## Part 1: Business Applications\n",
      "\n",
      "## Week 3\n",
      "\n",
      "06.05.\n",
      "\n",
      "## Lecture : Virtual Assistants Pt. 1: Chatbots\n",
      "\n",
      "The first core topic concerns chatbots. We'll discuss how chatbots are built, how they (should) handle harmful requests and you can tune it for your use case.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- LLMs alignment\n",
      "- Memory\n",
      "- Prompting &amp; automated prompt generation\n",
      "- Evaluation\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- Aligning Large Language Models with Human: A Survey (pages 1-14), Huawei Noah's Ark Lab\n",
      "- Self-Instruct: Aligning Language Models with Self-Generated Instructions, University of Washington et al.\n",
      "- A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications, Indian Institute of Technology Patna, Stanford &amp; Amazon AI\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Training language models to follow instructions with human feedback, OpenAI\n",
      "- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Anthropic\n",
      "- A Survey on the Memory Mechanism of Large Language Model based Agents, Renmin University of China &amp; Huawei Noah's Ark Lab\n",
      "- Augmenting Language Models with Long-Term Memory, UC Santa Barbara &amp; Microsoft Research\n",
      "- From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models, Beike Inc.\n",
      "- Automatic Prompt Selection for Large Language Models, Cinnamon AI, Hung Yen University of Technology and Education &amp; Deakin University\n",
      "- PromptGen: Automatically Generate Prompts using Generative Models, Baidu Research\n",
      "- Evaluating Large Language Models. A Comprehensive Survey, Tianjin University\n",
      "\n",
      "## 08.05. Lab : Basic LLM-based Chatbot\n",
      "\n",
      "On material of session 06.05\n",
      "\n",
      "In this lab, we'll build a chatbot and try different prompts and settings to see how it affects the output.\n",
      "\n",
      "## Reading :\n",
      "\n",
      "- Build a Chatbot, LangChain\n",
      "- LangGraph Quickstart: Build a Basic Chatbot (parts 1, 3), LangGraph\n",
      "- How to add summary of the conversation history, LangGraph\n",
      "- Prompt Templates, LangChain\n",
      "- Few-shot prompting, LangChain\n",
      "\n",
      "## Week 4\n",
      "\n",
      "## 13.05. Lecture : Virtual Assistants Pt. 2: RAG\n",
      "\n",
      "Continuing the first part, the second part will expand scope of chatbot functionality and will teach it to refer to custom knowledge base to retrieve and use user-specific information. Finally, the most widely used deployment methods will be briefly introduced.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- General knowledge vs context\n",
      "- Knowledge indexing, retrieval &amp; ranking\n",
      "- Retrieval tools\n",
      "- Agentic RAG\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- Retrieval Augmented Generation or Long-Context LLMs ? A Comprehensive Study and Hybrid Approach (pages 1-7), Google DeepMind &amp; University of Michigan\n",
      "- A Survey on Retrieval-Augmented Text Generation for Large Language Models (sections 1-7), York University\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National Chengchi University &amp; Academia Sinica\n",
      "- Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, University of Washington, Allen Institute for AI &amp; IBM Research AI\n",
      "- Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity, Korea Advanced Institute of Science and Technology\n",
      "- Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese Academy of Sciences\n",
      "- Querying Databases with Function Calling, Weaviate, Contextual AI &amp; Morningstar\n",
      "\n",
      "## 15.05. Lab : RAG Chatbot\n",
      "\n",
      "On material of session 13.05\n",
      "\n",
      "In this lab, we'll expand the functionality of the chatbot built at the last lab to connect it to user-specific information.\n",
      "\n",
      "## Reading :\n",
      "\n",
      "- How to load PDFs, LangChain\n",
      "- Text splitters, LangChain\n",
      "- Embedding models, LangChain\n",
      "- Vector stores, LangChain\n",
      "- Retrievers, LangChain\n",
      "- Retrieval augmented generation (RAG), LangChain\n",
      "- LangGraph Quickstart: Build a Basic Chatbot (part 2), LangGraph\n",
      "- Agentic RAG, LangGraph\n",
      "- Adaptive RAG, LangGraph\n",
      "- Multimodality, LangChain\n",
      "\n",
      "## Week 5\n",
      "\n",
      "## 20.05. Lecture : Virtual Assistants Pt. 3: Multi-agent Environment\n",
      "\n",
      "This lectures concludes the Virtual Assistants cycle and directs its attention to automating everyday / business operations in a multi-agent environment. We'll look at how agents communicate with each other, how their communication can be guided (both with and without involvement of a human), and this all is used in real applications.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Multi-agent environment\n",
      "- Human in the loop\n",
      "- LLMs as evaluators\n",
      "- Examples of pipelines for business operations\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- LLM-based Multi-Agent Systems: Techniques and Business Perspectives (pages 1-8), Shanghai Jiao Tong University &amp; OPPO Research Institute\n",
      "- Generative Agents: Interactive Simulacra of Human Behavior, Stanford, Google Research &amp; DeepMind\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Improving Factuality and Reasoning in Language Models through Multiagent Debate, MIT &amp; Google Brain\n",
      "- Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View, Zhejiang University, National University of Singapore &amp; DeepMind\n",
      "- AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, Microsoft Research et al.\n",
      "- How real-world businesses are transforming with AI - with more than 140 new stories, Microsoft (blog post)\n",
      "- Built with LangGraph, LangGraph (website page)\n",
      "- Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant, Delft University of Technology &amp; The University of Queensland\n",
      "\n",
      "## 22.05. Lab : Multi-agent Environment\n",
      "\n",
      "On material of session 20.05\n",
      "\n",
      "This lab will introduce a short walkthrough to creation of a multi-agent environment for automated meeting scheduling and preparation. We will see how the coordinator agent will communicate with two auxiliary agents to check time availability and prepare an agenda for the meeting.\n",
      "\n",
      "## Reading :\n",
      "\n",
      "- Multi-agent network, LangGraph\n",
      "- Human-in-the-loop, LangGraph\n",
      "- Plan-and-Execute, LangGraph\n",
      "- Reflection, LangGraph\n",
      "- Multi-agent supervisor, LangGraph\n",
      "- Quick Start, AutoGen\n",
      "\n",
      "## Week 6\n",
      "\n",
      "## 27.05. Lecture : Software Development Pt. 1: Code Generation, Evaluation &amp; Testing\n",
      "\n",
      "This lectures opens a new lecture mini-cycle dedicated to software development. The first lecture overviews how LLMs are used to generate reliable code and how generated code is tested and improved to deal with the errors.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Code generation &amp; refining\n",
      "- Automated testing\n",
      "- Generated code evaluation\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- Large Language Model-Based Agents for Software Engineering: A Survey, Fudan University, Nanyang Technological University &amp; University of Illinois at Urbana-Champaign\n",
      "- CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (pages 1-20), Salesforce Research\n",
      "- The ART of LLM Refinement: Ask, Refine, and Trust, ETH Zurich &amp; Meta AI\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Planning with Large Language Models for Code Generation, MIT-IBM Watson AI Lab et al.\n",
      "- Code Repair with LLMs gives an Exploration-Exploitation Tradeoff, Cornell, Shanghai Jiao Tong University &amp; University of Toronto\n",
      "- ChatUniTest: A Framework for LLM-Based Test Generation, Zhejiang University &amp; Hangzhou City University\n",
      "- TestART: Improving LLM-based Unit Testing via Co-evolution of Automated Generation and Repair Iteration, Nanjing University &amp; Huawei Cloud Computing Technologies\n",
      "- Evaluating Large Language Models Trained on Code, `OpenAI\n",
      "- Code Generation on HumanEval, OpenAI (leaderboard)\n",
      "- CodeJudge: Evaluating Code Generation with Large Language Models, Huazhong University of Science and Technology &amp; Purdue University\n",
      "\n",
      "## 29.05.\n",
      "\n",
      "Ausfalltermin\n",
      "\n",
      "## Week 7\n",
      "\n",
      "## 03.06. Lecture : Software Development Pt. 2: Copilots, LLM-powered Websites\n",
      "\n",
      "The second and the last lecture of the software development cycle focuses on practical application of LLM code generation, in particular, on widely-used copilots (real-time code generation assistants) and LLM-supported web development.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Copilots &amp; real-time hints\n",
      "- LLM-powered websites\n",
      "- LLM-supported deployment\n",
      "- Further considerations: reliability, sustainability etc.\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities and Limitations (pages 1-11), University of Oslo\n",
      "- A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis, Google DeepMind &amp; The University of Tokyo\n",
      "- Can ChatGPT replace StackOverflow ? A Study on Robustness and Reliability of Large Language Model Code Generation, UC San Diego\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Design and evaluation of AI copilots - case studies of retail copilot templates, Microsoft\n",
      "- Your AI Companion, Microsoft (blog post)\n",
      "- GitHub Copilot, GitHub (product page)\n",
      "- Research: quantifying GitHub Copilot's impact on developer productivity and happiness, GitHub (blog post)\n",
      "- Cursor: The AI Code Editor, Cursor (product page)\n",
      "- Automated Unit Test Improvement using Large Language Models at Meta, Meta\n",
      "- Human-In-the-Loop Software Development Agents, Monash University, The University of Melbourne &amp; Atlassian\n",
      "- An LLM-based Agent for Reliable Docker Environment Configuration, Harbin Institute of Technology &amp; ByteDance\n",
      "- Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation, TWT GmbH Science &amp; Innovation et al.\n",
      "- Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation, South China University of Technology &amp; University of Innsbruck\n",
      "\n",
      "## 05.06 Lab : LLM-powered Website\n",
      "\n",
      "On material of session 03.06\n",
      "\n",
      "In this lab, we'll have the LLM make a website for us: it will both generate the contents of the website and generate all the code required for rendering, styling and navigation.\n",
      "\n",
      "## Reading :\n",
      "\n",
      "- see session 22.05\n",
      "\n",
      "- HTML: Creating the content, MDN\n",
      "\n",
      "- Getting started with CSS, MDN\n",
      "\n",
      "## Week 8: Having Some Rest\n",
      "\n",
      "## 10.06.\n",
      "\n",
      "Ausfalltermin\n",
      "\n",
      "12.06.\n",
      "\n",
      "Ausfalltermin\n",
      "\n",
      "## Week 9\n",
      "\n",
      "17.06.\n",
      "\n",
      "## Pitch : RAG Chatbot\n",
      "\n",
      "On material of session 06.05 and session 13.05\n",
      "\n",
      "The first pitch will be dedicated to a custom RAG chatbot that the contractors (the presenting students, see the infos about Pitches) will have prepared to present. The RAG chatbot will have to be able to retrieve specific information from the given documents (not from the general knowledge ! ) and use it in its responses. Specific requirements will be released on 22.05.\n",
      "\n",
      "Reading : see session 06.05, session 08.05, session 13.05, and session 15.05\n",
      "\n",
      "19.06.\n",
      "\n",
      "Ausfalltermin\n",
      "\n",
      "## Week 10\n",
      "\n",
      "## 24.06. Pitch : Handling Customer Requests in a Multi-agent Environment\n",
      "\n",
      "On material of session 20.05\n",
      "\n",
      "In the second pitch, the contractors will present their solution to automated handling of customer requests. The solution will have to introduce a multi-agent environment to take off working load from an imagined support team. The solution will have to read and categorize tickets, generate replies and (in case of need) notify the human that their interference is required. Specific requirements will be released on 27.05.\n",
      "\n",
      "Reading : see session 20.05 and session 22.05\n",
      "\n",
      "## 26.06. Lecture : Other Business Applications: Game Design, Financial Analysis etc.\n",
      "\n",
      "This lecture will serve a small break and will briefly go over other business scenarios that the LLMs are used in.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Game design &amp; narrative games\n",
      "- Financial applications\n",
      "- Content creation\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Player-Driven Emergence in LLM-Driven Game Narrative, Microsoft Research\n",
      "- Generating Converging Narratives for Games with Large Language Models, U.S. Army Research Laboratory\n",
      "- Game Agent Driven by Free-Form Text Command: Using LLM-based Code Generation and Behavior Branch, University of Tokyo\n",
      "- AI Dungeon Games, AI Dungeon (game catalogue)\n",
      "- AI Town, Andreessen Horowitz &amp; Convex (game)\n",
      "- Introducing NPC-Playground, a 3D playground to interact with LLM-powered NPCs, HuggingFace (blog post)\n",
      "- Blip, bliporg (GitHub repo)\n",
      "- gigax, GigaxGames (GitHub repo)\n",
      "- Large Language Models in Finance: A Survey, Columbia &amp; New York University\n",
      "- FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications, Imperial College London &amp; MIT\n",
      "- Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance, Monash University\n",
      "- LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation, Shanghai Jiao Tong University et al.\n",
      "- Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models, Stanford\n",
      "- Large Language Models Can Solve Real-World Planning Rigorously with Formal Verification Tools, MIT, Harvard University &amp; MIT-IBM Watson AI Lab\n",
      "\n",
      "## Part 2: Applications in Science\n",
      "\n",
      "## Week 11\n",
      "\n",
      "## 01.07. Lecture : LLMs in Research: Experiment Planning &amp; Hypothesis Generation\n",
      "\n",
      "The first lecture dedicated to scientific applications shows how LLMs are used to plan experiments and generate hypothesis to accelerate research.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Experiment planning\n",
      "- Hypothesis generation\n",
      "- Predicting possible results\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- Hypothesis Generation with Large Language Models (pages 1-9), University of Chicago &amp; Toyota Technological Institute at Chicago\n",
      "- LLMs for Science: Usage for Code Generation and Data Analysis (pages 1-6), TUM\n",
      "- Emergent autonomous scientific research capabilities of large language models, Carnegie Mellon University\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models, University of Virginia\n",
      "- Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance, University of Illinois at Urbana-Champaign, Carnegie Mellon University &amp; Carleton College\n",
      "- SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding, University of Science and Technology of China &amp; DP Technology\n",
      "- Mapping the Increasing Use of LLMs in Scientific Papers, Stanford\n",
      "\n",
      "## 03.07: Lab : Experiment Planning &amp; Hypothesis Generation\n",
      "\n",
      "On material of session 01.07\n",
      "\n",
      "In this lab, we'll practice in facilitating researcher's work with LLMs on the example of a toy scientific research.\n",
      "\n",
      "Reading : see session 22.05\n",
      "\n",
      "## Week 12\n",
      "\n",
      "## 08.07: Pitch : Agent for Code Generation\n",
      "\n",
      "On material of session 27.05\n",
      "\n",
      "This pitch will revolve around the contractors' implementation of a self-improving code generator. The code generator will have to generate both scripts and test cases for a problem given in the input prompt, run the tests and refine the code if needed. Specific requirements will be released on 17.06.\n",
      "\n",
      "Reading : see session 27.05 and session 05.06\n",
      "\n",
      "## 10.07. Lecture : Other Applications in Science: Drug Discovery, Math etc. &amp; Scientific Reliability\n",
      "\n",
      "The final core topic will mention other scientific applications of LLMs that were not covered in the previous lectures and address the question of reliability of the results obtained with LLMs.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Drug discovery, math &amp; other applications\n",
      "- Scientific confidence &amp; reliability\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- Can LLMs replace Neil deGrasse Tyson ? Evaluating the Reliability of LLMs as Science Communicators (pages 1-9), Indian Institute of Technology\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery, University of Illinois at Urbana-Champaign et al.\n",
      "- Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials, Department of Data Science and AI, Monash University et al.\n",
      "- LLM-SR: Scientific Equation Discovery via Programming with Large Language Models, Virginia Tech et al.\n",
      "- Awesome Scientific Language Models, yuzhimanhua (GitHub repo)\n",
      "- CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning, Google et al.\n",
      "- Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong, Nanjing University of Aeronautics and Astronautics et al.\n",
      "\n",
      "## Block 3: Wrap-up\n",
      "\n",
      "## Week 13\n",
      "\n",
      "## 15.07. Pitch : Agent for Web Development\n",
      "\n",
      "On material of session 03.06\n",
      "\n",
      "The contractors will present their agent that will have to generate full (minimalistic) websites by a prompt. For each website, the agent will have to generate its own style and a simple menu with working navigation as well as the contents. Specific requirements will be released on 24.06.\n",
      "\n",
      "Reading : see session 03.06 and session 05.06\n",
      "\n",
      "## 17.07. Lecture : Role of AI in Recent Years\n",
      "\n",
      "The last lecture of the course will turn to societal considerations regarding LLMs and AI in general and will investigate its role and influence on the humanity nowadays.\n",
      "\n",
      "## Key points :\n",
      "\n",
      "- Studies on influence of AI in the recent years\n",
      "- Studies on AI integration rate\n",
      "- Ethical, legal &amp; environmental aspects\n",
      "\n",
      "## Core Reading :\n",
      "\n",
      "- Protecting Human Cognition in the Age of AI (pages 1-5), The University of Texas at Austin et al.\n",
      "- Artificial intelligence governance: Ethical considerations and implications for social responsibility (pages 1-12), University of Malta\n",
      "\n",
      "## Additional Reading :\n",
      "\n",
      "- Augmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI's Impact on Creative Tasks, Tsinghua University &amp; Wuhan University of Technology\n",
      "- Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking, University of Toronto\n",
      "- Empirical evidence of Large Language Model's influence on human spoken communication, MaxPlanck Institute for Human Development\n",
      "- The 2025 AI Index Report: Top Takeaways, Stanford\n",
      "- Growing Up: Navigating Generative AI's Early Years - AI Adoption Report: Executive Summary, AI at Wharton\n",
      "- Ethical Implications of AI in Data Collection: Balancing Innovation with Privacy, AI Data Chronicles\n",
      "- Legal and ethical implications of AI-based crowd analysis: the AI Act and beyond, Vrije Universiteit\n",
      "- A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges, Cleveland State University et al.\n",
      "\n",
      "## Week 14\n",
      "\n",
      "## 22.07. Pitch : LLM-based Research Assistant\n",
      "\n",
      "On material of session 01.07\n",
      "\n",
      "The last pitch will introduce an agent that will have to plan the research, generate hypotheses, find the literature etc. for a given scientific problem. It will then have to introduce its results in form of a TODO or a guide for the researcher to start off of. Specific requirements will be released on 01.07.\n",
      "\n",
      "Reading : see session 01.07 and session 03.07\n",
      "\n",
      "## 24.07. Debate : Role of AI in Recent Years + Wrap-up\n",
      "\n",
      "On material of session 17.07\n",
      "\n",
      "The course will be concluded by the final debates, after which a short Q&amp;A session will be held. Copyright ¬© 2025, Maksim Shmalts Made with Sphinx and @pradyunsg's Furo\n",
      "\n",
      "Debate topics:\n",
      "\n",
      "- LLM Behavior: Evidence of Awareness or Illusion of Understanding ?\n",
      "- Should We Limit the Usage of AI ?\n",
      "\n",
      "Reading : see session 17.07\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b292a",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "During RAG, relevant documents are usually retrieved by semantic similarity that is calculated between the search query and each document in the index. However, if we calculate vectors for the entire PDF pages, we risk not to capture any meaning in the embedding because the context is just too long. That is why usually, loaded text is _chunked_ in a RAG application; embeddings for smaller pieces of text are more discriminative, and thus the relevant context may be retrieved more reliably. Furthermore, it ensure process consistency when working documents of varying sizes.\n",
    "\n",
    "Different approaches to chunking are described in tutorial [Text splitters](https://docs.langchain.com/oss/python/integrations/splitters/index#text-splitters) from LangChain. Even though Docling returned a MD document, we won't be using the dedicated [`MarkdownHeaderTextSplitter`](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter); instead, for a more general picture, we choose the [`RecursiveCharacterTextSplitter`](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter) -- a good option in terms of simplicity-quality ratio for simple cases. This splitter tries to keep text structures (paragraphs, sentences) together and thus maintain text coherence in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db9b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92bef398",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, # maximum number of characters in a chunk\n",
    "    chunk_overlap=250, # number of characters to overlap between chunks\n",
    "    separators=[\"\\n\"]\n",
    ")\n",
    "\n",
    "def split_page(doc: Document) -> List[Document]:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                **doc.metadata,\n",
    "                \"chunk_n\": i\n",
    "            },\n",
    "        ) \n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6da17475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1 pages into 51 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for doc in docs:\n",
    "    chunks += split_page(doc)\n",
    "\n",
    "print(f\"Converted {len(docs)} pages into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "066c6f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 0}, page_content='## LLM-based Assistants\\n\\nSearch\\n\\nINFOS AND STUFF\\n\\nTopics Overview\\n\\nDebates\\n\\nPitches\\n\\nLLM Inference Guide\\n\\nBLOCK 1: INTRO\\n\\n22.04. LLMs as a Form of Intelligence vs LLMs as Statistical Machines\\n\\n24.04. LLM &amp; Agent Basics\\n\\n29.04. Intro to LangChain\\n\\nBLOCK 2: CORE TOPICS | PART 1: BUSINESS APPLICATIONS\\n\\n06.05. Virtual Assistants Pt. 1: Chatbots\\n\\n08.05. Basic LLM-based Chatbot\\n\\nBLOCK 2: CORE TOPICS | PART 2: APPLICATIONS IN SCIENCE\\n\\nUnder development\\n\\nBLOCK 3: WRAP-UP\\n\\nUnder development\\n\\n## Topics Overview\\n\\nThe schedule is preliminary and subject to changes !'),\n",
       " Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 1}, page_content=\"\\nThe reading for each lecture is given as references to the sources the respective lectures base on. You are not obliged to read anything. However, you are strongly encouraged to read references marked by pin emojis : those are comprehensive overviews on the topics or important works that are beneficial for a better understanding of the key concepts. For the pinned papers, I also specify the pages span for you to focus on the most important fragments. Some of the sources are also marked with a popcorn emoji : that is misc material you might want to take a look at: blog posts, GitHub repos, leaderboards etc. (also a couple of LLM-based games). For each of the sources, I also leave my subjective estimation of how important this work is for this specific topic: from yellow 'partially useful' though orange 'useful' to red ' crucial findings / thoughts ' . These estimations will be continuously updated as I revise the materials.\"),\n",
       " Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 2}, page_content='For the labs , you are provided with practical tutorials that respective lab tasks will mostly derive from. The core tutorials are marked with a writing emoji ; you are asked to inspect them in advance (better yet: try them out). On lab sessions, we will only briefly recap them so it is up to you to prepare in advance to keep up with the lab.\\n\\nDisclaimer : the reading entries are no proper citations; the bibtex references as well as detailed infos about the authors, publish date etc. can be found under the entry links.\\n\\n## Block 1: Intro\\n\\nWeek 1\\n\\n## 22.04. Lecture : LLMs as a Form of Intelligence vs LLMs as Statistical Machines')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e20fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Additional Reading :\n",
      "\n",
      "- Training language models to follow instructions with human feedback, OpenAI\n",
      "- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Anthropic\n",
      "- A Survey on the Memory Mechanism of Large Language Model based Agents, Renmin University of China &amp; Huawei Noah's Ark Lab\n",
      "- Augmenting Language Models with Long-Term Memory, UC Santa Barbara &amp; Microsoft Research\n",
      "- From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models, Beike Inc.\n",
      "- Automatic Prompt Selection for Large Language Models, Cinnamon AI, Hung Yen University of Technology and Education &amp; Deakin University\n"
     ]
    }
   ],
   "source": [
    "print(chunks[12].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77fe86",
   "metadata": {},
   "source": [
    "### Convert to Embeddings\n",
    "\n",
    "As discussed, the retrieval usually supported by vector similarity and so the index contains not the actual texts but their vector representations. Vector representations are created by _embedding models_ -- models usually made specifically for this objective by being trained to create more similar vectors for more similar sentences and to push apart dissimilar sentences in the vector space.\n",
    "\n",
    "We will use the [`nv-embedqa-e5-v5`](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5?snippet_tab=LangChain) model -- a model from NVIDIA pretrained for English QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd0dbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36457d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_NAME = \"nvidia/nv-embedqa-e5-v5\"\n",
    "\n",
    "embeddings = NVIDIAEmbeddings(\n",
    "    model=EMBEDDING_NAME, \n",
    "    # api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1d288",
   "metadata": {},
   "source": [
    "An embedding model receives an input text and returns a dense vector that is believed to capture its semantic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f68bafff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0036773681640625,\n",
       " -0.043121337890625,\n",
       " 0.047119140625,\n",
       " 0.0009489059448242188,\n",
       " 0.0088958740234375,\n",
       " 0.012298583984375,\n",
       " -0.032257080078125,\n",
       " -0.035064697265625,\n",
       " 0.049346923828125,\n",
       " -0.06866455078125,\n",
       " 0.0357666015625,\n",
       " -0.0167083740234375,\n",
       " 0.0596923828125,\n",
       " -0.019317626953125,\n",
       " 0.004131317138671875,\n",
       " 0.0254974365234375,\n",
       " -0.005035400390625,\n",
       " 0.0222320556640625,\n",
       " 0.03277587890625,\n",
       " -0.03277587890625,\n",
       " -0.01904296875,\n",
       " 0.0172882080078125,\n",
       " 0.01319122314453125,\n",
       " -0.027923583984375,\n",
       " 0.0295562744140625,\n",
       " -0.018035888671875,\n",
       " 0.01561737060546875,\n",
       " -0.0239410400390625,\n",
       " -0.00791168212890625,\n",
       " 0.06072998046875,\n",
       " 0.00841522216796875,\n",
       " 0.014068603515625,\n",
       " 0.03802490234375,\n",
       " 0.046722412109375,\n",
       " 0.0213165283203125,\n",
       " -0.01702880859375,\n",
       " -0.00449371337890625,\n",
       " -0.011322021484375,\n",
       " -0.02984619140625,\n",
       " -0.01305389404296875,\n",
       " -0.029876708984375,\n",
       " 0.00072479248046875,\n",
       " -0.0187530517578125,\n",
       " 0.0518798828125,\n",
       " -0.00241851806640625,\n",
       " 0.00720977783203125,\n",
       " -0.049896240234375,\n",
       " -0.035400390625,\n",
       " -0.025726318359375,\n",
       " -0.0462646484375,\n",
       " -0.01378631591796875,\n",
       " 0.03521728515625,\n",
       " -0.001850128173828125,\n",
       " 0.03240966796875,\n",
       " -0.07232666015625,\n",
       " 0.017608642578125,\n",
       " 0.058349609375,\n",
       " 0.08203125,\n",
       " 0.047027587890625,\n",
       " -0.032379150390625,\n",
       " 0.01053619384765625,\n",
       " -0.09100341796875,\n",
       " -0.01018524169921875,\n",
       " -0.0357666015625,\n",
       " -0.049163818359375,\n",
       " 0.036163330078125,\n",
       " 0.04278564453125,\n",
       " 0.0289154052734375,\n",
       " 0.033843994140625,\n",
       " -0.049346923828125,\n",
       " 0.01326751708984375,\n",
       " 0.0246734619140625,\n",
       " 0.003978729248046875,\n",
       " 0.0013866424560546875,\n",
       " 0.01065826416015625,\n",
       " -0.031494140625,\n",
       " 0.01143646240234375,\n",
       " 0.015411376953125,\n",
       " 0.021240234375,\n",
       " 0.00041103363037109375,\n",
       " 0.0126495361328125,\n",
       " 0.04278564453125,\n",
       " -0.00968170166015625,\n",
       " 0.0423583984375,\n",
       " -0.044281005859375,\n",
       " 0.0010509490966796875,\n",
       " 0.0141448974609375,\n",
       " -0.0113525390625,\n",
       " 0.00876617431640625,\n",
       " -0.0199737548828125,\n",
       " -0.023040771484375,\n",
       " -0.013885498046875,\n",
       " 0.0047760009765625,\n",
       " -0.0030918121337890625,\n",
       " -0.0184326171875,\n",
       " 0.016265869140625,\n",
       " -0.018402099609375,\n",
       " 0.02032470703125,\n",
       " 0.00311279296875,\n",
       " -0.038055419921875,\n",
       " -0.0245361328125,\n",
       " -0.0019626617431640625,\n",
       " -0.006793975830078125,\n",
       " 0.0445556640625,\n",
       " -0.050140380859375,\n",
       " -0.020965576171875,\n",
       " 0.0011234283447265625,\n",
       " -0.0399169921875,\n",
       " -0.003200531005859375,\n",
       " 0.00576019287109375,\n",
       " -0.0286865234375,\n",
       " 0.01165008544921875,\n",
       " -0.0045928955078125,\n",
       " -0.001678466796875,\n",
       " 0.0267181396484375,\n",
       " -0.0294189453125,\n",
       " -0.0081939697265625,\n",
       " -0.031158447265625,\n",
       " 0.071533203125,\n",
       " 0.057373046875,\n",
       " -0.02313232421875,\n",
       " -0.0369873046875,\n",
       " 0.039306640625,\n",
       " -0.00701141357421875,\n",
       " -0.0209808349609375,\n",
       " 0.011871337890625,\n",
       " 0.0109710693359375,\n",
       " -0.0021800994873046875,\n",
       " -0.048919677734375,\n",
       " 0.0265350341796875,\n",
       " -0.038055419921875,\n",
       " 0.0026073455810546875,\n",
       " 0.03546142578125,\n",
       " -0.004241943359375,\n",
       " 0.0130462646484375,\n",
       " 0.020751953125,\n",
       " -0.045745849609375,\n",
       " -0.050933837890625,\n",
       " 0.025543212890625,\n",
       " -0.0302734375,\n",
       " 0.03857421875,\n",
       " -0.06573486328125,\n",
       " -0.0021610260009765625,\n",
       " 0.036590576171875,\n",
       " 0.045257568359375,\n",
       " 0.023162841796875,\n",
       " 0.037322998046875,\n",
       " 0.05792236328125,\n",
       " 0.019500732421875,\n",
       " 0.041656494140625,\n",
       " -0.02227783203125,\n",
       " -0.03045654296875,\n",
       " 0.002468109130859375,\n",
       " -0.0155487060546875,\n",
       " -0.0311431884765625,\n",
       " -0.07989501953125,\n",
       " 0.058441162109375,\n",
       " 0.0227203369140625,\n",
       " -0.036163330078125,\n",
       " 0.0211029052734375,\n",
       " 0.0109405517578125,\n",
       " 0.0271453857421875,\n",
       " -0.0167388916015625,\n",
       " -0.01666259765625,\n",
       " 0.044708251953125,\n",
       " 0.0180816650390625,\n",
       " 0.07855224609375,\n",
       " 0.0299530029296875,\n",
       " 0.00025081634521484375,\n",
       " 0.0305633544921875,\n",
       " -0.00746917724609375,\n",
       " 0.034942626953125,\n",
       " 0.06231689453125,\n",
       " 0.0276031494140625,\n",
       " 0.01546478271484375,\n",
       " -0.00981903076171875,\n",
       " 0.020263671875,\n",
       " 0.027252197265625,\n",
       " 0.0157012939453125,\n",
       " 0.03924560546875,\n",
       " 0.016815185546875,\n",
       " -0.007266998291015625,\n",
       " -0.04644775390625,\n",
       " 0.00994110107421875,\n",
       " 0.0217132568359375,\n",
       " 0.017486572265625,\n",
       " -0.043182373046875,\n",
       " 0.0142822265625,\n",
       " 0.02252197265625,\n",
       " -0.05706787109375,\n",
       " -0.006626129150390625,\n",
       " -0.09820556640625,\n",
       " 0.016998291015625,\n",
       " 0.01763916015625,\n",
       " 0.0141143798828125,\n",
       " 0.005298614501953125,\n",
       " 0.00733184814453125,\n",
       " 0.0193939208984375,\n",
       " 0.018402099609375,\n",
       " -0.0447998046875,\n",
       " -0.032440185546875,\n",
       " -0.01479339599609375,\n",
       " 0.03277587890625,\n",
       " 0.0021381378173828125,\n",
       " -0.01151275634765625,\n",
       " 0.0362548828125,\n",
       " -0.032501220703125,\n",
       " -0.0025196075439453125,\n",
       " 0.00994873046875,\n",
       " 0.0214996337890625,\n",
       " -0.068115234375,\n",
       " -0.0249481201171875,\n",
       " -0.01247406005859375,\n",
       " -0.0013189315795898438,\n",
       " -0.0280303955078125,\n",
       " -0.042083740234375,\n",
       " 0.024017333984375,\n",
       " 0.036285400390625,\n",
       " -0.027008056640625,\n",
       " -0.0011396408081054688,\n",
       " -0.01788330078125,\n",
       " 0.0165863037109375,\n",
       " 0.030517578125,\n",
       " 0.0021190643310546875,\n",
       " -0.00809478759765625,\n",
       " 0.04388427734375,\n",
       " -0.0254058837890625,\n",
       " 0.007598876953125,\n",
       " -0.0092926025390625,\n",
       " 0.0235443115234375,\n",
       " -0.032073974609375,\n",
       " -0.0298614501953125,\n",
       " 0.07781982421875,\n",
       " 0.0301055908203125,\n",
       " -0.00817108154296875,\n",
       " 0.0390625,\n",
       " 0.0225830078125,\n",
       " -0.014190673828125,\n",
       " 0.06732177734375,\n",
       " 0.044921875,\n",
       " -0.007049560546875,\n",
       " 0.006626129150390625,\n",
       " 0.05126953125,\n",
       " 0.0161590576171875,\n",
       " 0.0241546630859375,\n",
       " 0.0103607177734375,\n",
       " 0.0044403076171875,\n",
       " 0.028564453125,\n",
       " 0.001232147216796875,\n",
       " -0.0085601806640625,\n",
       " 0.04150390625,\n",
       " 0.0013427734375,\n",
       " 0.0175628662109375,\n",
       " -0.043182373046875,\n",
       " -0.0036144256591796875,\n",
       " -0.0248565673828125,\n",
       " -0.0115203857421875,\n",
       " -0.033538818359375,\n",
       " -0.0074462890625,\n",
       " -0.017822265625,\n",
       " -0.02423095703125,\n",
       " -0.0022563934326171875,\n",
       " -0.0121917724609375,\n",
       " -0.01396942138671875,\n",
       " 0.021484375,\n",
       " -0.0178070068359375,\n",
       " 0.0095977783203125,\n",
       " -0.0307464599609375,\n",
       " 0.042266845703125,\n",
       " -0.01082611083984375,\n",
       " 0.0021076202392578125,\n",
       " -0.0190887451171875,\n",
       " 0.01305389404296875,\n",
       " -0.0616455078125,\n",
       " 0.046966552734375,\n",
       " 0.0102691650390625,\n",
       " 0.007556915283203125,\n",
       " 0.0017070770263671875,\n",
       " -0.00566864013671875,\n",
       " 0.0161285400390625,\n",
       " -0.026275634765625,\n",
       " 0.0138092041015625,\n",
       " 0.00635528564453125,\n",
       " 0.049041748046875,\n",
       " -0.04400634765625,\n",
       " -0.054351806640625,\n",
       " -0.0274200439453125,\n",
       " -0.036773681640625,\n",
       " 0.00041556358337402344,\n",
       " -0.0152740478515625,\n",
       " -0.044891357421875,\n",
       " -0.00690460205078125,\n",
       " -0.03680419921875,\n",
       " -0.03485107421875,\n",
       " -0.0283966064453125,\n",
       " -0.032684326171875,\n",
       " 0.032958984375,\n",
       " -0.0968017578125,\n",
       " 0.00399017333984375,\n",
       " 0.0400390625,\n",
       " -0.006946563720703125,\n",
       " -0.07501220703125,\n",
       " 0.01459503173828125,\n",
       " -0.0002799034118652344,\n",
       " -0.0051727294921875,\n",
       " 0.0037784576416015625,\n",
       " -0.0170135498046875,\n",
       " 0.004451751708984375,\n",
       " -0.0467529296875,\n",
       " 0.0261077880859375,\n",
       " 0.046600341796875,\n",
       " -0.0012645721435546875,\n",
       " 0.0156402587890625,\n",
       " -0.041961669921875,\n",
       " -0.03662109375,\n",
       " 0.0457763671875,\n",
       " -0.007049560546875,\n",
       " 0.082763671875,\n",
       " 0.039520263671875,\n",
       " 0.003162384033203125,\n",
       " 0.03021240234375,\n",
       " 0.0120849609375,\n",
       " -0.0302276611328125,\n",
       " -0.0076446533203125,\n",
       " 0.0295562744140625,\n",
       " 0.01788330078125,\n",
       " -0.02520751953125,\n",
       " 0.0180206298828125,\n",
       " 0.00982666015625,\n",
       " -0.012451171875,\n",
       " 0.0058441162109375,\n",
       " 0.005382537841796875,\n",
       " 0.0318603515625,\n",
       " 0.04815673828125,\n",
       " -0.01812744140625,\n",
       " -0.01338958740234375,\n",
       " -0.0175323486328125,\n",
       " 0.0115509033203125,\n",
       " -0.00748443603515625,\n",
       " 0.01380157470703125,\n",
       " 0.0133209228515625,\n",
       " -0.0281219482421875,\n",
       " 0.00982666015625,\n",
       " 0.009429931640625,\n",
       " 0.02191162109375,\n",
       " -0.0175018310546875,\n",
       " -0.00414276123046875,\n",
       " 0.0144500732421875,\n",
       " -0.00033020973205566406,\n",
       " 0.0056610107421875,\n",
       " 0.01519775390625,\n",
       " 0.03509521484375,\n",
       " -0.0010223388671875,\n",
       " -0.03790283203125,\n",
       " -0.011871337890625,\n",
       " 0.01666259765625,\n",
       " 0.06072998046875,\n",
       " -0.02386474609375,\n",
       " -0.00998687744140625,\n",
       " 0.038787841796875,\n",
       " 0.052093505859375,\n",
       " -0.04046630859375,\n",
       " 0.022979736328125,\n",
       " -0.07421875,\n",
       " -0.01294708251953125,\n",
       " 0.00910186767578125,\n",
       " -0.0002808570861816406,\n",
       " 0.0501708984375,\n",
       " 0.0039520263671875,\n",
       " 0.0131072998046875,\n",
       " 0.024169921875,\n",
       " 0.0131683349609375,\n",
       " -0.01302337646484375,\n",
       " 0.0494384765625,\n",
       " -0.0191802978515625,\n",
       " -0.0025615692138671875,\n",
       " 0.004367828369140625,\n",
       " 0.041595458984375,\n",
       " -0.026763916015625,\n",
       " 0.0645751953125,\n",
       " -0.0276031494140625,\n",
       " -0.05426025390625,\n",
       " -0.00942230224609375,\n",
       " 0.0037784576416015625,\n",
       " -0.06280517578125,\n",
       " -0.01096343994140625,\n",
       " -0.0244293212890625,\n",
       " -0.017333984375,\n",
       " 0.032135009765625,\n",
       " -0.01800537109375,\n",
       " 0.0132293701171875,\n",
       " -0.0249786376953125,\n",
       " -0.032501220703125,\n",
       " -0.0299530029296875,\n",
       " 0.0277557373046875,\n",
       " -0.034027099609375,\n",
       " 0.025634765625,\n",
       " -0.032501220703125,\n",
       " 0.00955963134765625,\n",
       " 0.00246429443359375,\n",
       " -0.006778717041015625,\n",
       " 0.058837890625,\n",
       " -0.01496124267578125,\n",
       " 0.018341064453125,\n",
       " 0.0362548828125,\n",
       " 0.017822265625,\n",
       " -0.019622802734375,\n",
       " -0.025543212890625,\n",
       " -0.005718231201171875,\n",
       " 0.03485107421875,\n",
       " 0.058349609375,\n",
       " 0.0494384765625,\n",
       " 0.060211181640625,\n",
       " -0.0178985595703125,\n",
       " 0.01271820068359375,\n",
       " -0.0102081298828125,\n",
       " -0.01279449462890625,\n",
       " 0.0196380615234375,\n",
       " 0.0008268356323242188,\n",
       " -0.004848480224609375,\n",
       " -0.0015802383422851562,\n",
       " -0.049041748046875,\n",
       " 0.002288818359375,\n",
       " 0.07073974609375,\n",
       " -0.05609130859375,\n",
       " -0.00505828857421875,\n",
       " -0.002162933349609375,\n",
       " 0.0289764404296875,\n",
       " -0.061614990234375,\n",
       " -0.018951416015625,\n",
       " 0.01087188720703125,\n",
       " 0.0297088623046875,\n",
       " -0.00437164306640625,\n",
       " -0.0265960693359375,\n",
       " 0.01242828369140625,\n",
       " 0.02459716796875,\n",
       " -0.04248046875,\n",
       " 0.033966064453125,\n",
       " -0.014678955078125,\n",
       " -0.041412353515625,\n",
       " -0.020660400390625,\n",
       " -0.0034275054931640625,\n",
       " -0.06890869140625,\n",
       " -0.07659912109375,\n",
       " -0.0115203857421875,\n",
       " 0.00959014892578125,\n",
       " -0.0160980224609375,\n",
       " 0.03375244140625,\n",
       " -0.01432037353515625,\n",
       " -0.0254364013671875,\n",
       " 0.0213623046875,\n",
       " -0.004596710205078125,\n",
       " -0.0122222900390625,\n",
       " 0.01849365234375,\n",
       " -0.13330078125,\n",
       " 0.0312347412109375,\n",
       " 0.04150390625,\n",
       " 0.058624267578125,\n",
       " -0.0243377685546875,\n",
       " 0.02117919921875,\n",
       " -0.06524658203125,\n",
       " -0.022003173828125,\n",
       " 0.0005121231079101562,\n",
       " 0.01242828369140625,\n",
       " 0.039093017578125,\n",
       " 0.0014390945434570312,\n",
       " -0.00290679931640625,\n",
       " -0.0239410400390625,\n",
       " -0.020416259765625,\n",
       " -0.0019435882568359375,\n",
       " 0.0638427734375,\n",
       " -0.01476287841796875,\n",
       " 0.01273345947265625,\n",
       " -0.029754638671875,\n",
       " 0.01137542724609375,\n",
       " 0.0207977294921875,\n",
       " 0.04473876953125,\n",
       " -0.00959014892578125,\n",
       " -0.032501220703125,\n",
       " -0.01873779296875,\n",
       " -0.018310546875,\n",
       " -0.019256591796875,\n",
       " 0.05706787109375,\n",
       " -0.0316162109375,\n",
       " -0.0116424560546875,\n",
       " 0.0167388916015625,\n",
       " 0.017425537109375,\n",
       " -0.044525146484375,\n",
       " 0.01812744140625,\n",
       " 0.01030731201171875,\n",
       " 0.03277587890625,\n",
       " -0.027923583984375,\n",
       " -0.032073974609375,\n",
       " -0.007904052734375,\n",
       " -0.0020961761474609375,\n",
       " 0.022247314453125,\n",
       " 0.0079498291015625,\n",
       " 0.0274810791015625,\n",
       " -0.0294189453125,\n",
       " 0.0194854736328125,\n",
       " 0.030548095703125,\n",
       " 0.0002593994140625,\n",
       " -0.0316162109375,\n",
       " 0.01071929931640625,\n",
       " 0.0146331787109375,\n",
       " -0.014739990234375,\n",
       " -0.0246124267578125,\n",
       " -0.011810302734375,\n",
       " -0.0034809112548828125,\n",
       " -0.0080413818359375,\n",
       " 0.015625,\n",
       " -0.00876617431640625,\n",
       " 0.0228424072265625,\n",
       " -0.0290679931640625,\n",
       " -0.01200103759765625,\n",
       " -0.021331787109375,\n",
       " 0.04290771484375,\n",
       " 0.002323150634765625,\n",
       " 0.0116119384765625,\n",
       " -0.006351470947265625,\n",
       " -0.043487548828125,\n",
       " -0.0550537109375,\n",
       " -0.00653076171875,\n",
       " 0.031494140625,\n",
       " 0.01448822021484375,\n",
       " -0.0086212158203125,\n",
       " 0.03564453125,\n",
       " 0.00826263427734375,\n",
       " 0.006439208984375,\n",
       " -0.01593017578125,\n",
       " 0.02752685546875,\n",
       " -0.007068634033203125,\n",
       " -0.004795074462890625,\n",
       " 0.0386962890625,\n",
       " 0.035552978515625,\n",
       " 0.03863525390625,\n",
       " -0.059478759765625,\n",
       " -0.0849609375,\n",
       " -0.038177490234375,\n",
       " -0.004863739013671875,\n",
       " 0.0244293212890625,\n",
       " 0.0175323486328125,\n",
       " 0.0234222412109375,\n",
       " 0.0251617431640625,\n",
       " 0.023101806640625,\n",
       " 0.02630615234375,\n",
       " -0.043060302734375,\n",
       " -0.03961181640625,\n",
       " -0.048828125,\n",
       " 0.005405426025390625,\n",
       " 0.0204010009765625,\n",
       " -0.021331787109375,\n",
       " 0.00539398193359375,\n",
       " 0.027374267578125,\n",
       " -0.0007472038269042969,\n",
       " -0.00911712646484375,\n",
       " 0.06329345703125,\n",
       " -0.012603759765625,\n",
       " 0.01522064208984375,\n",
       " 0.00926971435546875,\n",
       " -0.052032470703125,\n",
       " 0.032196044921875,\n",
       " 0.01471710205078125,\n",
       " -0.0176239013671875,\n",
       " 0.0085906982421875,\n",
       " -6.455183029174805e-05,\n",
       " 0.0733642578125,\n",
       " -0.0250701904296875,\n",
       " 0.031463623046875,\n",
       " -0.04937744140625,\n",
       " 0.03717041015625,\n",
       " -0.0172119140625,\n",
       " 0.03582763671875,\n",
       " 0.08856201171875,\n",
       " 0.01491546630859375,\n",
       " 0.052093505859375,\n",
       " -0.00038504600524902344,\n",
       " -0.040679931640625,\n",
       " -0.0267791748046875,\n",
       " -0.04107666015625,\n",
       " -0.049163818359375,\n",
       " -0.02581787109375,\n",
       " -0.0007958412170410156,\n",
       " -0.00873565673828125,\n",
       " -0.0073089599609375,\n",
       " -0.007781982421875,\n",
       " 0.040679931640625,\n",
       " -0.0008039474487304688,\n",
       " -0.00989532470703125,\n",
       " 0.01513671875,\n",
       " -0.020751953125,\n",
       " 0.0684814453125,\n",
       " -0.05108642578125,\n",
       " 0.00919342041015625,\n",
       " -0.00707244873046875,\n",
       " -0.0076141357421875,\n",
       " 0.032379150390625,\n",
       " 0.0030002593994140625,\n",
       " 0.0367431640625,\n",
       " 0.00714111328125,\n",
       " -0.0107269287109375,\n",
       " -0.02105712890625,\n",
       " 0.03228759765625,\n",
       " -0.005504608154296875,\n",
       " -0.0022487640380859375,\n",
       " 0.09613037109375,\n",
       " -0.052154541015625,\n",
       " 0.05224609375,\n",
       " -0.0191192626953125,\n",
       " 0.0218353271484375,\n",
       " 0.039337158203125,\n",
       " 0.004772186279296875,\n",
       " 0.0123443603515625,\n",
       " -0.002742767333984375,\n",
       " -0.0201873779296875,\n",
       " 0.062469482421875,\n",
       " 0.01195526123046875,\n",
       " -0.0233154296875,\n",
       " 7.134675979614258e-05,\n",
       " 0.002979278564453125,\n",
       " 0.03271484375,\n",
       " 0.0246734619140625,\n",
       " -0.04608154296875,\n",
       " 0.047637939453125,\n",
       " 0.00655364990234375,\n",
       " -0.026397705078125,\n",
       " -0.056396484375,\n",
       " 0.007160186767578125,\n",
       " -0.07476806640625,\n",
       " 0.037689208984375,\n",
       " 0.01239776611328125,\n",
       " -0.051849365234375,\n",
       " 0.033111572265625,\n",
       " -0.029937744140625,\n",
       " -0.0010967254638671875,\n",
       " 0.005908966064453125,\n",
       " -0.059478759765625,\n",
       " -0.05352783203125,\n",
       " 0.0249481201171875,\n",
       " -0.006259918212890625,\n",
       " 0.00789642333984375,\n",
       " 0.00595855712890625,\n",
       " -0.005649566650390625,\n",
       " 0.0077667236328125,\n",
       " 0.00548553466796875,\n",
       " 0.029327392578125,\n",
       " 0.0025882720947265625,\n",
       " -0.08892822265625,\n",
       " 0.032684326171875,\n",
       " 0.0089874267578125,\n",
       " -0.00875091552734375,\n",
       " 0.0033931732177734375,\n",
       " 0.026641845703125,\n",
       " -0.0345458984375,\n",
       " 0.05291748046875,\n",
       " -0.0189208984375,\n",
       " 0.037384033203125,\n",
       " 0.0305633544921875,\n",
       " 0.050323486328125,\n",
       " -0.0246124267578125,\n",
       " 0.0655517578125,\n",
       " -0.0097503662109375,\n",
       " 0.0265045166015625,\n",
       " 0.01000213623046875,\n",
       " -0.019622802734375,\n",
       " 0.1514892578125,\n",
       " 0.055450439453125,\n",
       " -0.0014438629150390625,\n",
       " -0.0870361328125,\n",
       " 0.010772705078125,\n",
       " -0.034515380859375,\n",
       " -0.00484466552734375,\n",
       " 0.00600433349609375,\n",
       " 0.0203094482421875,\n",
       " -0.0091400146484375,\n",
       " -0.0021152496337890625,\n",
       " 0.06353759765625,\n",
       " 0.0308990478515625,\n",
       " -0.03839111328125,\n",
       " 0.03240966796875,\n",
       " 0.0313720703125,\n",
       " -0.08251953125,\n",
       " -0.00556182861328125,\n",
       " 0.053436279296875,\n",
       " -0.005992889404296875,\n",
       " 0.029052734375,\n",
       " 0.0269927978515625,\n",
       " -0.0587158203125,\n",
       " -0.073974609375,\n",
       " 0.0128631591796875,\n",
       " -0.007373809814453125,\n",
       " 0.0173797607421875,\n",
       " -0.0102386474609375,\n",
       " -0.0187225341796875,\n",
       " -0.020660400390625,\n",
       " -0.040863037109375,\n",
       " 0.00832366943359375,\n",
       " -0.046905517578125,\n",
       " 0.0433349609375,\n",
       " 0.0604248046875,\n",
       " -0.021331787109375,\n",
       " -0.055694580078125,\n",
       " -0.0008091926574707031,\n",
       " -0.032073974609375,\n",
       " 0.024322509765625,\n",
       " 0.01326751708984375,\n",
       " 0.0010328292846679688,\n",
       " -0.0196380615234375,\n",
       " 0.029083251953125,\n",
       " -0.039764404296875,\n",
       " -0.01055145263671875,\n",
       " -0.0176239013671875,\n",
       " -0.024383544921875,\n",
       " -0.04144287109375,\n",
       " -0.0005016326904296875,\n",
       " -0.034027099609375,\n",
       " 0.004535675048828125,\n",
       " 0.01505279541015625,\n",
       " 0.006870269775390625,\n",
       " -0.0210418701171875,\n",
       " 0.0650634765625,\n",
       " 0.02325439453125,\n",
       " -0.01233673095703125,\n",
       " -0.0234222412109375,\n",
       " -0.017974853515625,\n",
       " 0.028289794921875,\n",
       " -0.040802001953125,\n",
       " 0.03778076171875,\n",
       " -0.0252227783203125,\n",
       " -0.017974853515625,\n",
       " 0.0518798828125,\n",
       " -0.03424072265625,\n",
       " 0.0173797607421875,\n",
       " 0.0234527587890625,\n",
       " -0.034332275390625,\n",
       " -0.07879638671875,\n",
       " 0.036102294921875,\n",
       " 0.005870819091796875,\n",
       " -0.0426025390625,\n",
       " 0.015533447265625,\n",
       " 0.03564453125,\n",
       " 0.037567138671875,\n",
       " 0.046234130859375,\n",
       " -0.0120391845703125,\n",
       " -0.0220184326171875,\n",
       " 0.0164794921875,\n",
       " -0.0168304443359375,\n",
       " -0.0113372802734375,\n",
       " -0.01509857177734375,\n",
       " -0.0318603515625,\n",
       " 0.04742431640625,\n",
       " 0.055328369140625,\n",
       " 0.0193023681640625,\n",
       " 0.028900146484375,\n",
       " -0.0185394287109375,\n",
       " -0.05584716796875,\n",
       " 0.042205810546875,\n",
       " 0.0148468017578125,\n",
       " -0.00290679931640625,\n",
       " 0.00537109375,\n",
       " 0.0276031494140625,\n",
       " -0.01232147216796875,\n",
       " -0.01922607421875,\n",
       " 0.0260772705078125,\n",
       " -0.01491546630859375,\n",
       " 0.0172119140625,\n",
       " -0.002925872802734375,\n",
       " 0.008819580078125,\n",
       " 0.03900146484375,\n",
       " 0.0036468505859375,\n",
       " 0.03179931640625,\n",
       " 0.058349609375,\n",
       " -0.057586669921875,\n",
       " -0.0261993408203125,\n",
       " -0.00011295080184936523,\n",
       " -0.039520263671875,\n",
       " -0.02264404296875,\n",
       " -0.0067138671875,\n",
       " 0.010223388671875,\n",
       " -0.022857666015625,\n",
       " 0.0291748046875,\n",
       " -0.01465606689453125,\n",
       " -0.01214599609375,\n",
       " -0.022247314453125,\n",
       " -0.0201416015625,\n",
       " 0.0241851806640625,\n",
       " 0.0211334228515625,\n",
       " 0.0110015869140625,\n",
       " -0.0248870849609375,\n",
       " -0.0229034423828125,\n",
       " -0.031219482421875,\n",
       " -0.0224609375,\n",
       " 0.007175445556640625,\n",
       " -0.037078857421875,\n",
       " 0.01169586181640625,\n",
       " 0.01070404052734375,\n",
       " -0.006633758544921875,\n",
       " 0.0259552001953125,\n",
       " 0.002162933349609375,\n",
       " -0.005413055419921875,\n",
       " 0.0065765380859375,\n",
       " -0.0210113525390625,\n",
       " 0.03753662109375,\n",
       " -0.0233612060546875,\n",
       " -0.053924560546875,\n",
       " 0.0169525146484375,\n",
       " -0.0318603515625,\n",
       " 0.0180816650390625,\n",
       " 0.0180816650390625,\n",
       " -0.0118408203125,\n",
       " 0.02349853515625,\n",
       " 0.007091522216796875,\n",
       " -0.0187225341796875,\n",
       " 0.019927978515625,\n",
       " 0.01678466796875,\n",
       " -0.032501220703125,\n",
       " -0.0236358642578125,\n",
       " -0.0066986083984375,\n",
       " 0.006565093994140625,\n",
       " 0.05401611328125,\n",
       " -0.004886627197265625,\n",
       " -0.022247314453125,\n",
       " -0.00885009765625,\n",
       " 0.01151275634765625,\n",
       " -0.044036865234375,\n",
       " 0.033935546875,\n",
       " -0.0260162353515625,\n",
       " -0.00785064697265625,\n",
       " 0.041046142578125,\n",
       " 0.0118560791015625,\n",
       " 0.01216888427734375,\n",
       " 0.019439697265625,\n",
       " -0.017333984375,\n",
       " -0.0024585723876953125,\n",
       " -0.00659942626953125,\n",
       " -0.01024627685546875,\n",
       " 0.05999755859375,\n",
       " 0.00699615478515625,\n",
       " 0.004711151123046875,\n",
       " -0.08062744140625,\n",
       " 0.01548004150390625,\n",
       " 0.024322509765625,\n",
       " 0.001079559326171875,\n",
       " 0.015045166015625,\n",
       " -0.0223236083984375,\n",
       " 0.0175323486328125,\n",
       " 0.0450439453125,\n",
       " -0.0182342529296875,\n",
       " 0.046234130859375,\n",
       " 0.0112457275390625,\n",
       " 0.03057861328125,\n",
       " -0.00567626953125,\n",
       " -0.01277923583984375,\n",
       " -0.00850677490234375,\n",
       " -0.041595458984375,\n",
       " 0.024688720703125,\n",
       " 0.0240020751953125,\n",
       " -0.0116119384765625,\n",
       " 0.022613525390625,\n",
       " -0.0224456787109375,\n",
       " -0.0067291259765625,\n",
       " -0.01068878173828125,\n",
       " 0.017303466796875,\n",
       " -0.0247802734375,\n",
       " -0.01331329345703125,\n",
       " 0.0210723876953125,\n",
       " -0.00922393798828125,\n",
       " 0.0062713623046875,\n",
       " 0.0005259513854980469,\n",
       " 0.0144195556640625,\n",
       " 0.02252197265625,\n",
       " 0.050323486328125,\n",
       " 0.005855560302734375,\n",
       " 0.0027523040771484375,\n",
       " -0.0114288330078125,\n",
       " 0.0259246826171875,\n",
       " -0.04364013671875,\n",
       " -0.03961181640625,\n",
       " 0.024261474609375,\n",
       " 0.036865234375,\n",
       " 0.050048828125,\n",
       " -0.058807373046875,\n",
       " 0.035125732421875,\n",
       " -0.00994110107421875,\n",
       " -0.043609619140625,\n",
       " -0.01427459716796875,\n",
       " 0.0772705078125,\n",
       " -0.0193939208984375,\n",
       " -0.0246734619140625,\n",
       " 0.042266845703125,\n",
       " 0.0041046142578125,\n",
       " 0.052154541015625,\n",
       " -0.003177642822265625,\n",
       " -0.01190185546875,\n",
       " -0.015869140625,\n",
       " 0.00748443603515625,\n",
       " 0.005687713623046875,\n",
       " -0.055450439453125,\n",
       " 0.0211334228515625,\n",
       " -0.0006909370422363281,\n",
       " -0.01049041748046875,\n",
       " -0.0181427001953125,\n",
       " -0.0102691650390625,\n",
       " -0.006931304931640625,\n",
       " -0.037689208984375,\n",
       " -0.049652099609375,\n",
       " -0.018524169921875,\n",
       " 0.0010576248168945312,\n",
       " 0.0168609619140625,\n",
       " -0.0067901611328125,\n",
       " -0.0171661376953125,\n",
       " -0.0012149810791015625,\n",
       " 0.034454345703125,\n",
       " -0.0035381317138671875,\n",
       " 0.00921630859375,\n",
       " 0.0093231201171875,\n",
       " -0.0053253173828125,\n",
       " -0.0203704833984375,\n",
       " -0.0084991455078125,\n",
       " 0.06793212890625,\n",
       " 0.001251220703125,\n",
       " -0.05657958984375,\n",
       " -0.047515869140625,\n",
       " -0.010284423828125,\n",
       " 0.01214599609375,\n",
       " 0.00298309326171875,\n",
       " 0.0155487060546875,\n",
       " -0.071533203125,\n",
       " -0.01207733154296875,\n",
       " -0.040008544921875,\n",
       " 0.04278564453125,\n",
       " 0.04730224609375,\n",
       " 0.01387786865234375,\n",
       " 0.031463623046875,\n",
       " 0.00888824462890625,\n",
       " -0.0045318603515625,\n",
       " -0.0297393798828125,\n",
       " 0.0202789306640625,\n",
       " -0.0301055908203125,\n",
       " 0.017333984375,\n",
       " -0.01084136962890625,\n",
       " -0.013671875,\n",
       " -0.01357269287109375,\n",
       " -0.0196380615234375,\n",
       " 0.043487548828125,\n",
       " 0.02734375,\n",
       " 0.0241241455078125,\n",
       " -0.01284027099609375,\n",
       " 0.00980377197265625,\n",
       " 0.015625,\n",
       " 0.006984710693359375,\n",
       " 0.025238037109375,\n",
       " 0.0293121337890625,\n",
       " 0.0054931640625,\n",
       " 0.0088043212890625,\n",
       " -0.0254669189453125,\n",
       " 0.024200439453125,\n",
       " -0.01358795166015625,\n",
       " 0.0202789306640625,\n",
       " -0.01113128662109375,\n",
       " -0.042877197265625,\n",
       " -0.0021305084228515625,\n",
       " -0.0209197998046875,\n",
       " -0.03851318359375,\n",
       " -0.00994110107421875,\n",
       " 0.0171051025390625,\n",
       " -0.01389312744140625,\n",
       " 0.0197601318359375,\n",
       " 0.0040130615234375,\n",
       " 0.06842041015625,\n",
       " -0.0328369140625,\n",
       " -0.033050537109375,\n",
       " -0.006175994873046875,\n",
       " 0.01165008544921875,\n",
       " -0.050323486328125,\n",
       " -0.0193023681640625,\n",
       " 0.0010814666748046875,\n",
       " 0.055206298828125,\n",
       " 0.0009455680847167969,\n",
       " 0.036346435546875,\n",
       " 0.0248260498046875,\n",
       " 0.04150390625,\n",
       " -0.01554107666015625,\n",
       " 0.01045989990234375,\n",
       " -0.007122039794921875,\n",
       " -0.0235137939453125,\n",
       " 0.043670654296875,\n",
       " -0.00475311279296875,\n",
       " 0.00954437255859375,\n",
       " 0.0572509765625,\n",
       " 0.029693603515625,\n",
       " 0.032012939453125,\n",
       " -0.0386962890625,\n",
       " -0.004596710205078125,\n",
       " -0.0016736984252929688,\n",
       " 0.01207733154296875,\n",
       " -0.0265960693359375,\n",
       " 0.06951904296875,\n",
       " 0.03131103515625,\n",
       " 0.001644134521484375,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = embeddings.embed_query(\"retrieval augmented generation\")\n",
    "test_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "764aa970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273ff5e",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Now that we have split our data and initialized the embeddings, we can start indexing it. There are a lot of different implementations of indexes, you can take a lot at available options in [Vector stores](https://docs.langchain.com/oss/python/integrations/vectorstores). One of the popular choices is [Qdrant](https://docs.langchain.com/oss/python/integrations/vectorstores/qdrant) that provides a simple data management and can be deployed both locally, on a remote machine, and on the cloud.\n",
    "\n",
    "Qdrant support persisting your vector storage, i.e. storing it on the working machine, but for simplicity, we will use it in the in-memory mode, so that the storage exists only as long as the notebook does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "954285bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee0509",
   "metadata": {},
   "source": [
    "First things first, we need to create a _client_ -- a Qdrant instance that will be the entrypoint for all the actions we do with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf05f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client = QdrantClient(\":memory:\")    # in-memory Qdrant client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6f625",
   "metadata": {},
   "source": [
    "Then, as we use an in-memory client that does not store the index between the notebook sessions, we need to initialize a _collection_. Alternatively, if we were persisting the data, we would perform a check if the collection exists and then either create or load it.\n",
    "\n",
    "For Qdrant to initialize the structure of the index correctly, we need to provide the dimentionality of the embedding we will be using as well as teh distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d020dfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = \"1211_1311\"\n",
    "\n",
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    # embedding params here\n",
    "    vectors_config=VectorParams(\n",
    "        size=len(test_embedding),   # is there a better way?\n",
    "        distance=Distance.COSINE    # cosine distance\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1682d",
   "metadata": {},
   "source": [
    "Finally, we use a LangChain wrapper to connect to the index to unify the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01711510",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qd_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c442af",
   "metadata": {},
   "source": [
    "Now we are ready to add our chunks to the vector storage. As we will be adding the chunks, the index will take care about converting our passages into embeddings.\n",
    "\n",
    "In order to be able to delete / modify the chunks afterwards, we assign them with unique ids that we generate dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c47e67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ff1d44e6-926e-431c-a9c7-c1e947cd5db8',\n",
       " '3ab796b6-7dcb-4e5c-82b3-bfed447a0425',\n",
       " '6e40974f-f56d-4c6d-96d0-d06b440130a0',\n",
       " 'b481f6fd-7019-4296-813d-0f3c0e9a3677',\n",
       " '2728c450-277b-40dd-98bf-5293e4396410',\n",
       " 'c188c5c6-3169-499a-876a-a3656f58fde5',\n",
       " 'cea2d90a-d1a9-4121-96e1-4e1c5a674709',\n",
       " '79dd8a9a-e52e-4b26-ac61-973a8edee906',\n",
       " '5c5d3038-1d3d-47a6-a428-ff7044c6abcf',\n",
       " 'f4764185-e826-44bb-aa9d-5873c5c2bffb',\n",
       " 'ff7e8c21-195c-4bdf-b33b-c2d3fb28ddc2',\n",
       " '5a516477-ec44-4311-9d7d-419c252c6522',\n",
       " '594939a9-689c-40a1-816b-d59cff168111',\n",
       " 'e1ff1bc3-9ad7-4d69-a26f-026df1a7e236',\n",
       " '8cc87bbd-ed31-4aa1-a65c-59691283ed1d',\n",
       " '3c2605f0-d665-44dd-85b7-6082bf60f671',\n",
       " 'af66bb59-022c-4300-b426-d02d927b9f9e',\n",
       " '4960a607-11ec-440a-b375-f36409e2a226',\n",
       " '3198ffef-3105-4947-8dcd-6be71d31fee9',\n",
       " '63fecefb-e60c-4ce0-8cd5-e19479950cce',\n",
       " 'a7ddddff-5585-47f2-953f-806db2b80096',\n",
       " '6258ee0a-8621-4dd6-bd1f-6c7691c9d995',\n",
       " 'c5423f79-58cf-440b-af90-9f65ad275853',\n",
       " 'ca818bba-8553-43c6-9746-42d920ba99be',\n",
       " 'f40f4342-f1c1-47fd-8cac-cde3e7e064b8',\n",
       " '3e9b31df-df9f-45bd-afeb-ab31d99e644f',\n",
       " '498a971a-df5f-4f4a-a9b8-82c705727aaf',\n",
       " 'e3cdc143-2aba-4373-9916-4e83f3c69957',\n",
       " 'dad68b6a-d60a-497e-96cc-2cbc0120966d',\n",
       " 'c1e6fc54-7499-4db0-8513-c36980066b0a',\n",
       " '89e1dd9f-0fb6-49bc-8c37-2b14d30b83e3',\n",
       " '995f1968-a09d-4afc-b348-fc9605aa3ff3',\n",
       " '219b4ae3-5231-4995-82b8-c61998085dd3',\n",
       " 'c413f7b9-fed7-4b86-9dda-c3baedc5f671',\n",
       " '7d0e058e-cde2-4c96-8763-8aae34aa1128',\n",
       " '822cd0bf-8f23-44cb-bbf6-e12ae3c954a8',\n",
       " '97030954-177d-4702-b817-f14b289a237a',\n",
       " 'c3afbcc9-7ff9-4a65-8dff-4db599371653',\n",
       " '85df2a46-3367-4448-8a81-eb49e4fa9c9f',\n",
       " '5bcb65de-eaf2-4a2b-be3f-69808d3c2460',\n",
       " '67959020-e7c6-400b-af05-3b2dfece8c8a',\n",
       " 'd7fb8fd9-08f7-49b9-ad56-a221a6d4793a',\n",
       " '1ec56007-eac7-40c6-8be4-1dac83de84c9',\n",
       " 'e4594757-0832-40a4-8e6f-7e86367c6aa0',\n",
       " 'd6ece085-c653-4da5-b20b-575f37550fab',\n",
       " '94ddaca5-bbbf-4a77-a27c-573d0eb3db1e',\n",
       " '590518da-bd0f-4858-9000-a3fc491ee2f6',\n",
       " '904a37c5-327e-41d0-baa5-805ab42bd37c',\n",
       " '95ae53f5-32ed-4b8f-b5eb-e8048a534e8e',\n",
       " 'ed6439f9-d5c1-43f6-92e5-63b3ed584bbd',\n",
       " '85dd0e29-0937-41f0-bee6-d125146cde27']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "vector_store.add_documents(\n",
    "    chunks,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75faf3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 50, '_id': '85dd0e29-0937-41f0-bee6-d125146cde27', '_collection_name': '1211_1311'}, page_content=\"Reading : see session 01.07 and session 03.07\\n\\n## 24.07. Debate : Role of AI in Recent Years + Wrap-up\\n\\nOn material of session 17.07\\n\\nThe course will be concluded by the final debates, after which a short Q&amp;A session will be held. Copyright ¬© 2025, Maksim Shmalts Made with Sphinx and @pradyunsg's Furo\\n\\nDebate topics:\\n\\n- LLM Behavior: Evidence of Awareness or Illusion of Understanding ?\\n- Should We Limit the Usage of AI ?\\n\\nReading : see session 17.07\")]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids([ids[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b718dbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.delete([ids[-1]])\n",
    "vector_store.get_by_ids([ids[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32db3ae",
   "metadata": {},
   "source": [
    "The index provides the necessary functionality for the query-based retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f03c67e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 15, '_id': '3c2605f0-d665-44dd-85b7-6082bf60f671', '_collection_name': '1211_1311'}, page_content=\"## Key points :\\n\\n- General knowledge vs context\\n- Knowledge indexing, retrieval &amp; ranking\\n- Retrieval tools\\n- Agentic RAG\\n\\n## Core Reading :\\n\\n- Retrieval Augmented Generation or Long-Context LLMs ? A Comprehensive Study and Hybrid Approach (pages 1-7), Google DeepMind &amp; University of Michigan\\n- A Survey on Retrieval-Augmented Text Generation for Large Language Models (sections 1-7), York University\\n\\n## Additional Reading :\\n\\n- Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National Chengchi University &amp; Academia Sinica\"),\n",
       " Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 16, '_id': 'af66bb59-022c-4300-b426-d02d927b9f9e', '_collection_name': '1211_1311'}, page_content=\"## Additional Reading :\\n\\n- Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks, National Chengchi University &amp; Academia Sinica\\n- Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, University of Washington, Allen Institute for AI &amp; IBM Research AI\\n- Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity, Korea Advanced Institute of Science and Technology\\n- Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese Academy of Sciences\\n- Querying Databases with Function Calling, Weaviate, Contextual AI &amp; Morningstar\\n\\n## 15.05. Lab : RAG Chatbot\"),\n",
       " Document(metadata={'source': './topic_overview.pdf', 'chunk_n': 17, '_id': '4960a607-11ec-440a-b375-f36409e2a226', '_collection_name': '1211_1311'}, page_content=\"- Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models, Chinese Academy of Sciences\\n- Querying Databases with Function Calling, Weaviate, Contextual AI &amp; Morningstar\\n\\n## 15.05. Lab : RAG Chatbot\\n\\nOn material of session 13.05\\n\\nIn this lab, we'll expand the functionality of the chatbot built at the last lab to connect it to user-specific information.\\n\\n## Reading :\\n\\n- How to load PDFs, LangChain\\n- Text splitters, LangChain\\n- Embedding models, LangChain\\n- Vector stores, LangChain\\n- Retrievers, LangChain\\n- Retrieval augmented generation (RAG), LangChain\\n- LangGraph Quickstart: Build a Basic Chatbot (part 2), LangGraph\\n- Agentic RAG, LangGraph\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"retrieval augmented generation\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfa4d4",
   "metadata": {},
   "source": [
    "<h2 id=\"prompts\">2. Prompting al LangChain üìù</h2>\n",
    "\n",
    "When you build more complex algorithms, just passing the human query directly might be not enough. Sometimes, you need to give more specific instructions, pre- and append additional stuff to the messages; for example, in (classic) RAG, the retrieved chunks are injected as a context in a dedicated placeholder.\n",
    "\n",
    "To handle the static instructions more efficiently, you can use `ChatPromptTemplate`. The key idea is simple: in a `ChatPromptTemplate`, you write all the static fragments in plain text and then use placeholders to mark the places where some variable parts will be added. Then, when you receive an input, LangChain fills the placeholders and you receive the desired version of the message with all the placeholders filled automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72b197c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab23639",
   "metadata": {},
   "source": [
    "Before moving to RAG, let us consider a different case to demonstrate the standalone effect of the prompting: CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "798a6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_template_str = \"\"\"\\\n",
    "The user is asking a question. Please answer it using step-by-step reasoning. \\\n",
    "On each reasoning step, assess whether this reasoning step is good or not, \\\n",
    "on a scale from 1 to 10.\n",
    "\n",
    "The user question is:\n",
    "\n",
    "============\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "input_template = ChatPromptTemplate.from_template(input_template_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac74461",
   "metadata": {},
   "source": [
    "Now, even though the user will provide a simple query as usual, the LLM will receive all the additional instructions you wrote. A `ChatPromptTemplate` uses **keys** to fill the placeholders so you should pass it a corresponding `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8303f4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = input_template.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70f239ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\n",
      "\n",
      "The user question is:\n",
      "\n",
      "============\n",
      "What is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example.messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783b0c7",
   "metadata": {},
   "source": [
    "You can also make higher level prompt templates -- that is, with placeholders not for a single message, but for a sequence of messages. To do so, you need to nest `ChatPromptTemplate`s for separate messages and use the `MessagesPlaceholder` for sequences. This approach gives you a universal way to fill the placeholders, be it a separate fragment of a certain message or a whole sequence of messages: all you need is to be careful with the keys, and LangChain will take care of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52f57dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = SystemMessagePromptTemplate.from_template(\"Answer in the following language: {language}.\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_template,\n",
    "        MessagesPlaceholder(variable_name=\"messages\")   # here, you add an entire sequence of messages\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51545476",
   "metadata": {},
   "source": [
    "Alternative: pass separate messages as pairs of raw strings where the first string describes the role (`\"system\"`, `\"user\"`, `\"assistant\"`) and the second -- the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bf37a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer in the following language: {language}.\"),    # here, you modify a fragment of the system message\n",
    "        MessagesPlaceholder(variable_name=\"messages\")   # here, you add an entire sequence of messages\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a72344f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.to_messages() * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bec58bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Answer in the following language: Spanish.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\\n\\nThe user question is:\\n\\n============\\nWhat is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = prompt_template.invoke({\n",
    "    \"language\": \"Spanish\",\n",
    "    \"messages\": example.to_messages() * 3\n",
    "})\n",
    "\n",
    "messages.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8de5a",
   "metadata": {},
   "source": [
    "We can now incorporate this logic into our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b30656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTChatbot(Chatbot):\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm)\n",
    "        # add templates\n",
    "        self.input_template = input_template\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def _input_node(self, state: SimpleState) -> dict:\n",
    "        user_query = input(\"Your message: \")\n",
    "        if user_query != \"quit\":\n",
    "            # invoke the template here\n",
    "            human_message = self.input_template.invoke(\n",
    "                {\n",
    "                    \"input\": user_query\n",
    "                }\n",
    "            ).to_messages()\n",
    "        else:\n",
    "            human_message = HumanMessage(content=user_query)\n",
    "        n_turns = state[\"n_turns\"]\n",
    "        # add the input to the messages\n",
    "        return {\n",
    "            \"messages\": human_message\n",
    "        }\n",
    "    \n",
    "    def _respond_node(self, state: SimpleState) -> dict:\n",
    "        # invoke the template here;\n",
    "        # since the state is already a dictionary, we can just pass it as is\n",
    "        prompt = self.prompt_template.invoke(state)\n",
    "        n_turns = state[\"n_turns\"]\n",
    "        response = self.llm.invoke(prompt)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response,\n",
    "            \"n_turns\": n_turns + 1\n",
    "        }\n",
    "\n",
    "    def run(self, language=None):\n",
    "        # since the system message is now part of the prompt template,\n",
    "        # we don't need to add it to the input\n",
    "        input = {\n",
    "            \"messages\": [],\n",
    "            \"n_turns\": 0,\n",
    "            \"language\": language or \"English\"    # new\n",
    "        }\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"values\"):\n",
    "            if event[\"messages\"]:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10586e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_chatbot = CoTChatbot(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e60ee066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The user is asking a question. Please answer it using step-by-step reasoning. On each reasoning step, assess whether this reasoning step is good or not, on a scale from 1 to 10.\n",
      "\n",
      "The user question is:\n",
      "\n",
      "============\n",
      "What is the distance between the Earth and the Moon divided by the number of the planets in the Solar system\n",
      "\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Schritt 1: Bestimmen des Abstands zwischen der Erde und dem Mond.\n",
      "Der Abstand zwischen der Erde und dem Mond betr√§gt im Durchschnitt etwa 384.400 Kilometer. Diese Information ist allgemein bekannt und kann als zuverl√§ssig betrachtet werden. Bewertung: 9/10 (gut, aber es k√∂nnte kleine Abweichungen geben, da der Mond nicht immer den gleichen Abstand zur Erde hat).\n",
      "\n",
      "Schritt 2: Bestimmen der Anzahl der Planeten in unserem Sonnensystem.\n",
      "Unser Sonnensystem besteht aus acht Planeten: Merkur, Venus, Erde, Mars, Jupiter, Saturn, Uranus und Neptun. Diese Information ist wissenschaftlich etabliert und kann als sehr zuverl√§ssig betrachtet werden. Bewertung: 10/10 (sehr gut, da die Anzahl der Planeten in unserem Sonnensystem allgemein anerkannt ist).\n",
      "\n",
      "Schritt 3: Berechnen des Ergebnisses durch Division des Abstands zwischen der Erde und dem Mond durch die Anzahl der Planeten.\n",
      "Um das Ergebnis zu erhalten, teilen wir den Abstand zwischen der Erde und dem Mond (384.400 Kilometer) durch die Anzahl der Planeten (8). Ergebnis = 384.400 km / 8 = 48.050 km. Bewertung: 9/10 (gut, da die mathematische Operation korrekt durchgef√ºhrt wurde, aber die Genauigkeit des Ergebnisses h√§ngt von der Genauigkeit der verwendeten Werte ab).\n",
      "\n",
      "Zusammenfassung: Der Abstand zwischen der Erde und dem Mond geteilt durch die Anzahl der Planeten in unserem Sonnensystem betr√§gt etwa 48.050 Kilometer. Die Bewertung der Gesamtbewertung dieser Schritte liegt bei durchschnittlich 9,3/10, was auf eine solide und zuverl√§ssige Argumentationskette hinweist.\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "quit\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cot_chatbot.run(\"German\")\n",
    "# What is the distance between the Earth and the Moon divided by the number of the planets in the Solar system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e75f5f",
   "metadata": {},
   "source": [
    "<h2 id=\"rag\">3. Basic RAG üíâ</h2>\n",
    "\n",
    "Now let's apply our new prompting tool to the basic RAG workflow. For that, we will just retrieve _k_ most relevant documents and them insert them into the prompt as a part of the context.\n",
    "\n",
    "We will combine the skills we have obtained so far to build a LangGraph agent that receives an input, checks if the user wants to quit, then do the retrieval and generate a context-aware response if not. We will build on the basic version of our first chatbot; to add the RAG functionality, we need to add a retrieval node and modify the generation prompt to inject the retrieved documents.\n",
    "\n",
    "LangGraph provides a pre-built tool to conveniently create a retriever tool. The retriever tool can be connected to the LLM; in this case, the LLM decides itself whether to use it or not. But for our RAG labs, we separate the tool from the LLM to concentrate on the retrieval and not the agentic aspect of the system (which will be covered in the next week's lab). We also won't generate queries for the retriever for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ad131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role: restrict it from the parametric knowledge\n",
    "rag_system_prompt = \"\"\"\\\n",
    "You are an assistant that has access to a knowledge base. \\\n",
    "You should use the knowledge base to answer the user's questions \\\n",
    "unless you can reliably answer them from your own knowledge.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# this will add the context to the input\n",
    "context_injection_prompt = \"\"\"\\\n",
    "The user is asking a question. \\\n",
    "Answer using the following knowledge from the knowledge base:\n",
    "\n",
    "\n",
    "==========================\n",
    "{context}\n",
    "==========================\n",
    "\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# finally, gather the system message, the previous messages,\n",
    "# and the input with the context\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),   # system message\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # previous messages\n",
    "        (\"user\", context_injection_prompt)  # user message\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRAGChatbot(Chatbot):\n",
    "\n",
    "    _graph_path = \"./graph_basic_rag.png\"\n",
    "    \n",
    "    def __init__(self, llm, k=5):\n",
    "        super().__init__(llm)\n",
    "        self.rag_prompt = rag_prompt\n",
    "        self.retriever = vector_store.as_retriever(search_kwargs={\"k\": k})    # retrieve 5 documents\n",
    "        self.retriever_tool = create_retriever_tool(    # and this is the tool\n",
    "            self.retriever,\n",
    "            \"retrieve_internal_data\",  # name\n",
    "            \"Search relevant information in internal documents.\",   # description\n",
    "        )\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(SimpleState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"retrieve\", self._retrieve_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        # basic rag: no planning, just always retrieve\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"retrieve\", True: END})\n",
    "        self._graph_builder.add_edge(\"retrieve\", \"respond\")\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "    \n",
    "    def _retrieve_node(self, state: SimpleState) -> dict:\n",
    "        # retrieve the context\n",
    "        user_query = state[\"messages\"][-1].content  # use the last message as the query\n",
    "        context = self.retriever_tool.invoke({\"query\": user_query})\n",
    "        # add the context to the messages\n",
    "        return {\n",
    "            \"messages\": AIMessage(content=context)\n",
    "        }\n",
    "    \n",
    "    def _respond_node(self, state: SimpleState) -> dict:\n",
    "        # the workflow is designed so that the context is always the last message\n",
    "        # and the user query is the second to last message\n",
    "        context = state[\"messages\"].pop(-1).content # don't want the context in the messages\n",
    "        user_query = state[\"messages\"][-1].content\n",
    "        prompt = self.rag_prompt.invoke(\n",
    "            {\n",
    "                \"messages\": state[\"messages\"],  # this goes to the message placeholder\n",
    "                \"context\": context,  # this goes to the user message\n",
    "                \"input\": user_query    # this goes to the user message\n",
    "            }\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        input = {\"messages\": []}\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"values\"):\n",
    "            if event[\"messages\"]:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afded1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag_chatbot = BasicRAGChatbot(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469265f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag_chatbot.run()\n",
    "# What sessions do I have about virtual assistants?\n",
    "# What is the reading for the third one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e7102",
   "metadata": {},
   "source": [
    "As you can see, it already works pretty well, but as the retrieval goes by the user query directly, the previous context of the conversation is not considered by the _retriever_. To handle that, let's add a node that would reformulate the query taking in consideration the previous interactions.\n",
    "\n",
    "For that, we need an additional prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_query_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # previous messages\n",
    "        (\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Complete the following user query in the last message to a full question based on the previous messages. \"\n",
    "            \"Return only the reformulated user query, without any other text. Do not expand the query \"\n",
    "\t\t\t\"or change its meaning and only fill in the missing information in the last question. \"\n",
    "\t\t\t\"\\n\\nUser query:\\n{query}\\n\\nReformulated query:\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPlusRAGChatbot(BasicRAGChatbot):\n",
    "\n",
    "    _graph_path = \"./graph_basic_plus_rag.png\"\n",
    "\n",
    "    def __init__(self, llm, k=5):\n",
    "        super().__init__(llm, k)\n",
    "        self.complete_query_prompt = complete_query_prompt\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(SimpleState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"complete_query\", self._complete_query_node)\n",
    "        self._graph_builder.add_node(\"retrieve\", self._retrieve_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        # basic rag: no planning, just always retrieve\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"complete_query\", True: END})\n",
    "        self._graph_builder.add_edge(\"complete_query\", \"retrieve\")\n",
    "        self._graph_builder.add_edge(\"retrieve\", \"respond\")\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "\n",
    "    def _complete_query_node(self, state: SimpleState) -> dict:\n",
    "        # since we use the generated query instead of the user query,\n",
    "        # we want to remove the original user query from the messages\n",
    "        # and replace it with the generated one, having converted it to a HumanMessage\n",
    "        if len(state[\"messages\"]) == 1:\n",
    "            return {}  # no need to reformulate the first query\n",
    "        user_query = state[\"messages\"].pop(-1)\n",
    "        prompt = self.complete_query_prompt.invoke({\n",
    "            **state,    # will take the messages from here\n",
    "            \"query\": user_query.content   # and the user query from here\n",
    "        })\n",
    "        generated_query = self.llm.invoke(prompt)  \n",
    "        return {\n",
    "            \"messages\": [HumanMessage(content=generated_query.content)] # append the generated query to the messages\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e84feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_plus_rag_chatbot = BasicPlusRAGChatbot(llm, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0790abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_plus_rag_chatbot.run()\n",
    "# What sessions do I have about virtual assistants?\n",
    "# What is the reading for the third one?\n",
    "# And second?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7a436",
   "metadata": {},
   "source": [
    "<h2 id=\"adv_rag\">4. Advanced RAG üòé</h2>\n",
    "\n",
    "Now we can move to a more complicated implementation. We will now make an iterative RAG chatbot: this chatbot will retrieve contexts iteratively and decide at each step whether the chunks retrieved so far are sufficient to answer the question; the answer is generated only when the retrieved contexts are enough (or when the limit on number of the contexts is reached).\n",
    "\n",
    "Basically, we have almost everything we need to implement an iterative RAG pipeline. We only need to add three more nodes:\n",
    "1. A node to (re)generate search queries for the index: now we won't use the user query but specifically generate queries for the index.\n",
    "2. A decision node, in which the LLM will decide whether the context retrieved so far is enough to proceed to the generation of the response.\n",
    "3. As a useful addition, we will also add LLM-based filtering of the retrieved documents to filter out the documents that are semantically similar to the query but are not really relevant for answering the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0f4a3",
   "metadata": {},
   "source": [
    "We will start by transforming the state to accumulate the contexts gathered so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGState(SimpleState): # \"messages\" is already defined in SimpleRAGState\n",
    "    queries: List[List[str]]            # this is the list of generated queries, one list per query generation\n",
    "    docs: List[List[Document]]    # this is the list of retrieved documents, one list per retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf46183",
   "metadata": {},
   "source": [
    "To (re)generate the queries, filter the retrieved documents, and decide whether the contexts are supportive enough, we add respective prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_query_template = \"\"\"\\\n",
    "The user is now asking a question. \\\n",
    "Your task is to think of a set of queries that will retrieve all necessary documents \\\n",
    "from the knowledge base to answer the user question. \\\n",
    "Return a comma-separated list of generated queries only (1-3 queries), without any other text.\n",
    "Imagine you are trying to find a user find information in the internet \\\n",
    "on the topic they are interested in: don't just repeat the same query in different words \\\n",
    "but try to understand what information is required for answering the question and \\\n",
    "what queries would retrieve this information.\n",
    "\n",
    "Below is a list of previously generated queries, if any. \\\n",
    "The previous queries retrieved documents turned out to be insufficient \\\n",
    "to answer the user's question. Avoid repeating the previous queries \\\n",
    "and explore different aspects of the question.\n",
    "\n",
    "Previous queries:\n",
    "{previous_queries}\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\n",
    "Your generated queries:\n",
    "\"\"\"\n",
    "\n",
    "generate_query_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t(\"system\", rag_system_prompt),\n",
    "        (\"user\", generate_query_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20076eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_relevant_template = \"\"\"\\\n",
    "The user is now asking a question. \\\n",
    "For answering the question, your colleague has retrieved the document below. \\\n",
    "Return the degree 1-5 to which the document is related to the user's question, \\\n",
    "where 1 means the document is about something absolutely different, and \\\n",
    "5 means the document is exactly about the topic brought up in the user's question.\n",
    "\n",
    "\n",
    "===========================\n",
    "Document:\n",
    "\n",
    "{document}\n",
    "===========================\n",
    "\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\n",
    "Rating (1-5):\n",
    "\"\"\"\n",
    "\n",
    "document_relevant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t(\"system\", rag_system_prompt),\n",
    "        (\"user\", document_relevant_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_sufficient_template = \"\"\"\\\n",
    "The user is now asking a question. \\\n",
    "For answering the question, your colleague has retrieved the documents below. \\\n",
    "Return True if the documents are sufficient for answering the question, and False otherwise.\n",
    "\n",
    "\n",
    "===========================\n",
    "Documents:\n",
    "\n",
    "{documents}\n",
    "===========================\n",
    "\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\n",
    "Verdict (True/False):\n",
    "\"\"\"\n",
    "\n",
    "documents_sufficient_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t(\"system\", rag_system_prompt),\n",
    "        (\"user\", documents_sufficient_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2abb9a",
   "metadata": {},
   "source": [
    "To reliably parse the generations, we need to add the following structure output schemes:\n",
    "1. String list for the generated queries.\n",
    "2. Rating 1-5 for document relevancy.\n",
    "3. Boolean `True`/`False` for binary documents sufficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d332ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOutput(BaseModel):\n",
    "    outputs: List[str] = Field(..., description=\"List of generated outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating1_5(BaseModel):\n",
    "    rating: int = Field(..., description=\"Rating from 1 to 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoVerdict(BaseModel):\n",
    "    verdict: bool = Field(..., description=\"Boolean answer to the given binary question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99272e46",
   "metadata": {},
   "source": [
    "For advanced RAG, we change the LLM and the settings to increase the overall stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any model, catalogue is available under https://build.nvidia.com/models\n",
    "MODEL_NAME_ADVANCED_RAG = \"meta/llama-3.1-405b-instruct\"\n",
    "\n",
    "# this rate limiter will ensure we do not exceed the rate limit\n",
    "# of 40 RPM given by NVIDIA\n",
    "rate_limiter_advanced_rag = InMemoryRateLimiter(\n",
    "    requests_per_second=1 / 6,  # 10 requests per minute to be sure\n",
    "    check_every_n_seconds=5,  # wake up every 5 seconds to check whether allowed to make a request,\n",
    "    max_bucket_size=1  # controls the maximum burst size\n",
    ")\n",
    "\n",
    "llm_advanced_rag = ChatNVIDIA(\n",
    "    model=MODEL_NAME_ADVANCED_RAG,\n",
    "    # api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    temperature=0,   # ensure reproducibility,\n",
    "    rate_limiter=rate_limiter_advanced_rag  # bind the rate limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ab6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeRAGChatbot(BasicPlusRAGChatbot):\n",
    "\n",
    "    _graph_path = \"./graph_iterative_rag.png\"\n",
    "\n",
    "    def __init__(self, llm, k=5, max_generations=3):\n",
    "        super().__init__(llm, k)\n",
    "        self.max_generations = max_generations\n",
    "        self.query_generator = generate_query_prompt | llm.with_structured_output(ListOutput)\n",
    "        self.document_relevant_grader = document_relevant_prompt | llm.with_structured_output(Rating1_5)\n",
    "        self.documents_sufficient_grader = documents_sufficient_prompt | llm.with_structured_output(YesNoVerdict)\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(AdvancedRAGState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"complete_query\", self._complete_query_node)\n",
    "        self._graph_builder.add_node(\"generate_query\", self._generate_query_node)\n",
    "        self._graph_builder.add_node(\"retrieve\", self._retrieve_node)\n",
    "        self._graph_builder.add_node(\"filter_documents\", self._filter_documents_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        # basic rag: no planning, just always retrieve\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"complete_query\", True: END})\n",
    "        self._graph_builder.add_edge(\"complete_query\", \"generate_query\")\n",
    "        self._graph_builder.add_edge(\"generate_query\", \"retrieve\")\n",
    "        self._graph_builder.add_edge(\"retrieve\", \"filter_documents\")\n",
    "        self._graph_builder.add_conditional_edges(\n",
    "            \"filter_documents\",\n",
    "            self._documents_sufficient_node,\n",
    "            {\n",
    "                False: \"generate_query\",\n",
    "                True: \"respond\",\n",
    "                None: END   # max generations reached\n",
    "            }\n",
    "        )\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "\n",
    "    def _generate_query_node(self, state: AdvancedRAGState) -> dict:    \n",
    "        complete_query = state[\"messages\"][-1].content  # complete user query\n",
    "        previous_queries = state[\"queries\"][-1] if state[\"queries\"] else []\n",
    "        generated_queries = self.query_generator.invoke(\n",
    "            {\n",
    "                # put previous queries\n",
    "                \"previous_queries\": \"\\n\".join(previous_queries) or \"NONE\",\n",
    "                \"input\": complete_query\n",
    "            }\n",
    "        )\n",
    "        return {\n",
    "            # could have also used `Annotated` here\n",
    "            \"queries\": state[\"queries\"] + [generated_queries.outputs]   # List[str]\n",
    "        }\n",
    "\n",
    "    # now store the documents in the separate field\n",
    "    def _retrieve_node(self, state: AdvancedRAGState) -> dict:    \n",
    "        # retrieve the documents\n",
    "        latest_queries = state[\"queries\"][-1]  # the last generated queries\n",
    "        # now use the retriever directly to get a list of documents and not a combined string\n",
    "        latest_documents = state[\"docs\"][-1] if state[\"docs\"] else []\n",
    "        newest_documents = []\n",
    "        for query in latest_queries:\n",
    "             documents = self.retriever.invoke(query)\n",
    "             newest_documents.extend([doc for doc in documents if doc not in newest_documents])\n",
    "        # add the document to the messages\n",
    "        newest_documents = [doc for doc in newest_documents if doc not in latest_documents]\n",
    "        return {\n",
    "            # could have also used `Annotated` here\n",
    "            \"docs\": state[\"docs\"] + [newest_documents]\n",
    "        }\n",
    "    \n",
    "    def _filter_documents_node(self, state: AdvancedRAGState) -> dict:\n",
    "        complete_query = state[\"messages\"][-1].content  # complete user query\n",
    "        # since the retrieved documents are graded at the same step,\n",
    "        # we only need to pass the last batch of documents\n",
    "        latest_documents = state[\"docs\"].pop(-1)  # will be replaced with the filtered ones\n",
    "        # grade each document separately and only keep the relevant ones\n",
    "        latest_documents_relevant = []\n",
    "        for document in latest_documents:\n",
    "            print(\"Grading document:\\n\\n\", document.page_content)\n",
    "            rating = self.document_relevant_grader.invoke(\n",
    "                {\n",
    "                    \"document\": document.page_content,    # this is a Document object\n",
    "                    \"input\": complete_query\n",
    "                }\n",
    "            )\n",
    "            print(\"\\n\\n\")\n",
    "            print(f\"Rating: {rating.rating}\")\n",
    "            print(\"\\n\\n=====================\\n\\n\")\n",
    "            if rating.rating >= 3:    # boolean value according to the Pydantic model\n",
    "                latest_documents_relevant.append(document)\n",
    "        return {\n",
    "            # could have also used `Annotated` here\n",
    "            \"docs\": state[\"docs\"] + [latest_documents_relevant]\n",
    "        }\n",
    "    \n",
    "    def _flatten_documents(self, state: AdvancedRAGState) -> List[Document]:\n",
    "        all_documents = state[\"docs\"]\n",
    "        return [document for sublist in all_documents for document in sublist]\n",
    "\n",
    "    def _documents_sufficient_node(self, state: AdvancedRAGState) -> dict:\n",
    "        complete_query = state[\"messages\"][-1].content  # complete user query\n",
    "        documents = self._flatten_documents(state)\n",
    "        if not documents:\n",
    "            return False\n",
    "        documents_str = \"\\n\\n\".join([document.page_content for document in documents])\n",
    "        print(\"Deciding whether the documents are sufficient\")\n",
    "        verdict = self.documents_sufficient_grader.invoke(\n",
    "                {\n",
    "                \"documents\": documents_str,    # this is a Document object\n",
    "                \"input\": complete_query\n",
    "            }\n",
    "        )\n",
    "        print(\"\\n\\n\")\n",
    "        print(f\"Verdict: {verdict.verdict}\")\n",
    "        print(\"\\n\\n=====================\\n\\n\")\n",
    "        if not verdict.verdict and len(documents) == self.max_generations:\n",
    "            return  # will route to END\n",
    "        return verdict.verdict\n",
    "    \n",
    "    def _respond_node(self, state: AdvancedRAGState) -> dict:\n",
    "        documents = self._flatten_documents(state)\n",
    "        documents_str = \"\\n\\n\".join([document.page_content for document in documents])\n",
    "        complete_query = state[\"messages\"][-1].content\n",
    "        prompt = self.rag_prompt.invoke(\n",
    "            {\n",
    "                \"messages\": state[\"messages\"],  # this goes to the message placeholder\n",
    "                \"context\": documents_str,  # this goes to the user message\n",
    "                \"input\": complete_query    # this goes to the user message\n",
    "            }\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response\n",
    "        }\n",
    "    \n",
    "    def run(self):\n",
    "        input = {\"messages\": [], \"docs\": [], \"queries\": []}\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"updates\"):\n",
    "            for key, value in event.items():\n",
    "                if value and value.get(\"messages\"):\n",
    "                    # print(key, end=\"\\n\\n\")\n",
    "                    if not isinstance(value[\"messages\"], list):\n",
    "                        value[\"messages\"] = [value[\"messages\"]]\n",
    "                    for message in  value[\"messages\"]:\n",
    "                        message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    "                elif value and value.get(\"queries\"):\n",
    "                    print(f\"\\n\\nGenerated queries: {', '.join(value['queries'][-1])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae551fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make very small k to ensure one retrieval is not enough\n",
    "iterative_rag_chatbot = IterativeRAGChatbot(llm_advanced_rag, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_rag_chatbot.run()\n",
    "# Summarize the topics of all the pitches in two sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
