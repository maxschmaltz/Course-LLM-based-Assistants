{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c99fa75",
   "metadata": {},
   "source": [
    "# 12.11 & 13.11. RAG Chatbot üìö\n",
    "\n",
    "üìç [Download notebook](https://github.com/maxschmaltz/Course-LLM-based-Assistants/tree/main/llm-based-assistants/sessions/block2_core/1211_1311/1211_1311.ipynb)\n",
    "\n",
    "In today'l lab, we will be expanding the chatbot we created in our [previous session](../0611.ipynb). We'll implement a RAG functionality so that the chatbot has access to custom knowledge. In the first part (12.11), we'll preprocess our data for further retrieval. Next day (13.11), we will complete the RAG chatbot and use the data we have preprocessed in the first part to inject our custom knowledge to the LLM. \n",
    "\n",
    "Our plan for 12.11 & 13.11:\n",
    "\n",
    "* [Data Preprocessing](#data)\n",
    "* [Prompting al LangChain](#prompts)\n",
    "* [Simple RAG](#rag)\n",
    "* [Advanced RAG](#adv_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6d768",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To start with the tutorial, complete the steps [Prerequisites](../../../../infos/llm_inference_guide/README.md#prerequisites), [Environment Setup](../../../../infos/llm_inference_guide/README.md#environment-setup), and [Getting API Key](../../../../infos/llm_inference_guide/README.md#getting-api-key) from the [LLM Inference Guide](../../../../infos/llm_inference_guide/README.md).\n",
    "\n",
    "Today, we have more packages so we'll use the requirements file to install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o requirements.txt https://raw.githubusercontent.com/maxschmaltz/Course-LLM-based-Assistants/main/llm-based-assistants/sessions/block2_core/1211_1311/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d89f95",
   "metadata": {},
   "source": [
    "Finally, download the data we'll be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d388b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o topic_overview.pdf https://raw.githubusercontent.com/maxschmaltz/Course-LLM-based-Assistants/main/llm-based-assistants/sessions/block2_core/1211_1311/topic_overview.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36839b67",
   "metadata": {},
   "source": [
    "In the [last session](../0611.ipynb), we created a basic chatbot implemented with LangGraph. The chatbot was built as a graph-like system with the following components:\n",
    "1. The input receival node. It prompted the user for the input and stored it in the messages for further interaction with the LLM.\n",
    "2. The router node. It performed the check whether the user wants to exit.\n",
    "3. The chatbot node. It received the input if the user had not quit, passed it to the LLM, and returned the generation.\n",
    "\n",
    "Before we begin, let's pull this chatbot, as it will be used as the base class for the further RAG chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "# read system variables\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()    # that loads the .env file variables into os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4426472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any model, catalogue is available under https://build.nvidia.com/models\n",
    "MODEL_NAME = \"meta/llama-3.3-70b-instruct\"\n",
    "\n",
    "# this rate limiter will ensure we do not exceed the rate limit\n",
    "# of 40 RPM given by NVIDIA\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=35 / 60,  # 35 requests per minute to be sure\n",
    "    check_every_n_seconds=0.1,  # wake up every 100 ms to check whether allowed to make a request,\n",
    "    max_bucket_size=7,  # controls the maximum burst size\n",
    ")\n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "    model=MODEL_NAME,\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    temperature=0,   # ensure reproducibility,\n",
    "    rate_limiter=rate_limiter  # bind the rate limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleState(TypedDict):\n",
    "    # `messages` is a list of messages of any kind. The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    # Since we didn't define a function to update it, it will be rewritten at each transition\n",
    "    # with the value you provide\n",
    "    n_turns: int    # just for demonstration\n",
    "    language: str   # new, for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "\n",
    "    _graph_path = \"./graph.png\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self._build()\n",
    "        self._display_graph()\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(SimpleState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"respond\", True: END})\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "\n",
    "    def _compile(self):\n",
    "        self.chatbot = self._graph_builder.compile()\n",
    "\n",
    "    def _input_node(self, state: SimpleState) -> dict:\n",
    "        user_query = input(\"Your message: \")\n",
    "        human_message = HumanMessage(content=user_query)\n",
    "        # add the input to the messages\n",
    "        return {\n",
    "            \"messages\": human_message   # this will append the input to the messages\n",
    "        }\n",
    "    \n",
    "    def _respond_node(self, state: SimpleState) -> dict:\n",
    "        messages = state[\"messages\"]    # will already contain the user query\n",
    "        n_turns = state[\"n_turns\"]\n",
    "        response = self.llm.invoke(messages)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response,   # this will append the response to the messages\n",
    "            \"n_turns\": n_turns + 1  # and this will rewrite the number of turns\n",
    "        }\n",
    "    \n",
    "    def _is_quitting_node(self, state: SimpleState) -> dict:\n",
    "        # check if the user wants to quit\n",
    "        user_message = state[\"messages\"][-1].content\n",
    "        return user_message.lower() == \"quit\"\n",
    "    \n",
    "    def _display_graph(self):\n",
    "        display(\n",
    "            Image(\n",
    "                self.chatbot.get_graph().draw_mermaid_png(\n",
    "                    output_file_path=self._graph_path\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # add the run method\n",
    "    def run(self, language=None):\n",
    "        input = {\n",
    "            \"messages\": [\n",
    "                SystemMessage(\n",
    "                    content=\"You are a helpful and honest assistant.\" # role\n",
    "                )\n",
    "            ],\n",
    "            \"n_turns\": 0,\n",
    "            \"language\": language or \"English\"    # new\n",
    "        }\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"values\"):   #stream_mode=\"updates\"):\n",
    "            for key, value in event.items():\n",
    "                print(f\"{key}:\\t{value}\")\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c7fbc",
   "metadata": {},
   "source": [
    "<h2 id=\"data\">1. Data Preprocessing üìï</h2>\n",
    "\n",
    "As you remember from the lecture, the first step to RAG is data preprocessing. That includes:\n",
    "1. Loading: load the source (document, website etc.) as a text.\n",
    "2. Chunking: chunk the loaded text onto smaller pieces.\n",
    "3. Converting to embeddings: embed the chunks into dense vector for further similarity search.\n",
    "4. Indexing: put the embeddings into a so-called index -- a special database for efficient storage and search of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea291c",
   "metadata": {},
   "source": [
    "### Loading\n",
    "\n",
    "We will take a PDF version of the Topic Overview for the last iteration of this course (the current version does not convert nicely). No LLM can know the contents of it, especially some highly specific facts such as dates or key points.\n",
    "\n",
    "One of ways to load a PDF is to use [`PyPDFLoader`](https://docs.langchain.com/oss/python/integrations/document_loaders/pypdfloader) that load simple textual PDFs and their metadata. In this tutorial, we focus on a simpler variant when there are no multimodal data in the PDF. You can find out more about advanced loading on the PyPDFLoader [LangChain page](https://docs.langchain.com/oss/python/integrations/document_loaders/pypdfloader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./topic_overview.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1000b5",
   "metadata": {},
   "source": [
    "This function returns a list of `Document` objects, each containing the text of the PDF and its metadata such as title, page, creation date etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7561e",
   "metadata": {},
   "source": [
    "As you can see, the result is not satisfying because the PDF has a more complex structure than just one-paragraph text. To handle it's layout, we could use `UnstructuredLoader`-like OCR engines that will return a `Document` not for the whole page but for a single structure. One of the open-source solutions is [Docling](https://docs.langchain.com/oss/python/integrations/document_loaders/docling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd803e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DoclingLoader(file_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f4104",
   "metadata": {},
   "source": [
    "Look at how Docling parsed a single structure (left sidebar) into a separate `Document`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80791265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c845c6",
   "metadata": {},
   "source": [
    "However, it also parsed every paragraph from the body of the PDF into a separate `Document`, which destroyed the links between the paragraphs of the same sections: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abddab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[5].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964892ef",
   "metadata": {},
   "source": [
    "To avoid such a fine chunking, we ask the engine to output the whole PDF as a single MD file and then chunk it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DoclingLoader(file_path, export_type=ExportType.MARKDOWN)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64395d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bcc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b292a",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "During RAG, relevant documents are usually retrieved by semantic similarity that is calculated between the search query and each document in the index. However, if we calculate vectors for the entire PDF pages, we risk not to capture any meaning in the embedding because the context is just too long. That is why usually, loaded text is _chunked_ in a RAG application; embeddings for smaller pieces of text are more discriminative, and thus the relevant context may be retrieved more reliably. Furthermore, it ensure process consistency when working documents of varying sizes.\n",
    "\n",
    "Different approaches to chunking are described in tutorial [Text splitters](https://docs.langchain.com/oss/python/integrations/splitters/index#text-splitters) from LangChain. Even though Docling returned a MD document, we won't be using the dedicated [`MarkdownHeaderTextSplitter`](https://docs.langchain.com/oss/python/integrations/splitters/markdown_header_metadata_splitter); instead, for a more general picture, we choose the [`RecursiveCharacterTextSplitter`](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter) -- a good option in terms of simplicity-quality ratio for simple cases. This splitter tries to keep text structures (paragraphs, sentences) together and thus maintain text coherence in chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bef398",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, # maximum number of characters in a chunk\n",
    "    chunk_overlap=250, # number of characters to overlap between chunks\n",
    "    separators=[\"\\n\"]\n",
    ")\n",
    "\n",
    "def split_page(doc: Document) -> List[Document]:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                **doc.metadata,\n",
    "                \"chunk_n\": i\n",
    "            },\n",
    "        ) \n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da17475",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for doc in docs:\n",
    "    chunks += split_page(doc)\n",
    "\n",
    "print(f\"Converted {len(docs)} pages into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks[12].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77fe86",
   "metadata": {},
   "source": [
    "### Convert to Embeddings\n",
    "\n",
    "As discussed, the retrieval usually supported by vector similarity and so the index contains not the actual texts but their vector representations. Vector representations are created by _embedding models_ -- models usually made specifically for this objective by being trained to create more similar vectors for more similar sentences and to push apart dissimilar sentences in the vector space.\n",
    "\n",
    "We will use the [`nv-embedqa-e5-v5`](https://build.nvidia.com/nvidia/nv-embedqa-e5-v5?snippet_tab=LangChain) model -- a model from NVIDIA pretrained for English QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0dbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36457d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_NAME = \"nvidia/nv-embedqa-e5-v5\"\n",
    "\n",
    "embeddings = NVIDIAEmbeddings(\n",
    "    model=EMBEDDING_NAME, \n",
    "    # api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1d288",
   "metadata": {},
   "source": [
    "An embedding model receives an input text and returns a dense vector that is believed to capture its semantic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68bafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = embeddings.embed_query(\"retrieval augmented generation\")\n",
    "test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273ff5e",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Now that we have split our data and initialized the embeddings, we can start indexing it. There are a lot of different implementations of indexes, you can take a lot at available options in [Vector stores](https://docs.langchain.com/oss/python/integrations/vectorstores). One of the popular choices is [Qdrant](https://docs.langchain.com/oss/python/integrations/vectorstores/qdrant) that provides a simple data management and can be deployed both locally, on a remote machine, and on the cloud.\n",
    "\n",
    "Qdrant support persisting your vector storage, i.e. storing it on the working machine, but for simplicity, we will use it in the in-memory mode, so that the storage exists only as long as the notebook does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954285bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee0509",
   "metadata": {},
   "source": [
    "First things first, we need to create a _client_ -- a Qdrant instance that will be the entrypoint for all the actions we do with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf05f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client = QdrantClient(\":memory:\")    # in-memory Qdrant client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6f625",
   "metadata": {},
   "source": [
    "Then, as we use an in-memory client that does not store the index between the notebook sessions, we need to initialize a _collection_. Alternatively, if we were persisting the data, we would perform a check if the collection exists and then either create or load it.\n",
    "\n",
    "For Qdrant to initialize the structure of the index correctly, we need to provide the dimentionality of the embedding we will be using as well as teh distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"1211_1311\"\n",
    "\n",
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    # embedding params here\n",
    "    vectors_config=VectorParams(\n",
    "        size=len(test_embedding),   # is there a better way?\n",
    "        distance=Distance.COSINE    # cosine distance\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1682d",
   "metadata": {},
   "source": [
    "Finally, we use a LangChain wrapper to connect to the index to unify the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01711510",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=qd_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c442af",
   "metadata": {},
   "source": [
    "Now we are ready to add our chunks to the vector storage. As we will be adding the chunks, the index will take care about converting our passages into embeddings.\n",
    "\n",
    "In order to be able to delete / modify the chunks afterwards, we assign them with unique ids that we generate dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "vector_store.add_documents(\n",
    "    chunks,\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.get_by_ids([ids[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete([ids[-1]])\n",
    "vector_store.get_by_ids([ids[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32db3ae",
   "metadata": {},
   "source": [
    "The index provides the necessary functionality for the query-based retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(\"retrieval augmented generation\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfa4d4",
   "metadata": {},
   "source": [
    "<h2 id=\"prompts\">2. Prompting al LangChain üìù</h2>\n",
    "\n",
    "When you build more complex algorithms, just passing the human query directly might be not enough. Sometimes, you need to give more specific instructions, pre- and append additional stuff to the messages; for example, in (classic) RAG, the retrieved chunks are injected as a context in a dedicated placeholder.\n",
    "\n",
    "To handle the static instructions more efficiently, you can use `ChatPromptTemplate`. The key idea is simple: in a `ChatPromptTemplate`, you write all the static fragments in plain text and then use placeholders to mark the places where some variable parts will be added. Then, when you receive an input, LangChain fills the placeholders and you receive the desired version of the message with all the placeholders filled automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b197c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab23639",
   "metadata": {},
   "source": [
    "Before moving to RAG, let us consider a different case to demonstrate the standalone effect of the prompting: CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_template_str = \"\"\"\\\n",
    "The user is asking a question. Please answer it using step-by-step reasoning. \\\n",
    "On each reasoning step, assess whether this reasoning step is good or not, \\\n",
    "on a scale from 1 to 10.\n",
    "\n",
    "The user question is:\n",
    "\n",
    "============\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "input_template = ChatPromptTemplate.from_template(input_template_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac74461",
   "metadata": {},
   "source": [
    "Now, even though the user will provide a simple query as usual, the LLM will receive all the additional instructions you wrote. A `ChatPromptTemplate` uses **keys** to fill the placeholders so you should pass it a corresponding `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = input_template.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the distance between the Earth and the Moon divided by the number of the planets in the Solar system?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f239ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example.messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783b0c7",
   "metadata": {},
   "source": [
    "You can also make higher level prompt templates -- that is, with placeholders not for a single message, but for a sequence of messages. To do so, you need to nest `ChatPromptTemplate`s for separate messages and use the `MessagesPlaceholder` for sequences. This approach gives you a universal way to fill the placeholders, be it a separate fragment of a certain message or a whole sequence of messages: all you need is to be careful with the keys, and LangChain will take care of the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f57dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = SystemMessagePromptTemplate.from_template(\"Answer in the following language: {language}.\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_template,\n",
    "        MessagesPlaceholder(variable_name=\"messages\")   # here, you add an entire sequence of messages\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51545476",
   "metadata": {},
   "source": [
    "Alternative: pass separate messages as pairs of raw strings where the first string describes the role (`\"system\"`, `\"user\"`, `\"assistant\"`) and the second -- the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf37a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer in the following language: {language}.\"),    # here, you modify a fragment of the system message\n",
    "        MessagesPlaceholder(variable_name=\"messages\")   # here, you add an entire sequence of messages\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec58bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke({\n",
    "    \"language\": \"Spanish\",\n",
    "    \"messages\": example.to_messages()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8de5a",
   "metadata": {},
   "source": [
    "We can now incorporate this logic into our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTChatbot(Chatbot):\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm)\n",
    "        # add templates\n",
    "        self.input_template = input_template\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def _input_node(self, state: SimpleState) -> dict:\n",
    "        user_query = input(\"Your message: \")\n",
    "        if user_query != \"quit\":\n",
    "            # invoke the template here\n",
    "            human_message = self.input_template.invoke(\n",
    "                {\n",
    "                    \"input\": user_query\n",
    "                }\n",
    "            ).to_messages()\n",
    "        else:\n",
    "            human_message = HumanMessage(content=user_query)\n",
    "        n_turns = state[\"n_turns\"]\n",
    "        # add the input to the messages\n",
    "        return {\n",
    "            \"messages\": human_message\n",
    "        }\n",
    "    \n",
    "    def _respond_node(self, state: SimpleState) -> dict:\n",
    "        # invoke the template here;\n",
    "        # since the state is already a dictionary, we can just pass it as is\n",
    "        prompt = self.prompt_template.invoke(state)\n",
    "        n_turns = state[\"n_turns\"]\n",
    "        response = self.llm.invoke(prompt)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response,\n",
    "            \"n_turns\": n_turns + 1\n",
    "        }\n",
    "\n",
    "    def run(self, language=None):\n",
    "        # since the system message is now part of the prompt template,\n",
    "        # we don't need to add it to the input\n",
    "        input = {\n",
    "            \"messages\": [],\n",
    "            \"n_turns\": 0,\n",
    "            \"language\": language or \"English\"    # new\n",
    "        }\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"values\"):\n",
    "            if event[\"messages\"]:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10586e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_chatbot = CoTChatbot(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ee066",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_chatbot.run(\"German\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e75f5f",
   "metadata": {},
   "source": [
    "<h2 id=\"rag\">3. Basic RAG üíâ</h2>\n",
    "\n",
    "Now let's apply our new prompting tool to the basic RAG workflow. For that, we will just retrieve _k_ most relevant documents and them insert them into the prompt as a part of the context.\n",
    "\n",
    "We will combine the skills we have obtained so far to build a LangGraph agent that receives an input, checks if the user wants to quit, then do the retrieval and generate a context-aware response if not. We will build on the basic version of our first chatbot; to add the RAG functionality, we need to add a retrieval node and modify the generation prompt to inject the retrieved documents.\n",
    "\n",
    "LangGraph provides a pre-built tool to conveniently create a retriever tool. The retriever tool can be connected to the LLM; in this case, the LLM decides itself whether to use it or not. But for our RAG labs, we separate the tool from the LLM to concentrate on the retrieval and not the agentic aspect of the system (which will be covered in the next week's lab). We also won't generate queries for the retriever for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ad131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# role: restrict it from the parametric knowledge\n",
    "rag_system_prompt = \"\"\"\\\n",
    "You are an assistant that has access to a knowledge base. \\\n",
    "You should use the knowledge base to answer the user's questions \\\n",
    "unless you can reliably answer them from your own knowledge.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# this will add the context to the input\n",
    "context_injection_prompt = \"\"\"\\\n",
    "The user is asking a question. \\\n",
    "Answer using the following knowledge from the knowledge base:\n",
    "\n",
    "\n",
    "==========================\n",
    "{context}\n",
    "==========================\n",
    "\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# finally, gather the system message, the previous messages,\n",
    "# and the input with the context\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),   # system message\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # previous messages\n",
    "        (\"user\", context_injection_prompt)  # user message\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRAGChatbot(Chatbot):\n",
    "\n",
    "    _graph_path = \"./graph_basic_rag.png\"\n",
    "    \n",
    "    def __init__(self, llm, k=5):\n",
    "        super().__init__(llm)\n",
    "        self.rag_prompt = rag_prompt\n",
    "        self.retriever = vector_store.as_retriever(search_kwargs={\"k\": k})    # retrieve 5 documents\n",
    "        self.retriever_tool = create_retriever_tool(    # and this is the tool\n",
    "            self.retriever,\n",
    "            \"retrieve_internal_data\",  # name\n",
    "            \"Search relevant information in internal documents.\",   # description\n",
    "        )\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(SimpleState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"retrieve\", self._retrieve_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        # basic rag: no planning, just always retrieve\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"retrieve\", True: END})\n",
    "        self._graph_builder.add_edge(\"retrieve\", \"respond\")\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "    \n",
    "    def _retrieve_node(self, state: SimpleState) -> dict:\n",
    "        # retrieve the context\n",
    "        user_query = state[\"messages\"][-1].content  # use the last message as the query\n",
    "        context = self.retriever_tool.invoke({\"query\": user_query})\n",
    "        # add the context to the messages\n",
    "        return {\n",
    "            \"messages\": AIMessage(content=context)\n",
    "        }\n",
    "    \n",
    "    def _respond_node(self, state: SimpleState) -> dict:\n",
    "        # the workflow is designed so that the context is always the last message\n",
    "        # and the user query is the second to last message\n",
    "        context = state[\"messages\"].pop(-1).content # don't want the context in the messages\n",
    "        user_query = state[\"messages\"][-1].content\n",
    "        prompt = self.rag_prompt.invoke(\n",
    "            {\n",
    "                \"messages\": state[\"messages\"],  # this goes to the message placeholder\n",
    "                \"context\": context,  # this goes to the user message\n",
    "                \"input\": user_query    # this goes to the user message\n",
    "            }\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        input = {\"messages\": []}\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"values\"):\n",
    "            if event[\"messages\"]:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afded1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag_chatbot = BasicRAGChatbot(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469265f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag_chatbot.run()\n",
    "# What sessions do I have about virtual assistants?\n",
    "# What is the reading for the third one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e7102",
   "metadata": {},
   "source": [
    "As you can see, it already works pretty well, but as the retrieval goes by the user query directly, the previous context of the conversation is not considered by the _retriever_. To handle that, let's add a node that would reformulate the query taking in consideration the previous interactions.\n",
    "\n",
    "For that, we need an additional prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_query_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),  # previous messages\n",
    "        (\n",
    "\t\t\t\"user\",\n",
    "\t\t\t\"Complete the following user query in the last message to a full question based on the previous messages. \"\n",
    "            \"Return only the reformulated user query, without any other text. Do not expand the query \"\n",
    "\t\t\t\"or change its meaning and only fill in the missing information in the last question. \"\n",
    "\t\t\t\"\\n\\nUser query:\\n{query}\\n\\nReformulated query:\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41795455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPlusRAGChatbot(BasicRAGChatbot):\n",
    "\n",
    "    _graph_path = \"./graph_basic_plus_rag.png\"\n",
    "\n",
    "    def __init__(self, llm, k=5):\n",
    "        super().__init__(llm, k)\n",
    "        self.complete_query_prompt = complete_query_prompt\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(SimpleState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"complete_query\", self._complete_query_node)\n",
    "        self._graph_builder.add_node(\"retrieve\", self._retrieve_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        # basic rag: no planning, just always retrieve\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"complete_query\", True: END})\n",
    "        self._graph_builder.add_edge(\"complete_query\", \"retrieve\")\n",
    "        self._graph_builder.add_edge(\"retrieve\", \"respond\")\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "\n",
    "    def _complete_query_node(self, state: SimpleState) -> dict:\n",
    "        # since we use the generated query instead of the user query,\n",
    "        # we want to remove the original user query from the messages\n",
    "        # and replace it with the generated one, having converted it to a HumanMessage\n",
    "        if len(state[\"messages\"]) == 1:\n",
    "            return {}  # no need to reformulate the first query\n",
    "        user_query = state[\"messages\"].pop(-1)\n",
    "        prompt = self.complete_query_prompt.invoke({\n",
    "            **state,    # will take the messages from here\n",
    "            \"query\": user_query.content   # and the user query from here\n",
    "        })\n",
    "        generated_query = self.llm.invoke(prompt)  \n",
    "        return {\n",
    "            \"messages\": [HumanMessage(content=generated_query.content)] # append the generated query to the messages\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e84feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_plus_rag_chatbot = BasicPlusRAGChatbot(llm, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0790abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_plus_rag_chatbot.run()\n",
    "# What sessions do I have about virtual assistants?\n",
    "# What is the reading for the third one?\n",
    "# And second?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7a436",
   "metadata": {},
   "source": [
    "<h2 id=\"adv_rag\">4. Advanced RAG üòé</h2>\n",
    "\n",
    "Now we can move to a more complicated implementation. We will now make an iterative RAG chatbot: this chatbot will retrieve contexts iteratively and decide at each step whether the chunks retrieved so far are sufficient to answer the question; the answer is generated only when the retrieved contexts are enough (or when the limit on number of the contexts is reached).\n",
    "\n",
    "Basically, we have almost everything we need to implement an iterative RAG pipeline. We only need to add three more nodes:\n",
    "1. A node to (re)generate search queries for the index: now we won't use the user query but specifically generate queries for the index.\n",
    "2. A decision node, in which the LLM will decide whether the context retrieved so far is enough to proceed to the generation of the response.\n",
    "3. As a useful addition, we will also add LLM-based filtering of the retrieved documents to filter out the documents that are semantically similar to the query but are not really relevant for answering the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0f4a3",
   "metadata": {},
   "source": [
    "We will start by transforming the state to accumulate the contexts gathered so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGState(SimpleState): # \"messages\" is already defined in SimpleRAGState\n",
    "    queries: List[List[str]]            # this is the list of generated queries, one list per query generation\n",
    "    docs: List[List[Document]]    # this is the list of retrieved documents, one list per retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf46183",
   "metadata": {},
   "source": [
    "To (re)generate the queries, filter the retrieved documents, and decide whether the contexts are supportive enough, we add respective prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_query_template = \"\"\"\\\n",
    "The user is now asking a question. \\\n",
    "Your task is to think of a set of queries that will retrieve all necessary documents \\\n",
    "from the knowledge base to answer the user question. \\\n",
    "Return a comma-separated list of generated queries only (1-3 queries), without any other text.\n",
    "Imagine you are trying to find a user find information in the internet \\\n",
    "on the topic they are interested in: don't just repeat the same query in different words \\\n",
    "but try to understand what information is required for answering the question and \\\n",
    "what queries would retrieve this information.\n",
    "\n",
    "Below is a list of previously generated queries, if any. \\\n",
    "The previous queries retrieved documents turned out to be insufficient \\\n",
    "to answer the user's question. Avoid repeating the previous queries \\\n",
    "and explore different aspects of the question.\n",
    "\n",
    "Previous queries:\n",
    "{previous_queries}\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\n",
    "Your generated queries:\n",
    "\"\"\"\n",
    "\n",
    "generate_query_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t(\"system\", rag_system_prompt),\n",
    "        (\"user\", generate_query_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20076eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_relevant_template = \"\"\"\\\n",
    "The user is now asking a question. \\\n",
    "For answering the question, your colleague has retrieved the document below. \\\n",
    "Return the degree 1-5 to which the document is related to the user's question, \\\n",
    "where 1 means the document is about something absolutely different, and \\\n",
    "5 means the document is exactly about the topic brought up in the user's question.\n",
    "\n",
    "\n",
    "===========================\n",
    "Document:\n",
    "\n",
    "{document}\n",
    "===========================\n",
    "\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\n",
    "Rating (1-5):\n",
    "\"\"\"\n",
    "\n",
    "document_relevant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t(\"system\", rag_system_prompt),\n",
    "        (\"user\", document_relevant_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_sufficient_template = \"\"\"\\\n",
    "The user is now asking a question. \\\n",
    "For answering the question, your colleague has retrieved the documents below. \\\n",
    "Return True if the documents are sufficient for answering the question, and False otherwise.\n",
    "\n",
    "\n",
    "===========================\n",
    "Documents:\n",
    "\n",
    "{documents}\n",
    "===========================\n",
    "\n",
    "\n",
    "The user question is:\n",
    "{input}\n",
    "\n",
    "Verdict (True/False):\n",
    "\"\"\"\n",
    "\n",
    "documents_sufficient_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t\t(\"system\", rag_system_prompt),\n",
    "        (\"user\", documents_sufficient_template)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2abb9a",
   "metadata": {},
   "source": [
    "To reliably parse the generations, we need to add the following structure output schemes:\n",
    "1. String list for the generated queries.\n",
    "2. Rating 1-5 for document relevancy.\n",
    "3. Boolean `True`/`False` for binary documents sufficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d332ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListOutput(BaseModel):\n",
    "    outputs: List[str] = Field(..., description=\"List of generated outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating1_5(BaseModel):\n",
    "    rating: int = Field(..., description=\"Rating from 1 to 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoVerdict(BaseModel):\n",
    "    verdict: bool = Field(..., description=\"Boolean answer to the given binary question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99272e46",
   "metadata": {},
   "source": [
    "For advanced RAG, we change the LLM and the settings to increase the overall stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any model, catalogue is available under https://build.nvidia.com/models\n",
    "MODEL_NAME_ADVANCED_RAG = \"meta/llama-3.1-405b-instruct\"\n",
    "\n",
    "# this rate limiter will ensure we do not exceed the rate limit\n",
    "# of 40 RPM given by NVIDIA\n",
    "rate_limiter_advanced_rag = InMemoryRateLimiter(\n",
    "    requests_per_second=1 / 6,  # 10 requests per minute to be sure\n",
    "    check_every_n_seconds=5,  # wake up every 5 seconds to check whether allowed to make a request,\n",
    "    max_bucket_size=1  # controls the maximum burst size\n",
    ")\n",
    "\n",
    "llm_advanced_rag = ChatNVIDIA(\n",
    "    model=MODEL_NAME_ADVANCED_RAG,\n",
    "    # api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    temperature=0,   # ensure reproducibility,\n",
    "    rate_limiter=rate_limiter_advanced_rag  # bind the rate limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ab6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeRAGChatbot(BasicPlusRAGChatbot):\n",
    "\n",
    "    _graph_path = \"./graph_iterative_rag.png\"\n",
    "\n",
    "    def __init__(self, llm, k=5, max_generations=3):\n",
    "        super().__init__(llm, k)\n",
    "        self.max_generations = max_generations\n",
    "        self.query_generator = generate_query_prompt | llm.with_structured_output(ListOutput)\n",
    "        self.document_relevant_grader = document_relevant_prompt | llm.with_structured_output(Rating1_5)\n",
    "        self.documents_sufficient_grader = documents_sufficient_prompt | llm.with_structured_output(YesNoVerdict)\n",
    "\n",
    "    def _build(self):\n",
    "        # graph builder\n",
    "        self._graph_builder = StateGraph(AdvancedRAGState)\n",
    "        # add the nodes\n",
    "        self._graph_builder.add_node(\"input\", self._input_node)\n",
    "        self._graph_builder.add_node(\"complete_query\", self._complete_query_node)\n",
    "        self._graph_builder.add_node(\"generate_query\", self._generate_query_node)\n",
    "        self._graph_builder.add_node(\"retrieve\", self._retrieve_node)\n",
    "        self._graph_builder.add_node(\"filter_documents\", self._filter_documents_node)\n",
    "        self._graph_builder.add_node(\"respond\", self._respond_node)\n",
    "        # define edges\n",
    "        self._graph_builder.add_edge(START, \"input\")\n",
    "        # basic rag: no planning, just always retrieve\n",
    "        self._graph_builder.add_conditional_edges(\"input\", self._is_quitting_node, {False: \"complete_query\", True: END})\n",
    "        self._graph_builder.add_edge(\"complete_query\", \"generate_query\")\n",
    "        self._graph_builder.add_edge(\"generate_query\", \"retrieve\")\n",
    "        self._graph_builder.add_edge(\"retrieve\", \"filter_documents\")\n",
    "        self._graph_builder.add_conditional_edges(\n",
    "            \"filter_documents\",\n",
    "            self._documents_sufficient_node,\n",
    "            {\n",
    "                False: \"generate_query\",\n",
    "                True: \"respond\",\n",
    "                None: END   # max generations reached\n",
    "            }\n",
    "        )\n",
    "        self._graph_builder.add_edge(\"respond\", \"input\")\n",
    "        # compile the graph\n",
    "        self._compile()\n",
    "\n",
    "    def _generate_query_node(self, state: AdvancedRAGState) -> dict:    \n",
    "        complete_query = state[\"messages\"][-1].content  # complete user query\n",
    "        previous_queries = state[\"queries\"][-1] if state[\"queries\"] else []\n",
    "        generated_queries = self.query_generator.invoke(\n",
    "            {\n",
    "                # put previous queries\n",
    "                \"previous_queries\": \"\\n\".join(previous_queries) or \"NONE\",\n",
    "                \"input\": complete_query\n",
    "            }\n",
    "        )\n",
    "        return {\n",
    "            # could have also used `Annotated` here\n",
    "            \"queries\": state[\"queries\"] + [generated_queries.outputs]   # List[str]\n",
    "        }\n",
    "\n",
    "    # now store the documents in the separate field\n",
    "    def _retrieve_node(self, state: AdvancedRAGState) -> dict:    \n",
    "        # retrieve the documents\n",
    "        latest_queries = state[\"queries\"][-1]  # the last generated queries\n",
    "        # now use the retriever directly to get a list of documents and not a combined string\n",
    "        latest_documents = state[\"docs\"][-1] if state[\"docs\"] else []\n",
    "        newest_documents = []\n",
    "        for query in latest_queries:\n",
    "             documents = self.retriever.invoke(query)\n",
    "             newest_documents.extend([doc for doc in documents if doc not in newest_documents])\n",
    "        # add the document to the messages\n",
    "        newest_documents = [doc for doc in newest_documents if doc not in latest_documents]\n",
    "        return {\n",
    "            # could have also used `Annotated` here\n",
    "            \"docs\": state[\"docs\"] + [newest_documents]\n",
    "        }\n",
    "    \n",
    "    def _filter_documents_node(self, state: AdvancedRAGState) -> dict:\n",
    "        complete_query = state[\"messages\"][-1].content  # complete user query\n",
    "        # since the retrieved documents are graded at the same step,\n",
    "        # we only need to pass the last batch of documents\n",
    "        latest_documents = state[\"docs\"].pop(-1)  # will be replaced with the filtered ones\n",
    "        # grade each document separately and only keep the relevant ones\n",
    "        latest_documents_relevant = []\n",
    "        for document in latest_documents:\n",
    "            print(\"Grading document:\\n\\n\", document.page_content)\n",
    "            rating = self.document_relevant_grader.invoke(\n",
    "                {\n",
    "                    \"document\": document.page_content,    # this is a Document object\n",
    "                    \"input\": complete_query\n",
    "                }\n",
    "            )\n",
    "            print(\"\\n\\n\")\n",
    "            print(f\"Rating: {rating.rating}\")\n",
    "            print(\"\\n\\n=====================\\n\\n\")\n",
    "            if rating.rating >= 3:    # boolean value according to the Pydantic model\n",
    "                latest_documents_relevant.append(document)\n",
    "        return {\n",
    "            # could have also used `Annotated` here\n",
    "            \"docs\": state[\"docs\"] + [latest_documents_relevant]\n",
    "        }\n",
    "    \n",
    "    def _flatten_documents(self, state: AdvancedRAGState) -> List[Document]:\n",
    "        all_documents = state[\"docs\"]\n",
    "        return [document for sublist in all_documents for document in sublist]\n",
    "\n",
    "    def _documents_sufficient_node(self, state: AdvancedRAGState) -> dict:\n",
    "        complete_query = state[\"messages\"][-1].content  # complete user query\n",
    "        documents = self._flatten_documents(state)\n",
    "        if not documents:\n",
    "            return False\n",
    "        documents_str = \"\\n\\n\".join([document.page_content for document in documents])\n",
    "        print(\"Deciding whether the documents are sufficient\")\n",
    "        verdict = self.documents_sufficient_grader.invoke(\n",
    "                {\n",
    "                \"documents\": documents_str,    # this is a Document object\n",
    "                \"input\": complete_query\n",
    "            }\n",
    "        )\n",
    "        print(\"\\n\\n\")\n",
    "        print(f\"Verdict: {verdict.verdict}\")\n",
    "        print(\"\\n\\n=====================\\n\\n\")\n",
    "        if not verdict.verdict and len(documents) == self.max_generations:\n",
    "            return  # will route to END\n",
    "        return verdict.verdict\n",
    "    \n",
    "    def _respond_node(self, state: AdvancedRAGState) -> dict:\n",
    "        documents = self._flatten_documents(state)\n",
    "        documents_str = \"\\n\\n\".join([document.page_content for document in documents])\n",
    "        complete_query = state[\"messages\"][-1].content\n",
    "        prompt = self.rag_prompt.invoke(\n",
    "            {\n",
    "                \"messages\": state[\"messages\"],  # this goes to the message placeholder\n",
    "                \"context\": documents_str,  # this goes to the user message\n",
    "                \"input\": complete_query    # this goes to the user message\n",
    "            }\n",
    "        )\n",
    "        response = self.llm.invoke(prompt)\n",
    "        # add the response to the messages\n",
    "        return {\n",
    "            \"messages\": response\n",
    "        }\n",
    "    \n",
    "    def run(self):\n",
    "        input = {\"messages\": [], \"docs\": [], \"queries\": []}\n",
    "        for event in self.chatbot.stream(input, stream_mode=\"updates\"):\n",
    "            for key, value in event.items():\n",
    "                if value and value.get(\"messages\"):\n",
    "                    # print(key, end=\"\\n\\n\")\n",
    "                    if not isinstance(value[\"messages\"], list):\n",
    "                        value[\"messages\"] = [value[\"messages\"]]\n",
    "                    for message in  value[\"messages\"]:\n",
    "                        message.pretty_print()\n",
    "                    print(\"\\n\\n\")\n",
    "                elif value and value.get(\"queries\"):\n",
    "                    print(f\"\\n\\nGenerated queries: {', '.join(value['queries'][-1])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae551fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make very small k to ensure one retrieval is not enough\n",
    "iterative_rag_chatbot = IterativeRAGChatbot(llm_advanced_rag, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_rag_chatbot.run()\n",
    "# Summarize the topics of all the pitches in two sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
