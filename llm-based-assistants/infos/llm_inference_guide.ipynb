{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb660812",
   "metadata": {},
   "source": [
    "# LLM Inference Guide\n",
    "\n",
    "For this course, we will be using NVIDIA Cloud that generously hosts various open-source LLMs and provides a free API limited by 40 requests per minute (RPM). This guide shows how you set up your account and start using the LLMs.\n",
    "\n",
    "\n",
    "## Contents\n",
    "* [Prerequisites](#prerequisites)\n",
    "* [Environment Setup](#environment-setup)\n",
    "* [Getting API Key](#getting-api-key)\n",
    "* [Test](#test)\n",
    "* [Next Steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48da0e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Install [Python](https://www.python.org/downloads/) on your machine.\n",
    "2. Install [Git](https://git-scm.com/downloads).\n",
    "3. Install any IDE of your choice: [VS Code](https://code.visualstudio.com/download), [Pycharm](https://www.jetbrains.com/help/pycharm/installation-guide.html) etc.\n",
    "4. Create an account at [NVIDIA Developer Program](https://developer.nvidia.com/developer-program) with your **student email**: `firstname.lastname@student.uni-tuebingen.de`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b7f8b",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "It is a good practice to have a separate isolated environment for each project. Such environment includes all of your code, resources, tests etc, as well as dependencies, (sometimes) executables and such.\n",
    "\n",
    "1. Make a new directory where your project will be stored and open it in your IDE.\n",
    "2. Open the terminal. If you are a Windows user, open GitBash (will be available after Git installation) and not the default cmd.\n",
    "3. Create a Python virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948a7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv  # create a copy of Python and so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f473cbd",
   "metadata": {},
   "source": [
    "Activate your environment to tell the interpreter that you will be working with this particular copy of Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d877133",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/bin/activate # for Unix-based (including MacOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda30d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/Scripts/activate   # for Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701d3f2",
   "metadata": {},
   "source": [
    "4. Install requirements. Here, for the test purposes, we only need `langchain_nvidia_ai_endpoints` and `python-dotenv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_nvidia_ai_endpoints python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30046e45",
   "metadata": {},
   "source": [
    "_Note_: a more robust (and really used) alternative is to create a _requirements.txt_ file like this:\n",
    "\n",
    "```text\n",
    "langchain_nvidia_ai_endpoints==0.3.18\n",
    "python-dotenv==1.2.1\n",
    "```\n",
    "\n",
    "and then execute\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74daa8",
   "metadata": {},
   "source": [
    "## Getting API Key\n",
    "\n",
    "Now that you have completed all the prerequisites and prepared an environment to work in, you only need to configure an API key.\n",
    "\n",
    "1. Create an empty _.env_ file with the following variable (leave empty for now):\n",
    "\n",
    "    ```bash\n",
    "    NVIDIA_API_KEY=\"\"\n",
    "    ```\n",
    "\n",
    "2. Log in to [NVIDIA Cloud](https://build.nvidia.com/) with the account you created in [prerequisites](#prerequisites).\n",
    "3. Go to your profile (upper right corner) > API Keys. Click Generate API Key, name it and copy.\n",
    "4. Put the key value to your _.env_ under the `NVIDIA_API_KEY` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d68c9b",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Finally, you can test if the API works for you. Below is a sample code you can run for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5111b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "dotenv.load_dotenv()    # that loads the .env file variables into os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8b870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose any model, catalogue is available under https://build.nvidia.com/models\n",
    "MODEL_NAME = \"meta/llama-3.3-70b-instruct\"\n",
    "\n",
    "# prompts are usually stored in a separate file\n",
    "# but for the sake of simplicity, we will have it here\n",
    "SYSTEM_MESSAGE = \"You are a medieval French knight.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6394b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        # this rate limiter will ensure we do not exceed the rate limit\n",
    "        # of 40 RPM given by NVIDIA\n",
    "        rate_limiter = InMemoryRateLimiter(\n",
    "            requests_per_second=35 / 60,  # 35 requests per minute to be sure\n",
    "            check_every_n_seconds=0.1,  # wake up every 100 ms to check whether allowed to make a request,\n",
    "            max_bucket_size=7,  # controls the maximum burst size\n",
    "        )\n",
    "        self.llm = ChatNVIDIA(\n",
    "            model=MODEL_NAME,\n",
    "            api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "            temperature=0,   # ensure reproducibility,\n",
    "            rate_limiter=rate_limiter  # bind the rate limiter\n",
    "        )\n",
    "\n",
    "    # the simplest example (synchronous implementation)\n",
    "    def invoke(self, user_query):\n",
    "        # prepare the messages\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=SYSTEM_MESSAGE\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=user_query\n",
    "            )\n",
    "        ]\n",
    "        # inference\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24178a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, my friend. I, Sir Guillaume, shall recount to thee the tale of the Battle of Agincourt, a most glorious and bloody conflict that took place on the 25th day of October, in the year of our Lord 1415.\n",
      "\n",
      "As a knight of the realm, I must admit that our French army, led by the noble Charles d'Albret, Constable of France, and Charles, Duke of Orléans, was confident in our superior numbers and chivalry. We had assembled a vast host, with estimates ranging from 20,000 to 30,000 men-at-arms, knights, and men of the common sort.\n",
      "\n",
      "However, the English army, led by the cunning King Henry V, had other plans. Though outnumbered, with a force of around 6,000 to 9,000 men, they had chosen a most advantageous position, with a narrow field between two forests, which funneled our attacks and limited our mobility.\n",
      "\n",
      "The English, with their longbowmen and men-at-arms, formed a defensive line, with stakes driven into the ground to protect themselves from our cavalry charges. Our French knights, weighed down by our armor and hindered by the muddy terrain, charged valiantly, but were cut down by the hail of arrows and the English defensive line.\n",
      "\n",
      "The battle raged on for hours, with our knights and men-at-arms making repeated charges, only to be repelled by the English. The mud and the stakes proved to be our undoing, as our horses became mired and our men were unable to breach the English lines.\n",
      "\n",
      "In the end, it is said that the English suffered fewer than 100 casualties, while our French army lost thousands, including many noble knights and men of high birth. The Constable of France, Charles d'Albret, was among the fallen, and the Duke of Orléans was taken prisoner.\n",
      "\n",
      "Verily, the Battle of Agincourt was a dark day for France, and a testament to the cunning and bravery of the English. Mayhap, one day, we shall avenge this defeat and restore the honor of our noble kingdom. Vive la France!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Agent()\n",
    "    # ask the knight a question\n",
    "    user_query = \"Give me a summary of the Battle of Agincourt.\"\n",
    "    response = agent.invoke(user_query)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e4415",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "As for now, you're good to go! Later, for each of the projects, you will only do the [environment setup](#environment-setup) and the steps 1 and 4 of [getting API Key](#getting-api-key); and instead of the sample code, you will have cool complex stuff, but we'll get to that yet.\n",
    "\n",
    "Contact me in case of any questions and problems you encounter during the setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
